<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MySQL索引/内存索引的一些点]]></title>
    <url>%2F2019%2F07%2F15%2FMySQL%E7%B4%A2%E5%BC%95%E5%86%85%E5%AD%98%E7%B4%A2%E5%BC%95%E7%9A%84%E4%B8%80%E4%BA%9B%E7%82%B9%2F</url>
    <content type="text"><![CDATA[数据库的点很多很杂，记录下来。磁盘索引和内存索引Q: 为何要有B-Tree为代表多叉平衡树和红黑树为代表的二叉平衡树两种索引数据结构？B-Tree是为硬盘设计的索引数据结构 针对硬盘的顺序读写速度优于随机读写速度，B-Tree的单个节点存在多个，这样能让一个指针(引用)不再对应一个键值对，而是对应一个连续的键值区间的键值对集合。 因为多个指针指向的数据不一定是连续，但是一个指针指向的存储区域一定是连续，能够将大量的随机读写转换为局部的顺序读写。 磁盘I/O相对于内存速度存在量级上的差距，B-tree适合做大数据量的索引结构，整棵树存储在硬盘，读写时将某个节点加载到内存。 在树形结构中，查询一个数据的最多的I/O次数为树的高度，B-Tree的多叉树结构相对于二叉树高度更低。每次 I/O 将单个 B-Tree 节点加载到内存，在内存中进行二分查找，因此总的开销是：一共要进行 h (B-Tree 的高度)次磁盘 I/O，加上若干次(以 2 为底数，对每个节点的”长度”取对数)的内存二分查找。 平衡二叉树是为内存设计的索引数据结构内存中随机读写与顺序读写速度相当，平衡二叉树存储的数据量较小，整颗树存储到内存,所以使用B-Tree的话，在查找时白白浪费CPU进行二分查找。B-tree存储的数据量大，平衡二叉树存储的数据量较小怎么理解，是什么造成这样的差异因为数据量大，所以必须存到硬盘(在不考虑持久化的问题上)。两种数据结构从视图上都是索引，却在结构上一个更适应内存，一个更适应机械硬盘，所以结论是数据的差异决定了数据结构的选型。造成差异的原因可能是业务决定的，比如一些元数据量就小，日志数据量就大。MySQL唯一索引、主键索引、辅助索引的区别 主键是一种约束，唯一索引是一种索引，两者在本质上是不同的。 主键列在创建时，已经默认为非空值 + 唯一索引了。 主键创建后一定包含一个唯一性索引，唯一性索引并不一定就是主键唯一性索引列允许空值，而主键列不允许为空值 主键可以被其他表引用为外键，而唯一索引不能。一个表最多只能创建一个主键，但可以创建多个唯一索引。主键更适合那些不容易更改的唯一标识，如自动递增列、身份证号等。 另外，在不同的隔离级别下加锁的操作有所不同，以最常用的Repeatable Read为例。假设在事务中执行语句：select * from t1 where id = ? for update 当id是主键的时候，Mysql会直接找到主键索引，然后将其加锁。当id是唯一索引的时候，Mysql会先根据唯一索引找到主键索引，然后将两个索引都加锁。当id是辅助索引的时候，Mysql会根据索引找到对应的主键索引（可能会有多个），然后全都加锁。当id没有索引的时候，Mysql会给所有记录加上行锁，即锁全表。 所以，在给记录加锁的时候，一定要保证 where 的条件列上有索引，否则会导致全表被锁。 作者：陈半仙儿链接：https://www.jianshu.com/p/7ab69aba3433来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
        <tag>B+树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins+Docker实现自动化构建部署]]></title>
    <url>%2F2019%2F05%2F07%2Fjenkins%2Bdocker%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%8C%96%E6%9E%84%E5%BB%BA%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[应用场景 程序员开发应用，开发后需要提交svn，然后从svn拉取代码，进行构建，发布到tomcat中，发布，然后看呈现效果，这样的工作是频繁反复的在进行的，浪费了程序员的大量时间，那么能不能把这些工作自动化呢，只需要程序员更新代码到svn，然后自动的构建，发布，呈现效果，当然是可以的，通过jenkins和docker来实现。 操作步骤 概述 开发者的工作大概流程是，IDEA编写代码，提交代码到svn，然后进行编译，打包，测试，部署，发布。 这其中很多重复的工作，影响开发人员的情绪，应当让开发者只重视代码阶段，后面的内容不用去理会，只要编写代码，提交代码，然后就能打开页面看到效果，那是最好的。 这就需要自动化构建，jenkins就是很好的自动化构建工具。 自动化流程具体流程如下： 1.编写代码，修改代码 2.提交代码到svn 3.Jenkins自动检测到svn代码更新，从svn拉取最新代码 4.Jenkins自动编译 5.Jenkins自动打成tomcat下能运行的war包 6.Jenkins自动上传war包到docker宿主机目录 7.Jenkins自动构建web容器镜像，包含项目demo 8.Jenkins自动启动镜像，变成容器，映射端口 9.最后就有项目运行的界面提供了 环境准备 1 jenkins Jenkins简单点说就是一个war包。一个自动化服务，提供各种插件集成。 jenkins下载地址 http://mirrors.jenkins-ci.org/war/latest/jenkins.war Jenkins部署有两种方法（可以装到windows或者linux，实验中以windows的方式一为例）： Jenkins是用Java语言开发的系统，首先要确定服务器上已经安装JDK或者JRE。 1.方式一 直接运行java –jar jenkins.war（注意jenkins.war得是绝对路径，例如下图所示），在浏览器中输入 http://localhost:8080即可。 到此，jenkins安装部署完毕，关于如何使用jenkins，下文详细介绍。2.方式二 把jenkins的war包拷贝到tomcat的webapps目录下，启动tomcat，访问地址：http://ip:8080/jenkins3.2 maven 参照windows安装maven博文3.3 svn 参照svn如何使用的相关博文 以及windows端安装svn服务器端的博文3.4 jdk 参考windows安装jdk博文3.5 docker 参考docker安装使用博文 基础配置 1 jenkins配置 首先需要进入jenkins主界面，安装必须要的插件。 进入插件页面，选择可选插件，搜索ssh，安装SSH plugin和Publish Over SSH插件，由于实验已经安装过，所以图中没有显示了。 然后回到主界面，进行系统设置。 配置maven configuretion，加上本地maven安装地址，指向settings文件。 jdk配置，指向本地jdk安装目录。 Maven配置，指向本地maven安装路径。 配置jenkins location，配置管理员邮箱。 配置邮件通知，需要邮箱开启pop3，smtp，可以测试验证。后面配置邮件通知时需要用到。 配置Publish over SSH。这里配置后才能在配置目标容器环境时，选择到构建完成后发布到的目标容器。 最后应用，保存。 5.配置自动化5.1 编写代码 这里需要基于maven构建的工程。5.2 提交代码至svn 右击项目，选择team，提交，到svn服务器的某个地址，实验中提交到如下目录： 需要在svn下新建mavenproject文件夹，具体操作参考3.3。5.3 新建jenkins项目连接svn 点击界面左侧新建，项目名称可以随意命名，选择maven项目： 选择源码地址，这功能可以让jenkins可以从svn上拉取代码。 Repository URL获取方式：5.4 配置构建任务 该功能，jenkins可以检测svn代码，如果发生变化，那么立即自动执行构建：5.5 配置构建参数 该功能，jenkins可以自动给maven项目打包5.6 邮件通知 该功能，jenkins构建过程中，可以向指定邮箱发送邮件，提示构建状况。5.7 配置目标容器环境 选择send build artifacts over SSH，该功能可以实现将本地打包的war包，ssh到指定服务器的目录中，并且能在服务器上执行脚本。脚本如下： 这样jenkins上就操作完毕了。5.8 生成docker实例 在docker宿主机输入如下命令： 执行完毕后，打开虚拟机上的/Dockerfile_tomcat目录，修改该目录下的Dockerfile文件，加入如下一行：5.9 测试 当我们修改eclipse上的代码，然后提交到svn： 等几分钟就能看到结果。 可以看到jenkins自动开始构建了： 访问页面： 最后可以看出，生成了指定的容器。原文链接：https://cloud.tencent.com/developer/article/1350301]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP和HTTPS对比 以及HTTPS是如何保证安全性的]]></title>
    <url>%2F2019%2F04%2F29%2FHTTP%E5%92%8CHTTPS%E5%AF%B9%E6%AF%94%20%20HTTPS%E6%98%AF%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%AE%89%E5%85%A8%E6%80%A7%E7%9A%84%2F</url>
    <content type="text"><![CDATA[HTTP和HTTPS对比 HTTPS是如何保证安全性的HTTPS（全称：Hypertext Transfer Protocol over Secure Socket Layer）HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL Https的劣势对数据进行加解密决定了它比http慢需要进行非对称的加解密，且需要三次握手。首次连接比较慢点，当然现在也有很多的优化。出于安全考虑，浏览器不会在本地保存HTTPS缓存。当然，只要在HTTP头中使用特定命令，HTTPS也是可以缓存的。 HTTPS和HTTP的区别https协议需要到CA申请证书。http是超文本传输协议，信息是明文传输；https 则是具有安全性的ssl加密传输协议。http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。下面就是https的整个架构，现在的https基本都使用TLS了，因为更加安全，所以下图中的SSL应该换为SSL/TLS。 下面就上图中的知识点进行一个大概的介绍。 加解密相关知识对称加密对称加密(也叫私钥加密)指加密和解密使用相同密钥的加密算法。有时又叫传统密码算法，就是加密密钥能够从解密密钥中推算出来，同时解密密钥也可以从加密密钥中推算出来。而在大多数的对称算法中，加密密钥和解密密钥是相同的，所以也称这种加密算法为秘密密钥算法或单密钥算法。常见的对称加密有：DES（Data Encryption Standard）、AES（Advanced Encryption Standard）、RC4、IDEA 非对称加密与对称加密算法不同，非对称加密算法需要两个密钥：公开密钥（publickey）和私有密钥（privatekey）；并且加密密钥和解密密钥是成对出现的。非对称加密算法在加密和解密过程使用了不同的密钥，非对称加密也称为公钥加密，在密钥对中，其中一个密钥是对外公开的，所有人都可以获取到，称为公钥，其中一个密钥是不公开的称为私钥。 非对称加密算法对加密内容的长度有限制，不能超过公钥长度。比如现在常用的公钥长度是 2048 位，意味着待加密内容不能超过 256 个字节。 摘要算法数字摘要是采用单项Hash函数将需要加密的明文“摘要”成一串固定长度（128位）的密文，这一串密文又称为数字指纹，它有固定的长度，而且不同的明文摘要成密文，其结果总是不同的，而同样的明文其摘要必定一致。“数字摘要“是https能确保数据完整性和防篡改的根本原因。 数字签名数字签名技术就是对“非对称密钥加解密”和“数字摘要“两项技术的应用，它将摘要信息用发送者的私钥加密，与原文一起传送给接收者。接收者只有用发送者的公钥才能解密被加密的摘要信息，然后用HASH函数对收到的原文产生一个摘要信息，与解密的摘要信息对比。如果相同，则说明收到的信息是完整的，在传输过程中没有被修改，否则说明信息被修改过，因此数字签名能够验证信息的完整性。数字签名的过程如下： 明文 --&gt; hash运算 --&gt; 摘要 --&gt; 私钥加密 --&gt; 数字签名 数字签名有两种功效：一、能确定消息确实是由发送方签名并发出来的，因为别人假冒不了发送方的签名。二、数字签名能确定消息的完整性。 注意： 数字签名只能验证数据的完整性，数据本身是否加密不属于数字签名的控制范围 数字证书为什么要有数字证书？对于请求方来说，它怎么能确定它所得到的公钥一定是从目标主机那里发布的，而且没有被篡改过呢？亦或者请求的目标主机本本身就从事窃取用户信息的不正当行为呢？这时候，我们需要有一个权威的值得信赖的第三方机构(一般是由政府审核并授权的机构)来统一对外发放主机机构的公钥，只要请求方这种机构获取公钥，就避免了上述问题的发生。 数字证书的颁发过程用户首先产生自己的密钥对，并将公共密钥及部分个人身份信息传送给认证中心。认证中心在核实身份后，将执行一些必要的步骤，以确信请求确实由用户发送而来，然后，认证中心将发给用户一个数字证书，该证书内包含用户的个人信息和他的公钥信息，同时还附有认证中心的签名信息(根证书私钥签名)。用户就可以使用自己的数字证书进行相关的各种活动。数字证书由独立的证书发行机构发布，数字证书各不相同，每种证书可提供不同级别的可信度。 证书包含哪些内容 证书颁发机构的名称 证书本身的数字签名 证书持有者公钥 证书签名用到的Hash算法 验证证书的有效性浏览器默认都会内置CA根证书，其中根证书包含了CA的公钥 证书颁发的机构是伪造的：浏览器不认识，直接认为是危险证书 证书颁发的机构是确实存在的，于是根据CA名，找到对应内置的CA根证书、CA的公钥。用CA的公钥，对伪造的证书的摘要进行解密，发现解不了，认为是危险证书。 对于篡改的证书，使用CA的公钥对数字签名进行解密得到摘要A，然后再根据签名的Hash算法计算出证书的摘要B，对比A与B，若相等则正常，若不相等则是被篡改过的。 证书可在其过期前被吊销，通常情况是该证书的私钥已经失密。较新的浏览器如Chrome、Firefox、Opera和Internet Explorer都实现了在线证书状态协议（OCSP）以排除这种情形：浏览器将网站提供的证书的序列号通过OCSP发送给证书颁发机构，后者会告诉浏览器证书是否还是有效的。 1、2点是对伪造证书进行的，3是对于篡改后的证书验证，4是对于过期失效的验证。 SSL 与 TLSSSL (Secure Socket Layer，安全套接字层)SSL为Netscape所研发，用以保障在Internet上数据传输之安全，利用数据加密(Encryption)技术，可确保数据在网络上之传输过程中不会被截取，当前为3.0版本。 SSL协议可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。 TLS (Transport Layer Security，传输层安全协议)用于两个应用程序之间提供保密性和数据完整性。TLS 1.0是IETF（Internet Engineering Task Force，Internet工程任务组）制定的一种新的协议，它建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本，可以理解为SSL 3.1，它是写入了 RFC 的。该协议由两层组成： TLS 记录协议（TLS Record）和 TLS 握手协议（TLS Handshake）。较低的层为 TLS 记录协议，位于某个可靠的传输协议（例如 TCP）上面。 SSL/TLS协议作用： 认证用户和服务器，确保数据发送到正确的客户机和服务器； 加密数据以防止数据中途被窃取； 维护数据的完整性，确保数据在传输过程中不被改变。 TLS比SSL的优势 对于消息认证使用密钥散列法：TLS 使用“消息认证代码的密钥散列法”（HMAC），当记录在开放的网络（如因特网）上传送时，该代码确保记录不会被变更。SSLv3.0还提供键控消息认证，但HMAC比SSLv3.0使用的（消息认证代码）MAC 功能更安全。 增强的伪随机功能（PRF）：PRF生成密钥数据。在TLS中，HMAC定义PRF。PRF使用两种散列算法保证其安全性。如果任一算法暴露了，只要第二种算法未暴露，则数据仍然是安全的。 改进的已完成消息验证：TLS和SSLv3.0都对两个端点提供已完成的消息，该消息认证交换的消息没有被变更。然而，TLS将此已完成消息基于PRF和HMAC值之上，这也比SSLv3.0更安全。 一致证书处理：与SSLv3.0不同，TLS试图指定必须在TLS之间实现交换的证书类型。 特定警报消息：TLS提供更多的特定和附加警报，以指示任一会话端点检测到的问题。TLS还对何时应该发送某些警报进行记录。 SSL、TLS的握手过程SSL与TLS握手整个过程如下图所示，下面会详细介绍每一步的具体内容： 客户端首次发出请求由于客户端(如浏览器)对一些加解密算法的支持程度不一样，但是在TLS协议传输过程中必须使用同一套加解密算法才能保证数据能够正常的加解密。在TLS握手阶段，客户端首先要告知服务端，自己支持哪些加密算法，所以客户端需要将本地支持的加密套件(Cipher Suite)的列表传送给服务端。除此之外，客户端还要产生一个随机数，这个随机数一方面需要在客户端保存，另一方面需要传送给服务端，客户端的随机数需要跟服务端产生的随机数结合起来产生后面要讲到的 Master Secret 。 客户端需要提供如下信息： 支持的协议版本，比如TLS 1.0版 一个客户端生成的随机数，稍后用于生成”对话密钥” 支持的加密方法，比如RSA公钥加密 支持的压缩方法 服务端首次回应服务端在接收到客户端的Client Hello之后，服务端需要确定加密协议的版本，以及加密的算法，然后也生成一个随机数，以及将自己的证书发送给客户端一并发送给客户端，这里的随机数是整个过程的第二个随机数。 服务端需要提供的信息： 协议的版本 加密的算法 随机数 服务器证书 客户端再次回应客户端首先会对服务器下发的证书进行验证，验证通过之后，则会继续下面的操作，客户端再次产生一个随机数（第三个随机数），然后使用服务器证书中的公钥进行加密，以及放一个ChangeCipherSpec消息即编码改变的消息，还有整个前面所有消息的hash值，进行服务器验证，然后用新秘钥加密一段数据一并发送到服务器，确保正式通信前无误。客户端使用前面的两个随机数以及刚刚新生成的新随机数，使用与服务器确定的加密算法，生成一个Session Secret。 ChangeCipherSpecChangeCipherSpec是一个独立的协议，体现在数据包中就是一个字节的数据，用于告知服务端，客户端已经切换到之前协商好的加密套件（Cipher Suite）的状态，准备使用之前协商好的加密套件加密数据并传输了。 服务器再次响应服务端在接收到客户端传过来的第三个随机数的 加密数据之后，使用私钥对这段加密数据进行解密，并对数据进行验证，也会使用跟客户端同样的方式生成秘钥，一切准备好之后，也会给客户端发送一个 ChangeCipherSpec，告知客户端已经切换到协商过的加密套件状态，准备使用加密套件和 Session Secret加密数据了。之后，服务端也会使用 Session Secret 加密一段 Finish 消息发送给客户端，以验证之前通过握手建立起来的加解密通道是否成功。 后续客户端与服务器间通信确定秘钥之后，服务器与客户端之间就会通过商定的秘钥加密消息了，进行通讯了。整个握手过程也就基本完成了。 值得特别提出的是：SSL协议在握手阶段使用的是非对称加密，在传输阶段使用的是对称加密，也就是说在SSL上传送的数据是使用对称密钥加密的！因为非对称加密的速度缓慢，耗费资源。其实当客户端和主机使用非对称加密方式建立连接后，客户端和主机已经决定好了在传输过程使用的对称加密算法和关键的对称加密密钥，由于这个过程本身是安全可靠的，也即对称加密密钥是不可能被窃取盗用的，因此，保证了在传输过程中对数据进行对称加密也是安全可靠的，因为除了客户端和主机之外，不可能有第三方窃取并解密出对称加密密钥！如果有人窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两个。整个通话的安全，只取决于第三个随机数（Premaster secret）能不能被破解。 其他补充对于非常重要的保密数据，服务端还需要对客户端进行验证，以保证数据传送给了安全的合法的客户端。服务端可以向客户端发出 Cerficate Request 消息，要求客户端发送证书对客户端的合法性进行验证。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供USB密钥，里面就包含了一张客户端证书。 PreMaster secret前两个字节是TLS的版本号，这是一个比较重要的用来核对握手数据的版本号，因为在Client Hello阶段，客户端会发送一份加密套件列表和当前支持的SSL/TLS的版本号给服务端，而且是使用明文传送的，如果握手的数据包被破解之后，攻击者很有可能串改数据包，选择一个安全性较低的加密套件和版本给服务端，从而对数据进行破解。所以，服务端需要对密文中解密出来对的PreMaster版本号跟之前Client Hello阶段的版本号进行对比，如果版本号变低，则说明被串改，则立即停止发送任何消息。 session的恢复有两种方法可以恢复原来的session：一种叫做session ID，另一种叫做session ticket。 session IDsession ID的思想很简单，就是每一次对话都有一个编号（session ID）。如果对话中断，下次重连的时候，只要客户端给出这个编号，且服务器有这个编号的记录，双方就可以重新使用已有的”对话密钥”，而不必重新生成一把。 session ID是目前所有浏览器都支持的方法，但是它的缺点在于session ID往往只保留在一台服务器上。所以，如果客户端的请求发到另一台服务器，就无法恢复对话 session ticket客户端发送一个服务器在上一次对话中发送过来的session ticket。这个session ticket是加密的，只有服务器才能解密，其中包括本次对话的主要信息，比如对话密钥和加密方法。当服务器收到session ticket以后，解密后就不必重新生成对话密钥了。 目前只有Firefox和Chrome浏览器支持。 总结https实际就是在TCP层与http层之间加入了SSL/TLS来为上层的安全保驾护航，主要用到对称加密、非对称加密、证书，等技术进行客户端与服务器的数据加密传输，最终达到保证整个通信的安全性。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>HTTPS</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[游戏圈的鄙视链]]></title>
    <url>%2F2019%2F04%2F08%2F%E6%B8%B8%E6%88%8F%E5%9C%88%E7%9A%84%E9%84%99%E8%A7%86%E9%93%BE-1%2F</url>
    <content type="text"><![CDATA[图中观点不代表博主 就是觉得总结的有点意思 哈哈]]></content>
      <categories>
        <category>游戏</category>
      </categories>
      <tags>
        <tag>游戏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04 搭建VPN服务]]></title>
    <url>%2F2019%2F04%2F02%2FUbuntu16.04%20%E6%90%AD%E5%BB%BAVPN%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[original url: http://www.jianshu.com/p/8ac024216888 http://jingyan.baidu.com/article/ce09321b23cf7c2bff858fb8.html http://www.guoziweb.com/archive/265.html 步骤:1.第一步需要安装PPTP，以用来提供VPN服务.sudo apt-get install pptpd如果有问题的话比如提示找不到之类的，apt-get update 一下应该就可以了，然后再来一次就会自动完成安装。 2.装好了之后我们需要进行配置一下以让它可以使用.sudo vi /etc/pptpd.conf 取消掉以下 2 行的注释并修改为自己设置的vpn网段：localip 192.168.1.254remoteip 192.168.150.234-238分别是通过VPN连接后主机和客户端所使用的IP，可以自行修改。注意这个IP在下面还会用的到。 3.然后我们需要分配账号给自己使用.sudo vi /etc/ppp/chap-secrets这个是用户列表文件，在里面添加账户按如下格式“username” pptpd “password” * username为你的用户名，password为你的密码，用引号引起,最后的*号表示允许在任意IP连接到服务 4.至此服务弄好了，如果你sudo service pptpd restart一下，就应该已经能连接到该VPN了，但是连接了之后会发现还访问不了外网，如果是需要连接内网的vpn，可以不需要这一步操作。需要让他能访问外网。首先，sudo vi /etc/ppp/pptpd-options找到ms-dns，取消掉注释，改成你喜欢的DNS比如8.8.8.8,8.8.4.4 5.然后我们要开启内核IP转发sudo vi /etc/sysctl.conf 取消掉 net.ipv4.ip_forward=1 这一行的注释，然后执行sudo sysctl -p使修改后的文件配置立即生效。 6.然后我们需要安装iptables，用来实现请求的NAT转发sudo apt-get install iptables 然后开启NAT转发.sudo iptables -F sudo iptables -t nat -A POSTROUTING -s 192.168.150.0/24 -o eth0 -j MASQUERADE sudo iptables -t nat -A POSTROUTING -s 192.168.150.0/24 -o br0 -j MASQUERADE (如果有虚拟网桥br0的话) 192.168.150.0/24是你在上面设置的IP段，让这个段转发，注意eth0是你连接外网的那块网卡，不一定是0也有可能是1或者看你的机器哪块网卡连的外网了。这样就以NAT的方式请求外网的东西了。不知道你的机器哪块网卡连的外网的话ifconfig一下看看哪个网卡是外网IP就知道了。 7.最后，我们需要重启服务，让配置生效 .sudo service pptpd restart 现在你已经可以连接到VPN获取内网150网段的地址了。 参照：https://my.oschina.net/isnail/blog/363151 查看有没有人连接该vpn: netstat -anpt|grep pptpd或者 ifconfig 也能看出有ppp0/ppp1等。 遇到的问题1： CTRL: PTY read or GRE write failed (pty,gre)=(6,7) 解决办法： 因为 /etc/pptpd.conf 配置文件中 logwtmp 参数不兼容导致，需要注释该参数，然后重启PPTPD 服务。 service pptpd restartRef：https://www.linuser.com/forum.php?mod=viewthread&amp;tid=582 遇到的问题2： 怎么查看log信息，或者怎么打开debug模式？ 解决办法： 打开debug模式： 把/etc/pptpd.conf文件的debug前面的注释去掉，然后重启PPTPD服务。 service pptpd restarttailf /var/log/syslog 查看log信息 遇到的问题3： 如何设置开机自动启动： 解决办法： 打开/etc/rc.local文件，在exit 0语句前加入：在exit 0之前加上 sudo iptables -t nat -A POSTROUTING -s 192.168.150.0/24 -o eth0 -j MASQUERADE sudo service pptpd start ====================================================== 客户端怎么连接： 一， ubuntu 16.04： 1、首先安装PPTP-VPN拨号客户端1 sudo apt-get install pptp-linux binutils 2、创建拨号连接1 sudo pptpsetup –create testvpn –server 219.224.167.172 –username user –password pass –encrypt –start 各参数说明： –create 后的是创建的连接名称，可以为任意名称; –server 后接的是vpn服务器的IP; –username 是用户名 –password 是密码，在这也可以没这个参数，命令稍后会自动询问。这样可以保证账号安全 –encrypt 是表示需要加密，不必指定加密方式，命令会读取配置文件中的加密方式 –start 是表示创建连接完后马上连接，如果你不想连，就不写 3、连接或断开VPN12 pon testvpn ＃&lt;– VPN的&ldquo;连接名称”poff ＃&lt;– 断开VPN连接 当连接成功后输入ifconfig命令会出现如下内容： ppp0 Link encap:点对点协议 inet 地址:172.16.36.2 点对点:172.16.36.1 掩码:255.255.255.255 UP POINTOPOINT RUNNING NOARP MULTICAST MTU:1496 跃点数:1 接收数据包:7 错误:0 丢弃:0 过载:0 帧数:0 发送数据包:7 错误:0 丢弃:0 过载:0 载波:0 碰撞:0 发送队列长度:3 接收字节:70 (70.0 B) 发送字节:76 (76.0 B)4、添加网关路由，让流量走VPN 从第3步可以看出，新增虚拟接口为ppp0,VPN的网关为172.16.36.1 在文件/etc/ppp/ip-up加入该网关。（此文件在VPN拨号成功后会默认调用该脚本，所以加在此文件中不需要手动添加） route add default gw 172.16.36.1 也可以在终端手动敲入上面命令：1 sudo route add default gw 172.16.36.1 5、删除原有默认网关路由 通过sudo route命令可以查看本机中已经添加的路由。 找到对应的路由，假设我这里还有一条路由是192.168.0.1使用以下命令进行删除之：1 sudo route del default gw 192.168.0.1 6、试着打开facebook看看 OK了]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>VPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java性能调优：利用VisualVM进行性能分析]]></title>
    <url>%2F2019%2F04%2F01%2FJava%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-%E5%88%A9%E7%94%A8VisualVM%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[JVisualVM 简介VisualVM 是Netbeans的profile子项目，已在JDK6.0 update 7 中自带，能够监控线程，内存情况，查看方法的CPU时间和内存中的对 象，已被GC的对象，反向查看分配的堆栈(如100个String对象分别由哪几个对象分配出来的)。在JDK_HOME/bin(默认是C:\Program Files\Java\jdk1.6.0_13\bin)目录下面，有一个jvisualvm.exe文件，双击打开，从UI上来看，这个软件是基于NetBeans开发的了。 VisualVM 提供了一个可视界面，用于查看 Java 虚拟机 (Java Virtual Machine, JVM) 上运行的基于 Java 技术的应用程序的详细信息。VisualVM 对 Java Development Kit (JDK) 工具所检索的 JVM 软件相关数据进行组织，并通过一种使您可以快速查看有关多个 Java 应用程序的数据的方式提供该信息。您可以查看本地应用程序或远程主机上运行的应用程序的相关数据。此外，还可以捕获有关 JVM 软件实例的数据，并将该数据保存到本地系统，以供后期查看或与其他用户共享。 双击启动 jvisualvm.exe，启动起来后和jconsole 一样同样可以选择本地和远程，如果需要监控远程同样需要配置相关参数。 主界面如下：VisualVM可以根据需要安装不同的插件，每个插件的关注点都不同，有的主要监控GC，有的主要监控内存，有的监控线程等。如何安装： 1、从主菜单中选择“工具”&gt;“插件”。 2、在“可用插件”标签中，选中该插件的“安装”复选框。单击“安装”。 3、逐步完成插件安装程序。 我这里以 Eclipse(pid 22296)为例，双击后直接展开，主界面展示了系统和jvm两大块内容，点击右下方jvm参数和系统属性可以参考详细的参数信息.因为VisualVM的插件太多，我这里主要介绍三个我主要使用几个：监控、线程、Visual GC 监控的主页其实也就是，cpu、内存、类、线程的图表线程和jconsole功能没有太大的区别Visual GC 是常常使用的一个功能，可以明显的看到年轻代、老年代的内存变化，以及gc频率、gc的时间等。以上的功能其实jconsole几乎也有，VisualVM更全面更直观一些，另外VisualVM非常多的其它功能，可以分析dump的内存快照， dump出来的线程快照并且进行分析等，还有其它很多的插件大家可以去探索 案例分析准备模拟内存泄漏样例1、定义静态变量HashMap2、分段循环创建对象，并加入HashMap3、配置jvm参数如下： -Xms512m -Xmx512m -XX:-UseGCOverheadLimit -XX:MaxPermSize=50m 4、运行程序并打卡visualvm监控代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.util.HashMap;import java.util.Map;public class CyclicDependencies &#123; //声明缓存对象 private static final Map map = new HashMap(); public static void main(String args[])&#123; try &#123; Thread.sleep(10000);//给打开visualvm时间 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //循环添加对象到缓存 for(int i=0; i&lt;1000000;i++)&#123; TestMemory t = new TestMemory(); map.put("key"+i,t); &#125; System.out.println("first"); //为dump出堆提供时间 try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; for(int i=0; i&lt;1000000;i++)&#123; TestMemory t = new TestMemory(); map.put("key"+i,t); &#125; System.out.println("second"); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; for(int i=0; i&lt;3000000;i++)&#123; TestMemory t = new TestMemory(); map.put("key"+i,t); &#125; System.out.println("third"); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; for(int i=0; i&lt;4000000;i++)&#123; TestMemory t = new TestMemory(); map.put("key"+i,t); &#125; System.out.println("forth"); try &#123; Thread.sleep(Integer.MAX_VALUE); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("qqqq"); &#125;&#125; 使用JVisualVM分析内存泄漏 1、查看Visual GC标签，内容如下，这是输出first的截图 这是输出forth的截图：通过2张图对比发现：老生代一直在gc，当程序继续运行可以发现老生代gc还在继续：增加到了7次，但是老生代的内存并没有减少。说明存在无法被回收的对象，可能是内存泄漏了。如何分析是那个对象泄漏了呢？打开抽样器标签：点击后如下图：按照程序输出进行堆dump，当输出second时，dump一次，当输出forth时dump一次。进入最后dump出来的堆标签，点击类： 比较结果如下：可以看出在两次间隔时间内TestMemory对象实例一直在增加并且多了，说明该对象引用的方法可能存在内存泄漏。如何查看对象引用关系呢？右键选择类TestMemory，选择“在实例视图中显示”，如下所示：左侧是创建的实例总数，右侧上部为该实例的结构，下面为引用说明，从图中可以看出在类CyclicDependencies里面被引用了，并且被HashMap引用。如此可以确定泄漏的位置，进而根据实际情况进行分析解决。 JVisualVM 远程监控 Tomcat1、修改远程tomcat的catalina.sh配置文件，在其中增加： JAVA_OPTS=&quot;$JAVA_OPTS -Djava.rmi.server.hostname=192.168.122.128 -Dcom.sun.management.jmxremote.port=18999 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false&quot;这次配置先不走权限校验。只是打开jmx端口。2、打开jvisualvm，右键远程，选择添加远程主机： 3、输入主机的名称，直接写ip，如下： 右键新建的主机，选择添加JMX连接，输入在tomcat中配置的端口即可。4、双击打开。完毕！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>VisualVM</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《非暴力沟通》读后感]]></title>
    <url>%2F2019%2F03%2F24%2F%E3%80%8A%E9%9D%9E%E6%9A%B4%E5%8A%9B%E6%B2%9F%E9%80%9A%E3%80%8B%E8%AF%BB%E5%90%8E%E6%84%9F%2F</url>
    <content type="text"><![CDATA[昨天花了三个小时左右把马歇尔·卢森堡著的《非暴力沟通》一书读完了。 不得不说这本书让我意识到如何有愉快的与人沟通并最终解决问题是多么重要且讲究技巧的一件事！ 在现实社会中，当自己的提议或者请求被别人拒绝以后，我们往往会感到恼怒并与对方拉开距离且倾向于拒绝对方的提议或者请求；当然，倘若对方也是这种心理，那么整个事情将会朝着恶性循环的方向发展。比如这样的一件事： 工作一整天的妈妈回到家，看见昨天的垃圾没有倒、碗也没有洗，甚至还没有做饭，想到自己累了一整天回来还要做这么多事，很是生气。 妈妈：小明！你怎么这么懒，整天在家都不知道做饭打扫卫生吗？！你这样下去是找不到女朋友的！ 小明听见妈妈这样说自己很是难过，砰的一声关门进自己屋了：哼！找不到就找不到！ 妈妈看见小明这样，也很难过：我每天工作那么累，回来还要收拾这乱糟糟的东西…故事纯属虚构，如有雷同，纯属巧合 这其实就是我们生活中经常发生的事情：妈妈其实只是想让小明多帮帮自己做些家务，可是到头来却把两个人的关系搞僵了。那如果换一种沟通的方式呢： 妈妈：小明，家里的卫生还没有打扫，让我觉得乱糟糟的，我想打扫一下家里的卫生，你能帮妈妈打扫一下卫生吗？ 小明：Yes,madam!我相信，作为晚辈，自己的长辈这样跟自己说话，没有晚辈会拒绝长辈的请求吧。其实这里就涉及到非暴力沟通的四个要素： 观察：客观的事情，而不是主观的评论 感受：自己的感受，而不是看法 需要：自己想要的是什么，需要清晰的表达出来 请求：提出具体的请求就拿上面的例子来举例分析： 观察：家里的卫生还没有打扫 感受：让我觉得乱糟糟的 需要：我想打扫一下家里的卫生 请求：你能帮妈妈打扫一下卫生吗？这四个要素分别有什么作用呢？观察 一定要基于客观的事实进行表述，不能将自己主观的评论混为一谈。尤其是消极的评论。人们倾向于听到批评，从而产生逆反心理。比如上面的例子中： 你怎么这么懒！ ---- 这是评论，而且是消极的评论 家里的卫生还没有打扫 ---- 这是事实，没有打扫卫生人们肯定倾向于听见消极的评论，从而产生逆反心理；如果说的事实，那人们将倾向于继续听你诉说。感受 基于前面的观察，你有什么感受？郁闷、苦恼还是其他的？只有说出你的感受，人们才会注意你的愿望是什么。拿书中的一个例子： 有一次，一对夫妻一起参加了一个研讨班，期间，太太对先生说：“我觉得我嫁给了一堵墙。”先生的反应真的就像一堵墙：他坐在那里一动不动。太太气坏了，转向我，嚷道：“看！他总是这样。坐在一边，闷声不响。和他过日子，就像对着一堵墙。” “你是不是感到孤单，希望先生多体贴你呢？”我问道。在她表示认同后，我试图说明，“我觉得我嫁给了一堵墙”这种话很难提醒她先生留意她的愿望。一旦认为自己受到了指责，他很可能就会觉得委屈并退缩，这样，双方的关系甚至会更加疏远。需要 有了客观的事实，有了对客观事实的感受，那么你需要什么： 需要儿子打扫卫生？ 需要丈夫多体贴你？ 需要安静的工作环境？你的需要必须清晰，不能模糊： 需要一个干净的家？ 不想跟一堵墙生活？ 不想在工作的时候心情乱糟糟的上面这里需求都是模糊不清晰的，就算你提出了你的需求，但是人们不能理解。这需求不能帮助你达成愿望。请求 你有了你的需求，那么你的请求是什么呢？你想让我做什么能够帮助你呢？对了，你的请求必须是清晰可行的，比如： 你能帮妈妈打扫一下卫生吗？ 平时我说话的时候能集中精力听我说话吗？ 可以在工作的时候不要来打扰我们？记住清晰可行的请求能够告诉对方怎么做才能帮助到你！ 只要在沟通的时候，注意这四个要素，不敢说对方一定会按自己想要的那样去做，但可以肯定不会让对方拒绝跟自己沟通，甚至背道而驰。 与此同时，认真的倾听对方并体会对方此时此刻的感受也是非常重要的。试想一下：如果对方没有认真倾听你在说什么，肯定会对你的倾诉起到消极的影响，进而使得你关闭沟通的大门！ 打破人与人之间阻碍沟通的那堵墙，让对方认真倾听你在说什么并认真考虑你的感受、请求，会让解决问题的成本低很多，甚至会避免暴力，这种沟通方式正可谓：非暴力沟通]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS终端命令]]></title>
    <url>%2F2018%2F12%2F09%2FMacOS%E7%BB%88%E7%AB%AF%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[1.改变Launch Pad 图标显示尺寸 2.让自动隐藏Dock减小延迟defaults write com.apple.Dock autohide-delay -float 0 &amp;&amp; killall Dock恢复:defaults delete com.apple.Dock autohide-delay &amp;&amp; killall Dock 3.显示Finder中的隐藏文件defaults write com.apple.finder AppleShowAllFiles -bool YES &amp;&amp; killall Finder恢复:defaults write com.apple.Finder AppleShowAllFiles FALSE 4.更改屏幕截图保存位置defaults write com.apple.screencapture location ~/Pictures/Screenshots5.更改默认屏幕截图保存类型defaults write com.apple.screencapture type jpg &amp;&amp; killall SystemUIServer6.defaults write com.apple.notificationcenterui bannerTime #改推送通知提示横幅显示持续时间，把#改成想要改的数字，比如10，就是10秒 7.延时（秒）执行命令sleep 10 &amp;&amp; 接指令]]></content>
      <categories>
        <category>MacOS</category>
      </categories>
      <tags>
        <tag>MacOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四大常用元注解]]></title>
    <url>%2F2018%2F11%2F04%2F%E5%9B%9B%E5%A4%A7%E5%B8%B8%E7%94%A8%E5%85%83%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[元注解：用于描述注解的注解@Target： 描述注解能够作用的位置 ElementType取值： TYPE：可以作用于类上 METHOD：可以作用于方法上 FIELD：可以作用于成员变量上 @Retention：描述注解被保留的阶段 @Retention(RetentionPolicy.RUNTIME)：当前被描述的注解，会保留到class字节码文件中，并被JVM读取到 @Documented：描述注解是否被抽取到api文档中 @Inherited：描述注解是否被子类继承]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Redis】Pipeline管道打包处理模式]]></title>
    <url>%2F2018%2F11%2F01%2F%E3%80%90Redis%E3%80%91Pipeline%E7%AE%A1%E9%81%93%E6%89%93%E5%8C%85%E5%A4%84%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Redis本身是一个cs模式的tcp server, client可以通过一个socket连续发起多个请求命令。 每个请求命令发出后client通常会阻塞并等待redis服务端处理，redis服务端处理完后将结果返回给client。 由于我们知道Redis非常快，这种发送模式中的性能瓶颈其实在于请求传输速度，就算redis server端有很强的处理能力，也由于收到的client消息少，而造成吞吐量小。我们可以修改一种处理模式：通过pipeline方式将client端命令一起发出，redis server会处理完多条命令后，将结果一起打包返回client,从而节省大量的网络延迟开销。下面以Java的客户端jedis来测试pipeline的效果。 12345678Pipeline pipeline = jedis.pipelined(); long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000; i++) &#123; pipeline.hset("server", "" + i, "" + i); &#125; List&lt;Object&gt; results = pipeline.execute(); long end = System.currentTimeMillis(); System.out.println("Pipelined SET: " + ((end - start)/1000.0) + " seconds"); 测试的结果采用pipeline方式，效率几乎与mset一样，每秒插入约15万数据，但内存占用仅为mset的1/3.]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【消息队列】几种常见的MQ总结对比]]></title>
    <url>%2F2018%2F10%2F01%2F%E3%80%90%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E3%80%91%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84MQ%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[为什么使用消息队列：解耦、异步、削峰 解耦 看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃…… mq-1 在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！ 如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。 mq-2 总结：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。 再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。 mq-3 一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。 如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！ mq-4削峰 每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。 一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。 但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。 mq-5 如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。 mq-6 这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。消息队列有什么优缺点 优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。 缺点有以下几个： 系统可用性降低 系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，人 ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用，可以点击这里查看。 系统复杂度提高 硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 一致性问题 A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。 如何选择MQ 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 怎么保证消息没有重复消费 1.如果是拿这个消息做数据库insert操作（事实上update和delete重复也不影响）给这个消息做一个唯一主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。 2.当拿到这个消息做redis的set的操作，那就容易了，不用解决，因为你无论set几次结果都是一样的，set操作本来就算幂等操作。 3.如果上面两种情况还不行，准备一个第三方存储,来做消费记录。以redis为例，给消息分配一个全局id，只要消费过该消息，将&lt;id,message&gt;以K-V形式写入redis。那消费者开始消费前，先去redis中查询有没消费记录即可。 怎么处理消息丢失的情况怎么保证消息传递的顺序性怎么保证多系统消息一致性]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eureka和Zookeeper的区别]]></title>
    <url>%2F2018%2F07%2F13%2FEureka%E5%92%8CZookeeper%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Eureka在之前已经操作配置过了，现在告一段落，现在来说说Eureka和Zookeeper的区别 Spring Cloud在现在的版本其实也可以使用Zookeeper来进行服务注册的。 那他们的区别在哪里呢？为什么要使用Eureka来进行服务注册呢？ 1、Zookeeper当master挂了，会在30-120s进行leader选举，这点类似于redis的哨兵机制，在选举期间Zookeeper是不可用的，这么长时间不能进行服务注册，是无法忍受的，别说30s，5s都不能忍受。这时Zookeeper集群会瘫痪，这也是Zookeeper的CP，保持节点的一致性，牺牲了A/高可用。而Eureka不会，即使Eureka有部分挂掉，还有其他节点可以使用的，他们保持平级的关系，只不过信息有可能不一致，这就是AP，牺牲了C/一致性。 2、在之前的文章已经提到过Eureka有自我保护机制（15分钟内超过85%的服务节点没有心跳/down），这点我觉得确实要比Zookeeper好，即使服务不可用，也会保留当前失效的微服务，默认90秒，在这90秒Eureka不会注销微服务，在这90秒内仍然可以接受新的服务注册，只是不会同步到其他节点上。当坏掉的服务恢复的时候，会自动加入到节点上，也是高可用的一种。然后退出自我保护机制，这也是应对网络异常的一种机制 总结：Zookeeper出现网络等故障的时候导致整个服务注册瘫痪太要命了。Eureka能很好的应对网络故障导致失去节点的情况。 ●Eureka：AP架构设计(高可用、分区容错性)，Zookeeper：CP架构设计（一致性、分区容错性） 那什么是AP、什么是CP，这个就是关于分布式的CAP理论: C - Consistent-一致性 A - Availability-可用性 P - Partition tolerance -分区容错性分布式系统之所以叫分布式，是因为提供服务的各个节点分布在不同机器上，相互之间通过网络交互。那么必然存在网络故障断开的风险，这个网络断开的专业场景成为网络分区。网络分区多个分布式节点无法进行通信，我们对于一个节点无法操作到另外一个节点，数据的一致性没办法满足，因为多节点的数据不再保持一致。如果要保持一致，我们必然要牺牲高可用，也就是需要暂停一部分的服务，不提供对数据的操作(修改、更新等)功能，等到网络恢复的时候再对外服务。当然也可以保证高可用，牺牲数据一致性 关于CAP理论大概的结论:在网络分区时，不能同时保证高可用和一致性 目前任何分布式系统都没办法同时满足3个，只能3选其2，分布式系统之所以叫分布式，都是分散在不同的服务器，P是必须要满足的，所以只能选择A或者C了 在很多的公司以及业务场景，很多会选择保持数据的一致性，在京东/双十一这样的情况，只能保证AP，不能保证CP。但也有高可用的.比如某米公司，一次在进行手机抢购的时候，牺牲了数据的一致性，当添加到购物车，然后付款，却发现没有库存了，他保证了服务高可用，但牺牲了数据的一致性 原文：https://blog.csdn.net/ypp91zr/article/details/88257388版权声明：本文为博主原创文章，转载请附上博文链接！]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务隔离级别(图文详解)]]></title>
    <url>%2F2018%2F07%2F02%2F%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB(%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3)%2F</url>
    <content type="text"><![CDATA[事务隔离级别(图文详解)什么是事务?事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 事物的特性(ACID) 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 并发事务带来的问题在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对统一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复度和幻读区别： 不可重复读的重点是修改，幻读的重点在于新增或者删除。 例1（同样的条件, 你读取过的数据, 再次读取出来发现值不一样了 ）：事务1中的A先生读取自己的工资为 1000的操作还没完成，事务2中的B先生就修改了A的工资为2000，导 致A再读自己的工资时工资变为 2000；这就是不可重复读。 例2（同样的条件, 第1次和第2次读出来的记录数不一样 ）：假某工资单表中工资大于3000的有4人，事务1读取了所有工资大于3000的人，共查到4条记录，这时事务2 又插入了一条工资大于3000的记录，事务1再次读取时查到的记录就变为了5条，这样就导致了幻读。 事务隔离级别SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻影读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看,MySQL 8.0 该命令改为SELECT @@transaction_isolation; 123456mysql&gt; SELECT @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| REPEATABLE-READ |+-----------------+ 这里需要注意的是：与 SQL 标准不同的地方在于InnoDB 存储引擎在 REPEATABLE-READ（可重读）事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生，这与其他数据库系统(如 SQL Server)是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读） 已经可以完全保证事务的隔离性要求，即达到了 SQL标准的SERIALIZABLE(可串行化)隔离级别。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容):，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读）并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到SERIALIZABLE(可串行化)隔离级别。 实际情况演示在下面我会使用 2 个命令行mysql ，模拟多线程（多事务）对同一份数据的脏读问题。 MySQL 命令行的默认配置中事务都是自动提交的，即执行SQL语句后就会马上执行 COMMIT 操作。如果要显式地开启一个事务需要使用命令：START TARNSACTION。 我们可以通过下面的命令来设置隔离级别。 1SET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL [READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE] 我们再来看一下我们在下面实际操作中使用到的一些并发控制语句: START TARNSACTION |BEGIN：显式地开启一个事务。 COMMIT：提交事务，使得对数据库做的所有修改成为永久性。 ROLLBACK：回滚会结束用户的事务，并撤销正在进行的所有未提交的修改。 脏读(读未提交) 避免脏读(读已提交) 不可重复读还是刚才上面的读已提交的图，虽然避免了读未提交，但是却出现了，一个事务还没有结束，就发生了 不可重复读问题。 可重复读 防止幻读(可重复读) 一个事务对数据库进行操作，这种操作的范围是数据库的全部行，然后第二个事务也在对这个数据库操作，这种操作可以是插入一行记录或删除一行记录，那么第一个是事务就会觉得自己出现了幻觉，怎么还有没有处理的记录呢? 或者 怎么多处理了一行记录呢? 幻读和不可重复读有些相似之处 ，但是不可重复读的重点是修改，幻读的重点在于新增或者删除。 参考 《MySQL技术内幕：InnoDB存储引擎》 https://dev.mysql.com/doc/refman/5.7/en/ Mysql 锁：灵魂七拷问 Innodb 中的事务隔离级别和锁的关系]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引知识点总结]]></title>
    <url>%2F2018%2F06%2F30%2FMySQL%E7%B4%A2%E5%BC%95%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[面试官：当一条查询执行较慢时通常可以如何进行优化我：加索引！面试官：那么到底什么是索引，其底层又是如何实现的呢我：懵逼！ 索引的常见模型索引的出现是为了提高查询效率，就像书的目录一样 常见的实现索引的模型有：哈希表、有序数组和搜索树哈希表：键 - 值(key - value)。哈希思路：把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置哈希冲突的处理办法：链表哈希表适用场景：只有等值查询的场景有序数组：按顺序存储。查询用二分法就可以快速查询，时间复杂度是：O(log(N))有序数组查询效率高，更新效率低有序数组的适用场景：静态存储引擎。二叉搜索树：每个节点的左儿子小于父节点，父节点又小于右儿子二叉搜索树：查询时间复杂度O(log(N))，更新时间复杂度O(log(N))数据库存储大多不适用二叉树，因为树高过高，会适用N叉树扩展：为什么树高过高就不好呢？树高表示N叉树的层数，首先，层数越高占用空间就越大，同时层数越高表示查找到目标数据所要跳层的次数越大，层与层（实际上是父节点与父节点）之间是通过指针连接的，而两个节点的内存地址是连续的概率很低，因此就会触发磁盘随机读效率较低 当索引有100万数据，树高20（2的20次方） 一次查询可能需要跳跃 20 个数据块 哈希表是一种以键-值（key-value）存储数据的结构，我们只要输入待查找的值即key，就可以找到其对应的值即Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。（与HashMap类似）优点：效率高缺点：因为不是有序的，所以哈希索引做区间查询的速度是很慢的。 你可以设想下，如果你现在要找某字段在[a, b]这个区间的数据，就必须全部扫描一遍了。 所以，哈希表这种结构适用于只有等值查询的场景，不适用于区间查询 有序数组等值查询和范围查询场景中的性能就都非常优秀 搜索树模型又可以细分为二叉树红黑树B+树 索引的实现由存储引擎来决定，InnoDB索引的实现使用B+树模型二叉树和红黑树的搜索效率很高，但是应用在数据库中时因为数据量较大，二叉树和红黑树每次只分裂出两个分支，导致分裂层数很大，空间占用率高而B+树选择增加分支树，把整颗树的高度维持在很小的范围内，同时在内存里缓存前面若干层的节点，可以极大地降低访问磁盘的次数，提高读的效率。同时要注意的一点是：二叉树类数据结构效率高的前提是数据有序，这也是数据库常存在一个自增主键的原因 扩展：什么是B+树 B-树 即Balance-tree即B树 B+树 B+树索引并不能找到一个给定键值的具体行。B+树索引能找到的只是被查找数据行所在的页。然后数据库通过把页读入到内存，再在内存中进行查找，最后得到要在找的数据。因为页目录中的槽是按照主键顺序排列的，所以在每一个页目录中，通过二分查找，定位到数据行所在的页，然后将整个页读入内存扩展：我们可以人工调整页的大小吗？所有的innodb索引都是btree索引，索引记录保存在叶子上，默认的索引页大小是16K。当有新的记录插入时，innodb出于对将来的insert和update操作的考虑，会尝试留下1/16的空闲页大小。 如果索引记录是完全按照索引记录的大小顺序插入的，那么索引也将填满整个页大小的15/16，如果插入顺序完全随机，那么索引页基本上填充为1/2至15/16自建。如果填充因子低于1/2,innodb会尝试重建b-tree。 Mysql5.6以后，可以通过innodb_page_size参数设置当前实例下每个索引页的大小，一旦设定，无法再更改回来。推荐的配置一般是16K，8K或者4K。另外假如一个Mysql实例设置了不同于默认值的innodb_page_size A，那么将无法使用其他不同于A值的实例上的文件（比如做一个物理备份和恢复） B*树 是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针 B树模型小结： B（B-）树：多路搜索树，每个结点存储M/2到M个关键字，非叶子结点存储指向关键字范围的子结点；所有关键字在整颗树中出现，且只出现一次，非叶子结点可以命中； B+树：在B-树基础上，为叶子结点增加链表指针，所有关键字都在叶子结点中出现，非叶子结点作为叶子结点的索引；B+树总是到叶子结点才命中； B*树：在B+树基础上，为非叶子结点也增加链表指针，将结点的最低利用率从1/2提高到2/3； B树和B+树的对比 如图所示，区别有以下两点： B+树中只有叶子节点会带有指向记录的指针（ROWID），而B树则所有节点都带有，在内部节点出现的索引项不会再出现在叶子节点中。 B+树中所有叶子节点都是通过指针连接在一起，而B树不会。 B+树的优点： 非叶子节点不会带上ROWID，这样，一个块中可以容纳更多的索引项，一是可以降低树的高度。二是一个内部节点可以定位更多的叶子节点。 叶子节点之间通过指针来连接，范围扫描将十分简单，而对于B树来说，则需要在叶子节点和内部节点不停的往返移动。 B树的优点： 对于在内部节点的数据，可直接得到，不必根据叶子节点来定位。 说完底层实现我们来说一下表层索引的种类普通索引：仅加速查询 唯一索引：加速查询 + 列值唯一（可以有null） 主键索引：加速查询 + 列值唯一（不可以有null）+ 表中只有一个 联合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并 全文索引：对文本的内容进行分词，进行搜索ps.索引合并，使用多个单列索引组合搜索覆盖索引，select的数据列只用从索引中就能够取得，不必读取数据行，换句话说查询列要被所建的索引覆盖联合索引本质： 当创建(a,b,c)联合索引时，相当于创建了(a)单列索引，(a,b)联合索引以及(a,b,c)联合索引。想要索引生效的话,只能使用 a和a,b和a,b,c三种组合；当然，a,c组合也可以，但实际上只用到了a的索引，c并没有用到！只用bc两个条件不会用到索引。联合索引比对每个列分别建索引更有优势因为索引建立得越多就越占磁盘空间，在更新数据的时候速度会更慢。另外建立多列索引时，顺序也是需要注意的，应该将严格的索引放在前面，这样筛选的力度会更大，效率更高。前缀索引有时候一个字段很长，我们只需要截取前几位或后几位（反转后取前几位）建立索引即可，节约了空间同时也能实现相同的效果索引下推优化没太懂，自行百度 索引失效场景1、如果条件中有or，除非条件中的列全部有索引否则不会使用索引查询2、like查询是以%开头（但是以%结尾却不会失效）3、如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引。（例如where ID = 3 和 where ID = “3”）4、如果mysql估计使用全表扫描要比使用索引快,则不使用索引。（因为server层有优化器）5、索引不会包含有null值的列，只要列中包含有null值都将不会被包含在索引中，复合索引中只要有一列含有null值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为null。6、在列上进行运算将导致索引失效注意点：虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行insert、update和delete。因为更新表时，不仅要保存数据，还要保存一下索引文件。所以对数据量不大，查询率不高更新率高的的列添加索引反而得不偿失。短索引对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个char(255)的列，如果在前10个或20个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。索引回表我们知道，一般查询数据时都是根据索引找到对应的索引id然后拿着索引id找到对应的数据，这个过程称为回表覆盖索引如果执行的语句是select ID from T where k between 3 and 5此时我们要查的数据就是索引，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。 问题：针对上面说的索引失效第四条 那么什么情况下使用索引反而没有全表扫描快？比如你要从一本字典中找到某一个字，使用目录查肯定很快，但是我现在要找到所有拼音a-x开头的字，走全表扫描就是直接翻，走索引需要将每个数据根据目录一层一层的去找反而变慢了。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu环境搭建]]></title>
    <url>%2F2018%2F06%2F07%2Fubuntu%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.重装系统2.设置root用户密码：​ sudo passwd root3.切换 root 用户：​ su root4.允许远程连接root用户： apt-get install vim vim /etc/ssh/sshd_config 添加 PermitRootLogin yes 允许远程连接 skip-grant-tables 跳过远程连接时的权限验证5.安装mysql： sudo apt-get update sudo apt-get install mysql-server6.配置mysql：​ sudo mysql_secure_installation​ （密码安全级别 0-密码 root-是否确认密码 Y-删除匿名用户 Y-是否禁止远程登录 N-是否删除test数据库-Y -是否重新加载权限表Y）7.安装JDK: sudo apt-get update sudo apt-get install openjdk-8-jdk java -version]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么索引能够提升数据查询效率]]></title>
    <url>%2F2018%2F05%2F05%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E7%B4%A2%E5%BC%95%E8%83%BD%E5%A4%9F%E6%8F%90%E5%8D%87%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E6%95%88%E7%8E%87%2F</url>
    <content type="text"><![CDATA[下面是我补充的一些内容 为什么索引能提高查询速度 以下内容整理自： 地址： https://juejin.im/post/5b55b842f265da0f9e589e79 作者 ：Java3y 先从 MySQL 的基本存储结构说起MySQL的基本存储结构是页(记录都存在页里边)： 各个数据页可以组成一个双向链表 每个数据页中的记录又可以组成一个单向链表 每个数据页都会为存储在它里边儿的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录 以其他列(非主键)作为搜索条件：只能从最小记录开始依次遍历单链表中的每条记录。 所以说，如果我们写select * from user where indexname = ‘xxx’这样没有进行任何优化的sql语句，默认会这样做： 定位到记录所在的页：需要遍历双向链表，找到所在的页 从所在的页内中查找相应的记录：由于不是根据主键查询，只能遍历所在页的单链表了 很明显，在数据量很大的情况下这样查找会很慢！这样的时间复杂度为O（n）。 使用索引之后索引做了些什么可以让我们查询加快速度呢？其实就是将无序的数据变成有序(相对)： 要找到id为8的记录简要步骤： 很明显的是：没有用索引我们是需要遍历双向链表来定位对应的页，现在通过 “目录” 就可以很快地定位到对应的页上了！（二分查找，时间复杂度近似为O(logn)） 其实底层结构就是B+树，B+树作为树的一种实现，能够让我们很快地查找出对应的记录。 关于索引其他重要的内容补充 以下内容整理自：《Java工程师修炼之道》 最左前缀原则MySQL中的索引可以以一定顺序引用多列，这种索引叫作联合索引。如User表的name和city加联合索引就是(name,city)，而最左前缀原则指的是，如果查询的时候查询条件精确匹配索引的左边连续一列或几列，则此列就可以被用到。如下： 123select * from user where name=xx and city=xx ; ／／可以命中索引select * from user where name=xx ; // 可以命中索引select * from user where city=xx ; // 无法命中索引 这里需要注意的是，查询的时候如果两个条件都用上了，但是顺序不同，如 city= xx and name ＝xx，那么现在的查询引擎会自动优化为匹配联合索引的顺序，这样是能够命中索引的。 由于最左前缀原则，在创建联合索引时，索引字段的顺序需要考虑字段值去重之后的个数，较多的放前面。ORDER BY子句也遵循此规则。 注意避免冗余索引冗余索引指的是索引的功能相同，能够命中 就肯定能命中 ，那么 就是冗余索引如（name,city ）和（name ）这两个索引就是冗余索引，能够命中后者的查询肯定是能够命中前者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引。 MySQLS.7 版本后，可以通过查询 sys 库的 schema_redundant_indexes 表来查看冗余索引 Mysql如何为表字段添加索引？？？1.添加PRIMARY KEY（主键索引） 1ALTER TABLE `table_name` ADD PRIMARY KEY ( `column` ) 2.添加UNIQUE(唯一索引) 1ALTER TABLE `table_name` ADD UNIQUE ( `column` ) 3.添加INDEX(普通索引) 1ALTER TABLE `table_name` ADD INDEX index_name ( `column` ) 4.添加FULLTEXT(全文索引) 1ALTER TABLE `table_name` ADD FULLTEXT ( `column`) 5.添加多列索引 1ALTER TABLE `table_name` ADD INDEX index_name ( `column1`, `column2`, `column3` ) 参考 《Java工程师修炼之道》 《MySQL高性能书籍_第3版》 https://juejin.im/post/5b55b842f265da0f9e589e79]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见加密算法]]></title>
    <url>%2F2018%2F05%2F05%2F%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[对称加密算法密钥管理：比较难，不适合互联网，一般用于内部系统 安全性：中 加密速度：快好 几个数量级 (软件加解密速度至少快 100 倍，每秒可以加解密数 M 比特 数据)，适合大数据量的加解密处理简介： 对称加密(也叫私钥加密)指加密和解密使用相同密钥的加密算法。有时又叫传统密码算法，就是加密密钥能够从解密密钥中推算出来，同时解密密钥也可以从加密密钥中推算出来。而在大多数的对称算法中，加密密钥和解密密钥是相同的，所以也称这种加密算法为秘密密钥算法或单密钥算法。它要求发送方和接收方在安全通信之前，商定一个密钥。对称算法的安全性依赖于密钥，泄漏密钥就意味着任何人都可以对他们发送或接收的消息解密，所以密钥的保密性对通信性至关重要。 特点： 对称加密算法的特点是算法公开、计算量小、加密速度快、加密效率高。 不足之处是，交易双方都使用同样钥匙，安全性得不到保证。此外，每对用户每次使用对称加密算法时，都需要使用其他人不知道的惟一钥匙，这会使得发收信双方所拥有的钥匙数量呈几何级数增长，密钥管理成为用户的负担。对称加密算法在分布式网络系统上使用较为困难，主要是因为密钥管理困难，使用成本较高。而与公开密钥加密算法比起来，对称加密算法能够提供加密和认证却缺乏了签名功能，使得使用范围有所缩小。在计算机专网系统中广泛使用的对称加密算法有DES和IDEA等。美国国家标准局倡导的AES即将作为新标准取代DES。 具体算法：DES算法，3DES算法，TDEA算法，Blowfish算法，RC5算法，IDEA算法。 原理应用：对称加密算法的优点在于加解密的高速度和使用长密钥时的难破解性。假设两个用户需要使用对称加密方法加密然后交换数据，则用户最少需要2个密钥并交换使用，如果企业内用户有n个，则整个企业共需要n×(n-1) 个密钥，密钥的生成和分发将成为企业信息部门的恶梦。对称加密算法的安全性取决于加密密钥的保存情况，但要求企业中每一个持有密钥的人都保守秘密是不可能的，他们通常会有意无意的把密钥泄漏出去–如果一个用户使用的密钥被入侵者所获得，入侵者便可以读取该用户密钥加密的所有文档，如果整个企业共用一个加密密钥，那整个企业文档的保密性便无从谈起。 非对称加密算法密钥管理：密钥容易管理 安全性：高 加密速度：比较慢，适合 小数据量 加解密或数据签名非对称加密算法是一种密钥的保密方法。 非对称加密算法需要两个密钥:公开密钥(publickey)和私有密钥(privatekey)。公开密钥与私有密钥是一对，如果用公开密钥对数据进行加密，只有用对应的私有密钥才能解密;如果用私有密钥对数据进行加密，那么只有用对应的公开密钥才能解密。因为加密和解密使用的是两个不同的密钥，所以这种算法叫作非对称加密算法。 非对称加密算法实现机密信息交换的基本过程是:甲方生成一对密钥并将其中的一把作为公用密钥向其它方公开;得到该公用密钥的乙方使用该密钥对机密信息进行加密后再发送给甲方;甲方再用自己保存的另一把专用密钥对加密后的信息进行解密。 另一方面，甲方可以使用乙方的公钥对机密信息进行签名后再发送给乙方;乙方再用自己的私匙对数据进行验签。 甲方只能用其专用密钥解密由其公用密钥加密后的任何信息。 非对称加密算法的保密性比较好，它消除了最终用户交换密钥的需要。 非对称密码体制的特点:算法强度复杂、安全性依赖于算法与密钥但是由于其算法复杂，而使得加密解密速度没有对称加密解密的速度快。对称密码体制中只有一种密钥，并且是非公开的，如果要解密就得让对方知道密钥。所以保证其安全性就是保证密钥的安全，而非对称密钥体制有两种密钥，其中一个是公开的，这样就可以不需要像对称密码那样传输对方的密钥了。这样安全性就大了很多。 起源：W.Diffie和M.Hellman 1976年在IEEE Trans.on Information刊物上发表了“ New Direction in Cryptography”文章，提出了“非对称密码体制即公开密钥密码体制”的概念，开创了密码学研究的新方向。主要功能： 非对称加密体系不要求通信双方事先传递密钥或有任何约定就能完成保密通信，并且密钥管理方便，可实现防止假冒和抵赖，因此，更适合网络通信中的保密通信要求。 主要算法： RSA、Elgamal、背包算法、Rabin、HD,ECC（椭圆曲线加密算法）。 使用最广泛的是RSA算法，Elgamal是另一种常用的非对称加密算法。 Elgamal由Taher Elgamal于1985年发明，其基础是DiffieˉHellman密钥交换算法，后者使通信双方能通过公开通信来推导出只有他们知道的秘密密钥值［DiffieˉHellman］。DiffieˉHellman是Whitfield Diffie和Martin Hellman于1976年发明的，被视为第一种 非对称加密算法，DiffieˉHellman 与RSA的不同之处在于，DiffieˉHellman不是加密算法，它只是生成可用作对称密钥的秘密数值。在DiffieˉHellman密钥交换过程中，发送方和接收方分别生成一个秘密的随机数，并根据随机数推导出公开值，然后，双方再交换公开值。DiffieˉHellman算法的基础是具备生成共享密钥的能力。只要交换了公开值，双方就能使用自己的私有数和对方的公开值来生成对称密钥，称为共享密钥，对双方来说，该对称密钥是相同的，可以用于使用对称加密算法加密数据。 与RSA相比，DiffieˉHellman的优势之一是每次交换密钥时都使用一组新值，而使用RSA算法时，如果攻击者获得了私钥，那么他不仅能解密之前截获的消息，还能解密之后的所有消息。然而，RSA可以通过认证（如使用X.509数字证书）来防止中间人攻击，但Diff ieˉHellman在应对中间人攻击时非常脆弱。 与对称加密算法的区别 首先，用于消息解密的密钥值与用于消息加密的密钥值不同; 其次，非对称加密算法比对称加密算法慢数千倍，但在保护通信安全方面，非对称加密算法却具有对称密码难以企及的优势。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>加密算法</tag>
        <tag>对称加密</tag>
        <tag>非对称加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis缓存更新策略]]></title>
    <url>%2F2018%2F05%2F04%2FRedis%E7%BC%93%E5%AD%98%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[在互联网项目开发中，缓存的应用是非常普遍了，缓存可以帮助页面提高加载速度，减少服务器或数据源的负载。 1、为什么需要缓存？一般在项目中，最消耗性能的地方就是后端服务的数据库了。而数据库的读写频率常常都是不均匀分布的，大多情况是读多写少，并且读操作（select）还会有一些复杂的判断条件，比如 like、group、join 等等，这些语法是非常消耗性能的，所有会出现很多的慢查询，因此数据库很容易在读操作的环节遇到瓶颈。那么通过在数据库前面，前置一个缓存服务，就可以有效的吸收不均匀的请求，抵挡流量波峰。另外，如果应用与数据源不在同一个服务器的情况下，中间还会有很多的网络消耗，也会对应用的响应速度有很大影响，如果当前应用对数据实时性的要求不那么强的话，在应用侧加上缓存就能很快速的提升效率。 2、那使用缓存会遇到哪些问题呢？虽然缓存可以提高整体性能，但是它也可能会带来别的问题。例如使用缓存之后，就相当于把数据存放了2份，一份是在数据库中，另一份存放在缓存中。当有新的数据要写入或者旧数据需要更新的时候，如果我们只更新了其中一份数据源，那两边的数据就不一致了，所以这里就存在一个缓存数据与数据库数据如何进行有效且快速的同步问题，才可以保证数据的最终一致性。另外，加上缓存服务其实也引入了系统架构的复杂度，因为还需要额外的关注缓存自身带来的下列问题： 缓存的过期时间问题：设计缓存的过期时间需要非常的有技巧，且必须与业务实际情况相结合。因为如果设计的过期时间太短了，那会导致缓存效果不佳，且还会造成频繁的从数据库中往缓存里写数据。如果缓存设计的过期时间太长了，又会导致内存的浪费。缓存的命中率问题：这也是设计缓存中需要存放哪些数据的很重要一点，如果设计的不好，可能会导致缓存命中率过低，失去缓存效果。一般对于热点数据而言，要保证命中率达到70%以上效果最佳。缓存的穿透/雪崩问题：是指如果缓存服务一旦宕机或全部丢失，那么有可能一瞬间所有的流量都直接打到了后端数据库上，可能会造成连锁反应，瞬间的请求高峰极有可能导致数据库无法承载。 3、缓存的更新策略具体有哪些？典型的缓存模式，一般有如下几种： Cache AsideRead/Write ThroughWrite Behind 每种模式都有不同的特点，适应与不同的项目场景，下面来依次看看： Cache Aside 模式这是大家经常用到的一种策略模式。这种模式主要流程如下： 应用在查询数据的时候，先从缓存Cache中读取数据，如果缓存中没有，则再从数据库中读取数据，得到数据库的数据之后，将这个数据也放到缓存Cache中。如果应用要更新某个数据，也是先去更新数据库中的数据，更新完成之后，则通过指令让缓存Cache中的数据失效。 这里为什么不让更新操作在写完数据库之后，紧接着去把缓存Cache中的数据也修改了呢？主要是因为这样做的话，就有2个写操作的事件了，担心在并发的情况下会导致脏数据，举个例子：假如同时有2个请求，请求A和请求B，并发的执行。请求A是要去读数据，请求B是要去更新数据。初始状态缓存中是没有数据的，当请求A读到数据之后，准备往回写的时候，此刻，请求B正好要更新数据，更新完了数据库之后，又去把缓存更新了，那请求A再往缓存中写的就是旧数据了，属于脏数据。那么 Cache Aside 模式就没有脏数据问题了吗？不是的，在极端情况下也可能会产生脏数据，比如：假如同时有2个请求，请求A和请求B，并发的执行。请求A是要去读数据，请求B是要去写数据。假如初始状态缓存中没有这个数据，那请求A发现缓存中没有数据，就会去数据库中读数据，读到了数据准备写回缓存中，就在这个时候，请求B是要去写数据的，请求B在写完数据库的数据之后，又去设置了缓存失效。这个时候，请求A由于在数据库中读到了之前的旧数据，开始往缓存中写数据了，此时写进入的就也是旧数据。那么最终就会导致，缓存中的数据与数据库的数据不一致，造成了脏数据。不过这种概率比上面一种概率要小很多。所以整体而言 Cache Aside 模式 还是一种比较简单实用的方式。 Read/Write Through 模式这个模式其实就是将 缓存服务 作为主要的存储，应用的所有读写请求都是直接与缓存服务打交道，而不管最后端的数据库了，数据库的数据由缓存服务来维护和更新。不过缓存中数据变更的时候是同步去更新数据库的，在应用的眼中只有缓存服务。流程就相当简单了： 应用要读数据和更新数据都直接访问缓存服务缓存服务同步的将数据更新到数据库 这个模式出现脏数据的概率就比较低，但是就强依赖缓存了，对缓存服务的稳定性有较大要求，另外，增加新缓存节点时还会有初始状态空数据问题。 Write Behind 模式 这个模式就是 Read/Write Through 模式 的一个变种。区别就是 Read/Write Through 模式的缓存写数据库的时候是同步的，而 Write Behind 模式 的缓存操作数据库是异步的。流程如下： 应用要读数据和更新数据都直接访问缓存服务缓存服务异步的将数据更新到数据库（通过异步任务） 这个模式的特点就是速度很快，效率会非常高，但是数据的一致性比较差，还可能会有数据的丢失情况，实现逻辑也较为复杂。以上就是目前三种主流的缓存更新策略，另外还有Refrsh-Ahead模式等由于使用的不是很常见就不详细介绍了。缓存是互联网项目中非常普遍的一个提高效率的方案，用法比较多，也比较关键，大家可以一起交流。 作者：不止思考链接：https://www.jianshu.com/p/22c7e9ab5d15来源：简书简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。]]></content>
      <categories>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>Redis</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL高性能优化规范]]></title>
    <url>%2F2018%2F05%2F01%2FMySQL%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E8%A7%84%E8%8C%83%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[数据库命令规范 所有数据库对象名称必须使用小写字母并用下划线分割 所有数据库对象名称禁止使用 MySQL 保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来） 数据库对象的命名要能做到见名识意，并且最后不要超过 32 个字符 临时库表必须以 tmp_为前缀并以日期为后缀，备份表必须以 bak_为前缀并以日期 (时间戳) 为后缀 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低） 数据库基本设计规范1. 所有表必须使用 Innodb 存储引擎没有特殊要求（即 Innodb 无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用 Innodb 存储引擎（MySQL5.5 之前默认使用 Myisam，5.6 以后默认的为 Innodb）。 Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。 2. 数据库和表的字符集统一使用 UTF8兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储 emoji 表情的需要，字符集需要采用 utf8mb4 字符集。 3. 所有表和字段都需要添加注释使用 comment 从句添加表和列的备注，从一开始就进行数据字典的维护 4. 尽量控制单表数据量的大小,建议控制在 500 万以内。500 万并不是 MySQL 数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。 可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小 5. 谨慎使用 MySQL 分区表分区表在物理上表现为多个文件，在逻辑上表现为一个表； 谨慎选择分区键，跨分区查询效率可能更低； 建议采用物理分表的方式管理大数据。 6.尽量做到冷热数据分离,减小表的宽度 MySQL 限制每个表最多存储 4096 列，并且每一行数据的大小不能超过 65535 字节。 减少磁盘 IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的 IO）； 更有效的利用缓存，避免读入无用的冷数据； 经常一起使用的列放到一个表中（避免更多的关联操作）。 7. 禁止在表中建立预留字段预留字段的命名很难做到见名识义。 预留字段无法确认存储的数据类型，所以无法选择合适的类型。 对预留字段类型的修改，会对表进行锁定。 8. 禁止在数据库中存储图片,文件等大的二进制数据通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机 IO 操作，文件很大时，IO 操作很耗时。 通常存储于文件服务器，数据库只存储文件地址信息 9. 禁止在线上做数据库压力测试10. 禁止从开发环境,测试环境直接连接生产环境数据库 数据库字段设计规范1. 优先选择符合存储需要的最小的数据类型原因： 列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多，索引的性能也就越差。 方法： a.将字符串转换成数字类型存储,如:将 IP 地址转换成整形数据 MySQL 提供了两个方法来处理 ip 地址 inet_aton 把 ip 转为无符号整型 (4-8 位) inet_ntoa 把整型的 ip 转为地址 插入数据前，先用 inet_aton 把 ip 地址转为整型，可以节省空间，显示数据时，使用 inet_ntoa 把整型的 ip 地址转为地址显示即可。 b.对于非负型的数据 (如自增 ID,整型 IP) 来说,要优先使用无符号整型来存储 原因： 无符号相对于有符号可以多出一倍的存储空间 12SIGNED INT -2147483648~2147483647UNSIGNED INT 0~4294967295 VARCHAR(N) 中的 N 代表的是字符数，而不是字节数，使用 UTF8 存储 255 个汉字 Varchar(255)=765 个字节。过大的长度会消耗更多的内存。 2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中 MySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。 如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select * 而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。 2、TEXT 或 BLOB 类型只能使用前缀索引 因为MySQL 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的 3. 避免使用 ENUM 类型修改 ENUM 值需要使用 ALTER 语句 ENUM 类型的 ORDER BY 操作效率低，需要额外操作 禁止使用数值作为 ENUM 的枚举值 4. 尽可能把所有列定义为 NOT NULL原因： 索引 NULL 列需要额外的空间来保存，所以要占用更多的空间 进行比较和计算时要对 NULL 值做特别的处理 5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07 TIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高 超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储 经常会有人用字符串存储日期型的数据（不正确的做法） 缺点 1：无法用日期函数进行计算和比较 缺点 2：用字符串存储日期要占用更多的空间 6. 同财务相关的金额类数据必须使用 decimal 类型 非精准浮点：float,double 精准浮点：decimal Decimal 类型为精准浮点数，在计算时不会丢失精度 占用空间由定义的宽度决定，每 4 个字节可以存储 9 位数字，并且小数点要占用一个字节 可用于存储比 bigint 更大的整型数据 索引设计规范1. 限制每张表上的索引数量,建议单张表索引不超过 5 个索引并不是越多越好！索引可以提高效率同样可以降低效率。 索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。 因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。 2. 禁止给表中的每一列都建立单独的索引5.6 版本之前，一个 sql 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。 3. 每个 Innodb 表必须有个主键Innodb 是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。 Innodb 是按照主键索引的顺序来组织表的 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引） 不要使用 UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长） 主键建议使用自增 ID 值 4. 常见索引列建议 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好 多表 join 的关联列 5.如何选择索引列的顺序建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引） 6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） 重复索引示例：primary key(id)、index(id)、unique index(id) 冗余索引示例：index(a,b,c)、index(a,b)、index(a) 7. 对于频繁的查询优先考虑使用覆盖索引 覆盖索引：就是包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引 覆盖索引的好处： 避免 Innodb 表进行索引的二次查询: Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。 8.索引 SET 规范尽量避免使用外键约束 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引 外键可用于保证数据的参照完整性，但建议在业务端实现 外键会影响父表和子表的写操作从而降低性能 数据库 SQL 开发规范1. 建议使用预编译语句进行数据库操作预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题。 只传参数，比传递 SQL 语句更高效。 相同语句可以一次解析，多次使用，提高处理效率。 2. 避免数据类型的隐式转换隐式转换会导致索引失效如: 1select name,phone from customer where id = &apos;111&apos;; 3. 充分利用表上已经存在的索引避免使用双%号的查询条件。如：a like &#39;%123%&#39;，（如果无前置%,只有后置%，是可以用到列上的索引的） 一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。 在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧，使用 left join 或 not exists 来优化 not in 操作，因为 not in 也通常会使用索引失效。 4. 数据库设计时，应该要对以后扩展进行考虑5. 程序连接不同的数据库使用不同的账号，禁止跨库查询 为数据库迁移和分库分表留出余地 降低业务耦合度 避免权限过大而产生的安全风险 6. 禁止使用 SELECT * 必须使用 SELECT &lt;字段列表&gt; 查询原因： 消耗更多的 CPU 和 IO 以网络带宽资源 无法使用覆盖索引 可减少表结构变更带来的影响 7. 禁止使用不含字段列表的 INSERT 语句如： 1insert into values (&apos;a&apos;,&apos;b&apos;,&apos;c&apos;); 应使用： 1insert into t(c1,c2,c3) values (&apos;a&apos;,&apos;b&apos;,&apos;c&apos;); 8. 避免使用子查询，可以把子查询优化为 join 操作通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。 子查询性能差的原因： 子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。 9. 避免使用 JOIN 关联太多的表对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。 在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。 如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。 同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。 10. 减少同数据库的交互次数数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。 11. 对应同一列进行 or 判断时，使用 in 代替 orin 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。 12. 禁止使用 order by rand() 进行随机排序order by rand() 会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的 CPU 和 IO 及内存资源。 推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。 13. WHERE 从句中禁止对列进行函数转换和计算对列进行函数转换或计算时会导致无法使用索引 不推荐： 1where date(create_time)=&apos;20190101&apos; 推荐： 1where create_time &gt;= &apos;20190101&apos; and create_time &lt; &apos;20190102&apos; 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作 UNION ALL 不会再对结果集进行去重操作 15. 拆分复杂的大 SQL 为多个小 SQL 大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL MySQL 中，一个 SQL 只能使用一个 CPU 进行计算 SQL 拆分后可以通过并行执行来提高处理效率 数据库操作行为规范1. 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作大批量操作可能会造成严重的主从延迟 主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间，而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况 binlog 日志为 row 格式时会产生大量的日志 大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因 避免产生大事务操作 大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。 特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批 2. 对于大表使用 pt-online-schema-change 修改表结构 避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。 pt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个 DDL 操作，分解成多个小的批次进行。 3. 禁止为程序使用的账号赋予 super 权限 当达到最大连接数限制时，还运行 1 个有 super 权限的用户连接 super 权限只能留给 DBA 处理问题的账号使用 4. 对于程序连接数据库账号,遵循权限最小原则 程序使用数据库账号只能在一个 DB 下使用，不准跨库 程序使用的账号原则上不准有 drop 权限]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WEB端 支付宝、微信、银联支付]]></title>
    <url>%2F2018%2F03%2F03%2FWEB%E7%AB%AF%20%E6%94%AF%E4%BB%98%E5%AE%9D%E3%80%81%E5%BE%AE%E4%BF%A1%E3%80%81%E9%93%B6%E8%81%94%E6%94%AF%E4%BB%98%2F</url>
    <content type="text"><![CDATA[支付宝接入1、开发前准备：申请一个通过实名认证的企业支付宝账号，并申请开通手机WAP支付功能。 2、流程 参数准备： 企业支付宝账号的PID(也叫ParnerID)和KEY，如果使用RSA签名而不是MD5的话，还要把RSA私钥准备好支付时用户看到的东西：商品名称(subject)、支付总额（total_fee）、购买数量（通常都是1吧）交易后的跳转地址，交易成功后用户可以手工点击，或页面延迟自动跳转到这个地址(return_url)交易状态异步通知地址，交易成功或交易关闭会把消息POST到这个地址(notify_url) 流程图： 流程解释： 用户点击购买按钮（或其他形式），向网站发起购买请求 网站创建订单，指派一个唯一订单号 网站把订单号、企业支付宝账号、交易金额、数量等信息，用私钥签名发送给支付宝 支付宝创建一个交易订单，返回一个交易令牌(token) 网站按照指定要求，用token和自己的私钥，构造一个重定向得到支付地址 网站把重定向地址返回给浏览器 浏览器自动重定向到该地址，即包含了token、网站签名的支付宝交易页面 支付宝显示当前交易金额、数量、卖家等信息 用户用自己的支付宝账号支付这笔金额 支付宝把用户支付成功（或失败）这个消息和订单号加上支付宝的签名，使用HTTP POST的方式通知网站（失败的话，会隔段时间重新发送） 网站处理交易后续逻辑（发货、订单状态存储之类的） 网站返回&quot;success&quot;字符串给支付宝，表示该通知已经处理，不用再重发 支付宝显示支付成功页面给用户（这一步和第10步是不分先后发生的） 支付成功页面延迟自动跳转，或用户点击“返回商户页面”，跳转到网站的支付结束页面（此时不一定成功处理支付宝发来的通知），但会在URL带上当前的订单号和状态。3、总结：整个流程主要分三步：一是申请支付宝交易号（获取token），这一步可以理解为，让支付宝验证网站的有效性、让网站指定该交易要支付多少钱 二是用户到支付宝页面付款，这一步可以理解为，让支付宝验证用户有效性，让用户在一个不受网站监视的环境下进行支付 三是用户付款后，处理结果页面告诉用户支付成功（同步通知），另外异步通知网站服务器该订单已支付。 微信支付接入1、开发前准备：商户在微信公众平台(申请扫码支付、公众号支付)或开放平台(申请APP支付)按照相应提示，申请相应微信支付模式。微信支付工作人员审核资料无误后开通相应的微信支付权限。微信支付申请审核通过后，商户在申请资料填写的邮箱中收取到由微信支付小助手发送的邮件，此邮件包含开发时需要使用的支付账户信息。 2、流程： 流程解释 1、用户在商户侧完成下单，使用微信支付进行支付 2、由商户后台向微信支付发起下单请求（调用统一下单接口）注：交易类型trade_type=MWEB 3、统一下单接口返回支付相关参数给商户后台，如支付跳转url（参数名“mweb_url”），商户通过mweb_url调起微信支付中间页 4、中间页进行H5权限的校验，安全性检查。 5、如支付成功，商户后台会接收到微信侧的异步通知 6、用户在微信支付收银台完成支付或取消支付,返回商户页面（默认为返回支付发起页面） 7、商户在展示页面，引导用户主动发起支付结果的查询 8,9、商户后台判断是否接到收微信侧的支付结果通知，如没有，后台调用我们的订单查询接口确认订单状态 10、展示最终的订单支付结果给用户银联接入1、开发前准备：商户与银联签约并支付费用，审核通过之后获取商户ID和秘钥、下载银联接入Demo。 2、流程图： 流程图解释：]]></content>
      <categories>
        <category>支付</category>
      </categories>
      <tags>
        <tag>支付</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java四种常用线程池]]></title>
    <url>%2F2018%2F03%2F01%2FJava%E5%9B%9B%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[创建线程池时通过设置线程池各项参数可以创建性能各异的线程池 1234567new ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 但官方也给我们设置好了四种常用的线程池，可以直接调用 Java通过Executors提供四种线程池，分别为： newCachedThreadPool 可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 1. newCachedThreadPool 可缓存线程池创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。示例代码如下： 123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; try &#123; Thread.sleep(index * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; cachedThreadPool.execute(new Runnable() &#123; public void run() &#123; System.out.println(index); &#125; &#125;); &#125; &#125; &#125; 线程池为无限大，当执行第二个任务时第一个任务已经完成，会复用执行第一个任务的线程，而不用每次新建线程。 2. newFixedThreadPool 定长线程池创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。示例代码如下： 123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService fixedThreadPool = Executors.newFixedThreadPool(3); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; fixedThreadPool.execute(new Runnable() &#123; public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; &#125; 因为线程池大小为3，每个任务输出index后sleep 2秒，所以每两秒打印3个数字。定长线程池的大小最好根据系统资源进行设置。如Runtime.getRuntime().availableProcessors() 3. newScheduledThreadPool 定时线程池创建一个定长线程池，支持定时及周期性任务执行。延迟执行示例代码如下： 1234567891011121314151617181920212223242526272829package test; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.schedule(new Runnable() &#123; public void run() &#123; System.out.println("延迟3秒执行"); &#125; &#125;, 3, TimeUnit.SECONDS); &#125; &#125; package test; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.scheduleAtFixedRate(new Runnable() &#123; public void run() &#123; System.out.println("延迟1秒后每3秒执行一次"); &#125; &#125;, 1, 3, TimeUnit.SECONDS); &#125; &#125; 4. newSingleThreadExecutor 单线程线程池创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。示例代码如下： 123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; singleThreadExecutor.execute(new Runnable() &#123; public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; &#125;]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【消息队列】RabbitMQ如何处理消息丢失]]></title>
    <url>%2F2018%2F03%2F01%2F%E3%80%90%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E3%80%91RabbitMQ%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1%2F</url>
    <content type="text"><![CDATA[首先明确一点 一条消息的传送流程：生产者-&gt;MQ-&gt;消费者 所以有三个地方都会丢失数据： 生产者发送给MQ的途中出现网络问题 MQ自己没保管好弄丢了 消费者拿到数据后出错了没有最终完成任务依次分析 1）生产者弄丢了数据 生产者将数据发送到rabbitmq的时候，可能因为网络问题导致数据就在半路给搞丢了。 1.使用事务（性能差）可以选择用rabbitmq提供的事务功能，在生产者发送数据之前开启rabbitmq事务（channel.txSelect），然后发送消息，如果消息没有成功被rabbitmq接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。但是问题是，开始rabbitmq事务机制，基本上吞吐量会下来，因为太耗性能。 2.发送回执确认（推荐）可以开启confirm模式，在生产者那里设置开启confirm模式之后，你每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会给你回传一个ack消息，告诉你说这个消息ok了。如果rabbitmq没能处理这个消息，会回调你一个nack接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和cnofirm机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调你一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用confirm机制的。 2）RabbitMQ弄丢了数据-开启RabbitMQ的数据持久化 为了防止rabbitmq自己弄丢了数据，这个你必须开启rabbitmq的持久化，就是消息写入之后会持久化到磁盘，哪怕是rabbitmq自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，rabbitmq还没持久化，自己就挂了，可能导致少量数据会丢失的，但是这个概率较小。 设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。 而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。 若生产者那边的confirm机制未开启的情况下，哪怕是你给rabbitmq开启了持久化机制，也有一种可能，就是这个消息写到了rabbitmq中，但是还没来得及持久化到磁盘上，结果不巧，此时rabbitmq挂了，就会导致内存里的一点点数据会丢失。 3）消费端弄丢了数据 主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了比如重启了，那么就尴尬了，RabbitMQ认为你都消费了，这数据就丢了。或者消费者拿到数据之后挂了，这时候需要MQ重新指派另一个消费者去执行任务（一块肉，刚用筷子夹起来，发地震抖了一下，肉掉了） 这个时候得用RabbitMQ提供的ack机制，也是一种处理完成发送回执确认的机制。如果MQ等待一段时间后你没有发送过来处理完成 那么RabbitMQ就认为你还没处理完，这个时候RabbitMQ会把这个消费分配给别的consumer去处理，消息是不会丢的。]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo 配置文件详解]]></title>
    <url>%2F2018%2F02%2F21%2Fdubbo%20%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[dubbo 配置文件详解一、dubbo常用配置12345678910111213141516171819202122&lt;dubbo:service/&gt; 服务配置，用于暴露一个服务，定义服务的元信息，一个服务可以用多个协议暴露，一个服务也可以注册到多个注册中心。eg、&lt;dubbo:service ref="demoService" interface="com.unj.dubbotest.provider.DemoService" /&gt;&lt;dubbo:reference/&gt; 引用服务配置，用于创建一个远程服务代理，一个引用可以指向多个注册中心。eg、&lt;dubbo:reference id="demoService" interface="com.unj.dubbotest.provider.DemoService" /&gt;&lt;dubbo:protocol/&gt; 协议配置，用于配置提供服务的协议信息，协议由提供方指定，消费方被动接受。eg、&lt;dubbo:protocol name="dubbo" port="20880" /&gt;&lt;dubbo:application/&gt; 应用配置，用于配置当前应用信息，不管该应用是提供者还是消费者。eg、&lt;dubbo:application name="xixi_provider" /&gt; &lt;dubbo:application name="hehe_consumer" /&gt;&lt;dubbo:module/&gt; 模块配置，用于配置当前模块信息，可选。&lt;dubbo:registry/&gt; 注册中心配置，用于配置连接注册中心相关信息。eg、&lt;dubbo:registry address="zookeeper://192.168.2.249:2181" /&gt;&lt;dubbo:monitor/&gt; 监控中心配置，用于配置连接监控中心相关信息，可选。&lt;dubbo:provider/&gt; 提供方的缺省值，当ProtocolConfig和ServiceConfig某属性没有配置时，采用此缺省值，可选。&lt;dubbo:consumer/&gt; 消费方缺省配置，当ReferenceConfig某属性没有配置时，采用此缺省值，可选。&lt;dubbo:method/&gt; 方法配置，用于ServiceConfig和ReferenceConfig指定方法级的配置信息。&lt;dubbo:argument/&gt; 用于指定方法参数配置。 二、服务调用超时设置上图中以timeout为例，显示了配置的查找顺序，其它retries, loadbalance, actives也类似。方法级优先，接口级次之，全局配置再次之。如果级别一样，则消费方优先，提供方次之。 其中，服务提供方配置，通过URL经由注册中心传递给消费方。建议由服务提供方设置超时，因为一个方法需要执行多长时间，服务提供方更清楚，如果一个消费方同时引用多个服务，就不需要关心每个服务的超时设置。理论上ReferenceConfig的非服务标识配置，在ConsumerConfig，ServiceConfig, ProviderConfig均可以缺省配置。 三、启动时检查Dubbo缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止Spring初始化完成，以便上线时，能及早发现问题，默认check=true。 如果你的Spring容器是懒加载的，或者通过API编程延迟引用服务，请关闭check，否则服务临时不可用时，会抛出异常，拿到null引用，如果check=false，总是会返回引用，当服务恢复时，能自动连上。 可以通过check=”false”关闭检查，比如，测试时，有些服务不关心，或者出现了循环依赖，必须有一方先启动。 123456789101112131415161、关闭某个服务的启动时检查：(没有提供者时报错)&lt;dubbo:reference interface="com.foo.BarService" check="false" /&gt;2、关闭所有服务的启动时检查：(没有提供者时报错) 写在定义服务消费者一方&lt;dubbo:consumer check="false" /&gt;3、关闭注册中心启动时检查：(注册订阅失败时报错)&lt;dubbo:registry check="false" /&gt;引用缺省是延迟初始化的，只有引用被注入到其它Bean，或被getBean()获取，才会初始化。如果需要饥饿加载，即没有人引用也立即生成动态代理，可以配置：&lt;dubbo:reference interface="com.foo.BarService" init="true" /&gt; 四、订阅1、问题为方便开发测试，经常会在线下共用一个所有服务可用的注册中心，这时，如果一个正在开发中的服务提供者注册，可能会影响消费者不能正常运行。 2、解决方案可以让服务提供者开发方，只订阅服务(开发的服务可能依赖其它服务)，而不注册正在开发的服务，通过直连测试正在开发的服务。 禁用注册配置： 1234&lt;dubbo:registry address="10.20.153.10:9090" register="false" /&gt;或者：&lt;dubbo:registry address="10.20.153.10:9090?register=false" /&gt; 五、回声测试(测试服务是否可用)回声测试用于检测服务是否可用，回声测试按照正常请求流程执行，能够测试整个调用是否通畅，可用于监控。所有服务自动实现EchoService接口，只需将任意服务引用强制转型为EchoService，即可使用。 1234567eg、&lt;dubbo:reference id="memberService" interface="com.xxx.MemberService" /&gt;MemberService memberService = ctx.getBean("memberService"); // 远程服务引用EchoService echoService = (EchoService) memberService; // 强制转型为EchoServiceString status = echoService.$echo("OK"); // 回声测试可用性assert(status.equals("OK")) 六、延迟连接延迟连接，用于减少长连接数，当有调用发起时，再创建长连接。只对使用长连接的dubbo协议生效。 1&lt;dubbo:protocol name="dubbo" lazy="true" /&gt; 七、令牌验证防止消费者绕过注册中心访问提供者，在注册中心控制权限，以决定要不要下发令牌给消费者，注册中心可灵活改变授权方式，而不需修改或升级提供者复制代码 12345678910111213141516171819201、全局设置开启令牌验证：&lt;!--随机token令牌，使用UUID生成--&gt;&lt;dubbo:provider interface="com.foo.BarService" token="true" /&gt;&lt;!--固定token令牌，相当于密码--&gt;&lt;dubbo:provider interface="com.foo.BarService" token="123456" /&gt;2、服务级别设置开启令牌验证：&lt;!--随机token令牌，使用UUID生成--&gt;&lt;dubbo:service interface="com.foo.BarService" token="true" /&gt;&lt;!--固定token令牌，相当于密码--&gt;&lt;dubbo:service interface="com.foo.BarService" token="123456" /&gt;3、协议级别设置开启令牌验证：&lt;!--随机token令牌，使用UUID生成--&gt;&lt;dubbo:protocol name="dubbo" token="true" /&gt;&lt;!--固定token令牌，相当于密码--&gt;&lt;dubbo:protocol name="dubbo" token="123456" /&gt; 八、日志适配1234567891011121314缺省自动查找：log4j、slf4j、jcl、jdk可以通过以下方式配置日志输出策略：dubbo:application logger="log4j"/&gt;访问日志：如果你想记录每一次请求信息，可开启访问日志，类似于apache的访问日志。此日志量比较大，请注意磁盘容量。将访问日志输出到当前应用的log4j日志：&lt;dubbo:protocol accesslog="true" /&gt;将访问日志输出到指定文件：&lt;dubbo:protocol accesslog="http://10.20.160.198/wiki/display/dubbo/foo/bar.log" /&gt; 九、配置Dubbo缓存文件 配置方法如下： 1&lt;dubbo:registryfile=”$&#123;user.home&#125;/output/dubbo.cache” /&gt; 注意：文件的路径，应用可以根据需要调整，保证这个文件不会在发布过程中被清除。如果有多个应用进程注意不要使用同一个文件，避免内容被覆盖。 这个文件会缓存：注册中心的列表服务提供者列表 有了这项配置后，当应用重启过程中，Dubbo注册中心不可用时则应用会从这个缓存文件读取服务提供者列表的信息，进一步保证应用可靠性。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[过滤器与拦截器的区别]]></title>
    <url>%2F2018%2F02%2F11%2F%E8%BF%87%E6%BB%A4%E5%99%A8%E4%B8%8E%E6%8B%A6%E6%88%AA%E5%99%A8%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[-----《Spring源码深度解析》]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>过滤器</tag>
        <tag>拦截器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis知识点总结]]></title>
    <url>%2F2018%2F02%2F05%2FRedis%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[简单来说 redis 就是一个数据库，不过与传统数据库不同的是 redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向。另外，redis 也经常用来做分布式锁。redis 提供了多种数据类型来支持不同的业务场景。除此之外，redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。 为什么要用 redis/为什么要用缓存主要从“高性能”和“高并发”这两点来看待这个问题。 高性能： 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发： 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 为什么要用 redis 而不用 map/guava 做缓存? 下面的内容来自 segmentfault 一位网友的提问，地址：https://segmentfault.com/q/1010000009106416 缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。 redis 和 memcached 的区别对于 redis 和 memcached 我总结了下面四点。现在公司一般都是用 redis 来实现缓存，而且 redis 自身也越来越强大了！ redis支持更丰富的数据类型（支持更复杂的应用场景）：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。memcache支持简单的数据类型，String。 Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而Memecache把数据全部存在内存之中。 集群模式：memcached没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 redis 目前是原生支持 cluster 模式的. Memcached是多线程，非阻塞IO复用的网络模型；Redis使用单线程的多路 IO 复用模型。 来自网络上的一张图，这里分享给大家！ redis 常见数据结构以及使用场景分析1.String 常用命令: set,get,decr,incr,mget 等。 String数据结构是简单的key-value类型，value其实不仅可以是String，也可以是数字。常规key-value缓存应用；常规计数：微博数，粉丝数等。 2.Hash 常用命令： hget,hset,hgetall 等。 hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。比如下面我就用 hash 类型存放了我本人的一些信息： 1234567key=JavaUser293847value=&#123; “id”: 1, “name”: “SnailClimb”, “age”: 22, “location”: “Wuhan, Hubei”&#125; 3.List 常用命令: lpush,rpush,lpop,rpop,lrange等 list 就是链表，Redis list 的应用场景非常多，也是Redis最重要的数据结构之一，比如微博的关注列表，粉丝列表，消息列表等功能都可以用Redis的 list 结构来实现。 Redis list 的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。 另外可以通过 lrange 命令，就是从某个元素开始读取多少个元素，可以基于 list 实现分页查询，这个很棒的一个功能，基于 redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西（一页一页的往下走），性能高。 4.Set 常用命令：sadd,spop,smembers,sunion 等 set 对外提供的功能与list类似是一个列表的功能，特殊之处在于 set 是可以自动排重的。 当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。 比如：在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程，具体命令如下： 1sinterstore key1 key2 key3 将交集存在key1内 5.Sorted Set 常用命令： zadd,zrange,zrem,zcard等 和set相比，sorted set增加了一个权重参数score，使得集合中的元素能够按score进行有序排列。 举例： 在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息，适合使用 Redis 中的 Sorted Set 结构进行存储。 redis 设置过期时间Redis中有个设置时间过期的功能，即对存储在 redis 数据库中的值可以设置一个过期时间。作为一个缓存数据库，这是非常实用的。如我们一般项目中的 token 或者一些登录信息，尤其是短信验证码都是有时间限制的，按照传统的数据库处理方式，一般都是自己判断过期，这样无疑会严重影响项目性能。 我们 set key 的时候，都可以给一个 expire time，就是过期时间，通过过期时间我们可以指定这个 key 可以存活的时间。 如果假设你设置了一批 key 只能存活1个小时，那么接下来1小时后，redis是怎么对这批key进行删除的？ 定期删除+惰性删除。 通过名字大概就能猜出这两个删除方式的意思了。 定期删除：redis默认是每隔 100ms 就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除。注意这里是随机抽取的。为什么要随机呢？你想一想假如 redis 存了几十万个 key ，每隔100ms就遍历所有的设置过期时间的 key 的话，就会给 CPU 带来很大的负载！ 惰性删除 ：定期删除可能会导致很多过期 key 到了时间并没有被删除掉。所以就有了惰性删除。假如你的过期 key，靠定期删除没有被删除掉，还停留在内存里，除非你的系统去查一下那个 key，才会被redis给删除掉。这就是所谓的惰性删除，也是够懒的哈！ 但是仅仅通过设置过期时间还是有问题的。我们想一下：如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期key堆积在内存里，导致redis内存块耗尽了。怎么解决这个问题呢？ redis 内存淘汰机制。 redis 内存淘汰机制(MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据?)redis 配置文件 redis.conf 中有相关注释，我这里就不贴了，大家可以自行查阅或者通过这个网址查看： http://download.redis.io/redis-stable/redis.conf redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的） allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 4.0版本后增加以下两种： volatile-lfu：从已设置过期时间的数据集(server.db[i].expires)中挑选最不经常使用的数据淘汰 allkeys-lfu：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的key 备注： 关于 redis 设置过期时间以及内存淘汰机制，我这里只是简单的总结一下，后面会专门写一篇文章来总结！ redis 持久化机制(怎么保证 redis 挂掉之后再重启数据可以进行恢复)很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis不同于Memcached的很重一点就是，Redis支持持久化，而且支持两种不同的持久化操作。Redis的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file,AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 快照（snapshotting）持久化（RDB） Redis可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis主从结构，主要用来提高Redis性能），还可以将快照留在原地以便重启服务器的时候使用。 快照持久化是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： 123456save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 AOF（append-only file）持久化 与快照持久化相比，AOF持久化 的实时性更好，因此已成为主流的持久化方案。默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启： 1appendonly yes 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof。 在Redis的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： 123appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec选项 ，让Redis每秒同步一次AOF文件，Redis性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。 Redis 4.0 对于持久化机制的优化 Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 补充内容：AOF 重写 AOF重写可以产生一个新的AOF文件，这个新的AOF文件和原有的AOF文件所保存的数据库状态一样，但体积更小。 AOF重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有AOF文件进行任何读入、分析或者写入操作。 在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新AOF文件期间，记录服务器执行的所有写命令。当子进程完成创建新AOF文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新AOF文件的末尾，使得新旧两个AOF文件所保存的数据库状态一致。最后，服务器用新的AOF文件替换旧的AOF文件，以此来完成AOF文件重写操作 更多内容可以查看我的这篇文章： Redis持久化 redis 事务Redis 通过 MULTI、EXEC、WATCH 等命令来实现事务(transaction)功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而改去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。 在传统的关系式数据库中，常常用 ACID 性质来检验事务功能的可靠性和安全性。在 Redis 中，事务总是具有原子性（Atomicity）、一致性（Consistency）和隔离性（Isolation），并且当 Redis 运行在某种特定的持久化模式下时，事务也具有持久性（Durability）。 缓存雪崩和缓存穿透问题解决方案缓存雪崩 简介：缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决办法（中华石杉老师在他的视频中提到过，视频地址在最后一个问题中有提到）： 事前：尽量保证整个 redis 集群的高可用性，发现机器宕机尽快补上。选择合适的内存淘汰策略。 事中：本地ehcache缓存 + hystrix限流&amp;降级，避免MySQL崩掉 事后：利用 redis 持久化机制保存的数据尽快恢复缓存 缓存穿透 简介：一般是黑客故意去请求缓存中不存在的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。 解决办法： 有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），如果一个查询返回的数据为空（不管是数 据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 参考： https://blog.csdn.net/zeb_perfect/article/details/54135506 如何解决 Redis 的并发竞争 Key 问题所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！ 推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能） 基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。 在实践中，当然是从以可靠性为主。所以首推Zookeeper。 参考： https://www.jianshu.com/p/8bddd381de06 如何保证缓存与数据库双写时的数据一致性?你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况 串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 参考： Java工程师面试突击第1季（可能是史上最好的Java面试突击课程）]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[让 hexo 博客在后台跑起来]]></title>
    <url>%2F2018%2F01%2F01%2F%E8%AE%A9%20hexo%20%E5%8D%9A%E5%AE%A2%E5%9C%A8%E5%90%8E%E5%8F%B0%E8%B7%91%E8%B5%B7%E6%9D%A5%2F</url>
    <content type="text"><![CDATA[让 hexo 博客在后台跑起来 前几天利用 hexo 搭建了一个博客，于是很高兴地用 $ hexo server 在我的 Linux 服务器上跑了起来。 但是发现，只要关闭了进程，博客也就跟着关闭了。想着，这样的博客给谁看啊，总不能我本地一直开着进程吧。 于是我开始搜索相关 hexo 后台运行的方法，貌似很多人也有同样的问题，官方给出的方法是$ hexo s &amp;,试了一下进程还是莫名其妙中断了。于是乎看到一篇文章说使用 forever 可以解决，这个给了我启发，那么就用 pm2 运行我的博客不就行了。 那么开始操作： 安装pm21npm install -g pm2 写一个执行脚本学习一下 Node.js 官方文档的 child_process.exec(command[, options][, callback]) 部分 参考官方文档 在博客根目录下创建一个文件 run.js 12345678910//run.jsconst &#123; exec &#125; = require('child_process')exec('hexo server',(error, stdout, stderr) =&gt; &#123; if(error)&#123; console.log(`exec error: $&#123;error&#125;`) return &#125; console.log(`stdout: $&#123;stdout&#125;`); console.log(`stderr: $&#123;stderr&#125;`);&#125;) 运行脚本进入博客根目录 1pm2 start run.js]]></content>
      <categories>
        <category>个人博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端程序员必备的Linux基础知识]]></title>
    <url>%2F2018%2F01%2F01%2F%E5%90%8E%E7%AB%AF%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E5%A4%87%E7%9A%84Linux%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[一 从认识操作系统开始 1.1 操作系统简介 1.2 操作系统简单分类 二 初探Linux 2.1 Linux简介 2.2 Linux诞生简介 2.3 Linux的分类 三 Linux文件系统概览 3.1 Linux文件系统简介 3.2 文件类型与目录结构 四 Linux基本命令 4.1 目录切换命令 4.2 目录的操作命令（增删改查） 4.3 文件的操作命令（增删改查） 4.4 压缩文件的操作命令 4.5 Linux的权限命令 4.6 Linux 用户管理 4.7 Linux系统用户组的管理 4.8 其他常用命令 推荐一个Github开源的Linux学习指南(Java工程师向)：https://github.com/judasn/Linux-Tutorial 学习Linux之前，我们先来简单的认识一下操作系统。 一 从认识操作系统开始1.1 操作系统简介我通过以下四点介绍什么操作系统： 操作系统（Operation System，简称OS）是管理计算机硬件与软件资源的程序，是计算机系统的内核与基石； 操作系统本质上是运行在计算机上的软件程序 ； 为用户提供一个与系统交互的操作界面 ； 操作系统分内核与外壳（我们可以把外壳理解成围绕着内核的应用程序，而内核就是能操作硬件的程序）。 1.2 操作系统简单分类 Windows: 目前最流行的个人桌面操作系统 ，不做多的介绍，大家都清楚。 Unix： 最早的多用户、多任务操作系统 .按照操作系统的分类，属于分时操作系统。Unix 大多被用在服务器、工作站，现在也有用在个人计算机上。它在创建互联网、计算机网络或客户端/服务器模型方面发挥着非常重要的作用。 Linux: Linux是一套免费使用和自由传播的类Unix操作系统.Linux存在着许多不同的Linux版本，但它们都使用了 Linux内核 。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、视频游戏控制台、台式计算机、大型机和超级计算机。严格来讲，Linux这个词本身只表示Linux内核，但实际上人们已经习惯了用Linux来形容整个基于Linux内核，并且使用GNU 工程各种工具和数据库的操作系统。 二 初探Linux2.1 Linux简介我们上面已经介绍到了Linux，我们这里只强调三点。 类Unix系统： Linux是一种自由、开放源码的类似Unix的操作系统 Linux内核： 严格来说，Linux这个词本身只表示Linux内核 Linux之父： 一个编程领域的传奇式人物。他是Linux内核的最早作者，随后发起了这个开源项目，担任Linux内核的首要架构师与项目协调者，是当今世界最著名的电脑程序员、黑客之一。他还发起了Git这个开源项目，并为主要的开发者。 2.2 Linux诞生简介 1991年，芬兰的业余计算机爱好者Linus Torvalds编写了一款类似Minix的系统（基于微内核架构的类Unix操作系统）被ftp管理员命名为Linux 加入到自由软件基金的GNU计划中; Linux以一只可爱的企鹅作为标志，象征着敢作敢为、热爱生活。 2.3 Linux的分类Linux根据原生程度，分为两种： 内核版本： Linux不是一个操作系统，严格来讲，Linux只是一个操作系统中的内核。内核是什么？内核建立了计算机软件与硬件之间通讯的平台，内核提供系统服务，比如文件管理、虚拟内存、设备I/O等； 发行版本： 一些组织或公司在内核版基础上进行二次开发而重新发行的版本。Linux发行版本有很多种（ubuntu和CentOS用的都很多，初学建议选择CentOS），如下图所示： 三 Linux文件系统概览3.1 Linux文件系统简介在Linux操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。 也就是说在LINUX系统中有一个重要的概念：一切都是文件。其实这是UNIX哲学的一个体现，而Linux是重写UNIX而来，所以这个概念也就传承了下来。在UNIX系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。 3.2 文件类型与目录结构Linux支持5种文件类型 ： Linux的目录结构如下： Linux文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： 常见目录说明： /bin： 存放二进制可执行文件(ls、cat、mkdir等)，常用命令一般都在这里； /etc： 存放系统管理和配置文件； /home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示； /usr ： 用于存放系统应用程序； /opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里； /proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息； /root： 超级用户（系统管理员）的主目录（特权阶级^o^）； /sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等； /dev： 用于存放设备文件； /mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统； /boot： 存放用于系统引导时使用的各种文件； /lib ： 存放着和系统运行相关的库文件 ； /tmp： 用于存放各种临时文件，是公用的临时文件存储点； /var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等； /lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。 四 Linux基本命令下面只是给出了一些比较常用的命令。推荐一个Linux命令快查网站，非常不错，大家如果遗忘某些命令或者对某些命令不理解都可以在这里得到解决。 Linux命令大全：http://man.linuxde.net/ 4.1 目录切换命令 cd usr： 切换到该目录下usr目录 cd ..（或cd../）： 切换到上一层目录 cd /： 切换到系统根目录 cd ~： 切换到用户主目录 cd -： 切换到上一个操作所在目录 4.2 目录的操作命令(增删改查) mkdir 目录名称： 增加目录 ls或者ll（ll是ls -l的别名，ll命令可以看到该目录下的所有目录和文件的详细信息）：查看目录信息 find 目录 参数： 寻找目录（查） 示例： 列出当前目录及子目录下所有文件和文件夹: find . 在/home目录下查找以.txt结尾的文件名:find /home -name &quot;*.txt&quot; 同上，但忽略大小写: find /home -iname &quot;*.txt&quot; 当前目录及子目录下查找所有以.txt和.pdf结尾的文件:find . \( -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot; \)或find . -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot; mv 目录名称 新目录名称： 修改目录的名称（改） 注意：mv的语法不仅可以对目录进行重命名而且也可以对各种文件，压缩包等进行 重命名的操作。mv命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。后面会介绍到mv命令的另一个用法。 mv 目录名称 目录的新位置： 移动目录的位置—剪切（改） 注意：mv语法不仅可以对目录进行剪切操作，对文件和压缩包等都可执行剪切操作。另外mv与cp的结果不同，mv好像文件“搬家”，文件个数并未增加。而cp对文件进行复制，文件个数增加了。 cp -r 目录名称 目录拷贝的目标位置： 拷贝目录（改），-r代表递归拷贝 注意：cp命令不仅可以拷贝目录还可以拷贝文件，压缩包等，拷贝文件和压缩包时不 用写-r递归 rm [-rf] 目录: 删除目录（删） 注意：rm不仅可以删除目录，也可以删除其他文件或压缩包，为了增强大家的记忆， 无论删除任何目录或文件，都直接使用rm -rf 目录/文件/压缩包 4.3 文件的操作命令(增删改查) touch 文件名称: 文件的创建（增） cat/more/less/tail 文件名称 文件的查看（查） cat： 查看显示文件内容 more： 可以显示百分比，回车可以向下一行， 空格可以向下一页，q可以退出查看 less： 可以使用键盘上的PgUp和PgDn向上 和向下翻页，q结束查看 tail-10 ： 查看文件的后10行，Ctrl+C结束 注意：命令 tail -f 文件 可以对某个文件进行动态监控，例如tomcat的日志文件， 会随着程序的运行，日志会变化，可以使用tail -f catalina-2016-11-11.log 监控 文 件的变化 vim 文件： 修改文件的内容（改） vim编辑器是Linux中的强大组件，是vi编辑器的加强版，vim编辑器的命令和快捷方式有很多，但此处不一一阐述，大家也无需研究的很透彻，使用vim编辑修改文件的方式基本会使用就可以了。 在实际开发中，使用vim编辑器主要作用就是修改配置文件，下面是一般步骤： vim 文件——&gt;进入文件—–&gt;命令模式——&gt;按i进入编辑模式—–&gt;编辑文件 ——-&gt;按Esc进入底行模式—–&gt;输入：wq/q! （输入wq代表写入内容并退出，即保存；输入q!代表强制退出不保存。） rm -rf 文件： 删除文件（删） 同目录删除：熟记 rm -rf 文件 即可 4.4 压缩文件的操作命令1）打包并压缩文件： Linux中的打包文件一般是以.tar结尾的，压缩的命令一般是以.gz结尾的。 而一般情况下打包和压缩是一起进行的，打包并压缩后的文件的后缀名一般.tar.gz。命令：tar -zcvf 打包压缩后的文件名 要打包压缩的文件其中： z：调用gzip压缩命令进行压缩 c：打包文件 v：显示运行过程 f：指定文件名 比如：加入test目录下有三个文件分别是：aaa.txt bbb.txt ccc.txt，如果我们要打包test目录并指定压缩后的压缩包名称为test.tar.gz可以使用命令：tar -zcvf test.tar.gz aaa.txt bbb.txt ccc.txt或：tar -zcvf test.tar.gz /test/ 2）解压压缩包： 命令：tar [-xvf] 压缩文件 其中：x：代表解压 示例： 1 将/test下的test.tar.gz解压到当前目录下可以使用命令：tar -xvf test.tar.gz 2 将/test下的test.tar.gz解压到根目录/usr下:tar -xvf xxx.tar.gz -C /usr（- C代表指定解压的位置） 4.5 Linux的权限命令 操作系统中每个文件都拥有特定的权限、所属用户和所属组。权限是操作系统用来限制资源访问的机制，在Linux中权限一般分为读(readable)、写(writable)和执行(excutable)，分为三组。分别对应文件的属主(owner)，属组(group)和其他用户(other)，通过这样的机制来限制哪些用户、哪些组可以对特定的文件进行什么样的操作。通过 ls -l 命令我们可以 查看某个目录下的文件或目录的权限 示例：在随意某个目录下ls -l 第一列的内容的信息解释如下： 下面将详细讲解文件的类型、Linux中权限以及文件有所有者、所在组、其它组具体是什么？ 文件的类型： d： 代表目录 -： 代表文件 l： 代表软链接（可以认为是window中的快捷方式） Linux中权限分为以下几种： r：代表权限是可读，r也可以用数字4表示 w：代表权限是可写，w也可以用数字2表示 x：代表权限是可执行，x也可以用数字1表示 文件和目录权限的区别： 对文件和目录而言，读写执行表示不同的意义。 对于文件： 权限名称 可执行操作 r 可以使用cat查看文件的内容 w 可以修改文件的内容 x 可以将其运行为二进制文件 对于目录： 权限名称 可执行操作 r 可以查看目录下列表 w 可以创建和删除目录下文件 x 可以使用cd进入目录 需要注意的是超级用户可以无视普通用户的权限，即使文件目录权限是000，依旧可以访问。在linux中的每个用户必须属于一个组，不能独立于组外。在linux中每个文件有所有者、所在组、其它组的概念。 所有者 一般为文件的创建者，谁创建了该文件，就天然的成为该文件的所有者，用ls ‐ahl命令可以看到文件的所有者 也可以使用chown 用户名 文件名来修改文件的所有者 。 文件所在组 当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组 用ls ‐ahl命令可以看到文件的所有组 也可以使用chgrp 组名 文件名来修改文件所在的组。 其它组 除开文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组 我们再来看看如何修改文件/目录的权限。 修改文件/目录的权限的命令：chmod 示例：修改/test下的aaa.txt的权限为属主有全部权限，属主所在的组有读写权限，其他用户只有读的权限 chmod u=rwx,g=rw,o=r aaa.txt 上述示例还可以使用数字表示： chmod 764 aaa.txt 补充一个比较常用的东西: 假如我们装了一个zookeeper，我们每次开机到要求其自动启动该怎么办？ 新建一个脚本zookeeper 为新建的脚本zookeeper添加可执行权限，命令是:chmod +x zookeeper 把zookeeper这个脚本添加到开机启动项里面，命令是：chkconfig --add zookeeper 如果想看看是否添加成功，命令是：chkconfig --list 4.6 Linux 用户管理Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。 Linux用户管理相关命令: useradd 选项 用户名:添加用户账号 userdel 选项 用户名:删除用户帐号 usermod 选项 用户名:修改帐号 passwd 用户名:更改或创建用户的密码 passwd -S 用户名 :显示用户账号密码信息 passwd -d 用户名: 清除用户密码 useradd命令用于Linux中创建的新的系统用户。useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。 passwd命令用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 4.7 Linux系统用户组的管理每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 Linux系统用户组的管理相关命令: groupadd 选项 用户组 :增加一个新的用户组 groupdel 用户组:要删除一个已有的用户组 groupmod 选项 用户组 : 修改用户组的属性 4.8 其他常用命令 pwd： 显示当前所在位置 grep 要搜索的字符串 要搜索的文件 --color： 搜索命令，–color代表高亮显示 ps -ef/ps -aux： 这两个命令都是查看当前系统正在运行进程，两者的区别是展示格式不同。如果想要查看特定的进程可以使用这样的格式：ps aux|grep redis （查看包括redis字符串的进程），也可使用 pgrep redis -a。 注意：如果直接用ps（（Process Status））命令，会显示所有进程的状态，通常结合grep命令查看某进程的状态。 kill -9 进程的pid： 杀死进程（-9 表示强制终止。） 先用ps查找进程，然后用kill杀掉 网络通信命令： 查看当前系统的网卡信息：ifconfig 查看与某台机器的连接情况：ping 查看当前系统的端口使用：netstat -an net-tools 和 iproute2 ： net-tools起源于BSD的TCP/IP工具箱，后来成为老版本Linux内核中配置网络功能的工具。但自2001年起，Linux社区已经对其停止维护。同时，一些Linux发行版比如Arch Linux和CentOS/RHEL 7则已经完全抛弃了net-tools，只支持iproute2。linux ip命令类似于ifconfig，但功能更强大，旨在替代它。更多详情请阅读如何在Linux中使用IP命令和示例 shutdown： shutdown -h now： 指定现在立即关机；shutdown +5 &quot;System will shutdown after 5 minutes&quot;：指定5分钟后关机，同时送出警告信息给登入用户。 reboot： reboot： 重开机。reboot -w： 做个重开机的模拟（只有纪录并不会真的重开机）。]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>服务器</tag>
        <tag>操作系统</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深入理解计算机系统》 day01]]></title>
    <url>%2F2017%2F12%2F20%2F%E3%80%8A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E3%80%8B%20day01%2F</url>
    <content type="text"><![CDATA[预处理阶段预处理器（cpp）识别以#字符开头的指令 修改源程序，比如#iclude&lt;stdio.h&gt;告诉预处理器读取系统头文件stdio.h的内容并把它插入程序文本中就得到另一个C程序，以 .i 为文件扩展名 编译阶段编译器（ccl）将hello.i翻译成文本文件hello.s，它包含一个汇编程序，每条语句都以一种文本格式描述了一条低级的机器语言指令 汇编阶段接下来汇编器（as）将hello.s翻译成机器语言指令，并将结果打包在一个二进制文件 hello.o 中 链接阶段将程序中需要用到的函数，例如printf，从printf.o包中用链接器（ld）合并到hello.o程序中得到一个hello文件，加载到内存中交给系统执行。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap为什么不是线程安全的？]]></title>
    <url>%2F2017%2F12%2F13%2FHashMap%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E6%98%AF%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[当Hashmap的容量不够时，需要扩容扩容包含扩容和ReHash两个步骤，ReHash在并发的情况下可能会形成链表环。具体细节参考：漫画：高并发下的HashMap(https://mp.weixin.qq.com/s/dzNq50zBQ4iDrOAhM4a70A) 程序员小灰]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务的隔离级别]]></title>
    <url>%2F2017%2F11%2F11%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[事务 就是要保证一组数据库操作，要么全部成功，要么全部失败。 在MySQL中，事务支持是在引擎层实现的 MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如MySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代的重要原因之一。事务的四大特性：ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。 SQL标准的事务隔离级别包括：（从上到下越来越严实） 读未提交（read uncommitted） 读提交（read committed） 可重复读（repeatable read） 串行化（serializable ）逐一解释： 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。这会带来脏读、幻读、不可重复读问题。（基本没用） 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。避免了脏读，但仍然存在不可重复读和幻读问题。 可重复读是指，一个事务执行过程中看到的数据，这个事务自己看到的数据始终不变（当然他可能已经被其他事务改变了）在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 避免了脏读和不可重复读问题，但幻读依然存在。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行，避免了以上所有问题。（基本没用） 直接看文字描述可能不太好理解，那我们来看图吧 我们来看看在不同的隔离级别下，事务A会有哪些不同的返回结果，也就是图里面V1、V2、V3的返回值分别是什么。 若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2。 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle数据库的默认隔离级别其实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致，你一定要记得将MySQL的隔离级别设置为“读提交”。 总结来说，存在即合理，哪个隔离级别都有它自己的使用场景，你要根据自己的业务情况来定。我想你可能会问那什么时候需要“可重复读”的场景呢？我们来看一个数据校对逻辑的案例。 假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。 这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。 悲观锁与乐观锁 悲观锁，正如它的名字那样，数据库总是认为别人会去修改它所要操作的数据，因此在数据库处理过程中将数据加锁。其实现依靠数据库底层。 乐观锁，如它的名字那样，总是认为别人不会去修改，只有在提交更新的时候去检查数据的状态。通常是给数据增加一个字段来标识数据的版本。 MySQL的MVCC（多版本并发控制） 我们知道，MySQL的innodb采用的是行锁，而且采用了多版本并发控制来提高读操作的性能。 什么是多版本并发控制呢 ？其实就是在每一行记录的后面增加两个隐藏列，记录创建版本号和删除版本号， 而每一个事务在启动的时候，都有一个唯一的递增的版本号。 1、在插入操作时 ： 记录的创建版本号就是事务版本号。 比如我插入一条记录, 事务id 假设是1 ，那么记录如下：也就是说，创建版本号就是事务版本号。 2、在更新操作的时候，采用的是先标记旧的那行记录为已删除，并且删除版本号是事务版本号，然后插入一行新的记录的方式。 比如，针对上面那行记录，事务Id为2 要把name字段更新 update table set name= ‘new_value’ where id=1; 3、删除操作的时候，就把事务版本号作为删除版本号。比如 delete from table where id=1; 4、查询操作： 从上面的描述可以看到，在查询时要符合以下两个条件的记录才能被事务查询出来： 1) 删除版本号 大于 当前事务版本号，就是说删除操作是在当前事务启动之后做的。 2) 创建版本号 小于或者等于 当前事务版本号 ，就是说记录创建是在事务中（等于的情况）或者事务启动之前。 这样就保证了各个事务互不影响。从这里也可以体会到一种提高系统性能的思路，就是： 通过版本号来减少锁的争用。 另外，只有read-committed和 repeatable-read 两种事务隔离级别才能使用mVcc read-uncommited由于是读到未提交的，所以不存在版本的问题 而serializable 则会对所有读取的行加锁。 问题：那我们用什么办法能看到两个隐藏列呢？]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中创建对象的五种方式]]></title>
    <url>%2F2017%2F11%2F11%2FJava%E4%B8%AD%E5%88%9B%E5%BB%BA%E5%AF%B9%E8%B1%A1%E7%9A%84%E4%BA%94%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[作为Java开发者，我们每天创建很多对象，但是我们通常使用依赖注入的方式管理系统，比如：Spring去创建对象，然而这里有很多其他创建对象的方法： 使用New关键字、使用Class类的newInstance方法、使用Constructor类的newInstance方法、使用Clone方法、使用反序列化。 使用new关键字：这是我们最常见的也是最简单的创建对象的方式，通过这种方式我们还可以调用任意的够赞函数（无参的和有参的）。比如：Student student = new Student(); 使用Class类的newInstance方法：我们也可以使用Class类的newInstance方法创建对象，这个newInstance方法调用无参的构造器创建对象，如：Student student2 = (Student)Class.forName(“根路径.Student”).newInstance(); 或者：Student stu = Student.class.newInstance(); 使用Constructor类的newInstance方法：此方法和Class类的newInstance方法很像，java.lang.relect.Constructor类里也有一个newInstance方法可以创建对象。我们可以通过这个newInstance方法调用有参数的和私有的构造函数。如：Constructor constructor = Student.class.getInstance();Student stu = constructor.newInstance(); 这两种newInstance的方法就是大家所说的反射，事实上Class的newInstance方法内部调用Constructor的newInstance方法。这也是众多框架Spring、Hibernate、Struts等使用后者的原因。 使用Clone的方法：无论何时我们调用一个对象的clone方法，JVM就会创建一个新的对象，将前面的对象的内容全部拷贝进去，用clone方法创建对象并不会调用任何构造函数。要使用clone方法，我们必须先实现Cloneable接口并实现其定义的clone方法。如：Student stu2 = stu.clone(); 使用反序列化：当我们序列化和反序列化一个对象，JVM会给我们创建一个单独的对象，在反序列化时，JVM创建对象并不会调用任何构造函数。为了反序列化一个对象，我们需要让我们的类实现Serializable接口。如：ObjectInputStream in = new ObjectInputStream (new FileInputStream(“data.obj”));Student stu3 = (Student)in.readObject(); 从上面的例子可以看出来，除了使用new关键字之外的其他方法全部都是转变为invokevirtual（创建对象的直接方法），使用被new的方式转变为两个调用，new和invokespecial（构造函数调用）。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>创建对象</tag>
        <tag>反射</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【设计模式】动态代理]]></title>
    <url>%2F2017%2F11%2F05%2F%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[个人理解：一个工厂生产冰箱，冰箱从工厂生产出来到消费者手中一般还有有一个代理商，提供一些服务，比如附赠一些小礼品，送货上门等等Spring中就使用了动态代理的思想，比如Spring的反向代理加依赖注入，就相当于送货上门Spring里的AOF面向切面编程就使用了动态代理达到事务控制、日志打印功能，就相当于附赠一些小礼品。 按照代理的创建时期，代理类可以分为两种： 静态代理：由程序员创建代理类或特定工具自动生成源代码再对其编译。在程序运行前代理类的.class文件就已经存在了。 动态代理：在程序运行时运用反射机制动态创建而成。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[类加载过程]]></title>
    <url>%2F2017%2F10%2F02%2F%E7%B1%BB%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[类加载过程Class 文件需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些 Class 文件呢？ 系统加载 Class 类型的文件主要三步:加载-&gt;连接-&gt;初始化。连接过程又可分为三步:验证-&gt;准备-&gt;解析。 加载类加载过程的第一步，主要完成下面3件事情： 通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的 Class 对象,作为方法区这些数据的访问入口 虚拟机规范多上面这3点并不具体，因此是非常灵活的。比如：”通过全类名获取定义此类的二进制字节流” 并没有指明具体从哪里获取、怎样获取。比如：比较常见的就是从 ZIP 包中读取（日后出现的JAR、EAR、WAR格式的基础）、其他文件生成（典型应用就是JSP）等等。 一个非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的 loadClass() 方法）。数组类型不通过类加载器创建，它由 Java 虚拟机直接创建。 类加载器、双亲委派模型也是非常重要的知识点，这部分内容会在后面的文章中单独介绍到。 加载阶段和连接阶段的部分内容是交叉进行的，加载阶段尚未结束，连接阶段可能就已经开始了。 验证 准备准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 这里所设置的初始值”通常情况”下是数据类型默认的零值（如0、0L、null、false等），比如我们定义了public static int value=111 ，那么 value 变量在准备阶段的初始值就是 0 而不是111（初始化阶段才会复制）。特殊情况：比如给 value 变量加上了 fianl 关键字public static final int value=111 ，那么准备阶段 value 的值就被复制为 111。 基本数据类型的零值： 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行。 符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。在程序实际运行时，只有符号引用是不够的，举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java 虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方发表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。 综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。 初始化初始化是类加载的最后一步，也是真正执行类中定义的 Java 程序代码(字节码)，初始化阶段是执行类构造器 &lt;clinit&gt; ()方法的过程。 对于&lt;clinit&gt;（） 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 &lt;clinit&gt;（） 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起死锁，并且这种死锁很难被发现。 对于初始化阶段，虚拟机严格规范了有且只有5中情况下，必须对类进行初始化： 当遇到 new 、 getstatic、putstatic或invokestatic 这4条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。 使用 java.lang.reflect 包的方法对类进行反射调用时 ，如果类没初始化，需要触发其初始化。 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。 当使用 JDK1.7 的动态动态语言时，如果一个 MethodHandle 实例的最后解析结构为 REF_getStatic、REF_putStatic、REF_invokeStatic、的方法句柄，并且这个句柄没有初始化，则需要先触发器初始化。 参考 《深入理解Java虚拟机》 《实战Java虚拟机》 https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-5.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
        <tag>类加载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【设计模式】工厂模式]]></title>
    <url>%2F2017%2F10%2F01%2F%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[工厂模式分为三种：简单工厂模式、工厂模式、抽象工厂模式从实现上看，代码复杂度依次上升简单工厂模式：采用switch语句根据传入的参数不同返回不同的对象，缺点是必须得传参，传参有问题会导致调用不成功，且后期扩展不方便，如果要加一个对象需要改动原来的代码工厂模式：将每个对象写作不同的方法，不用传参，且后期扩展方便，直接加方法就行抽象工厂模式：将方法加上了static修饰，使用的时候直接调用，不用实例化，更方便]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>工厂模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[选择排序]]></title>
    <url>%2F2017%2F09%2F02%2F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[思路首先，找到数组中最小的元素，拎出来，将它和数组的第一个元素交换位置，第二步，在剩下的元素中继续寻找最小的元素，拎出来，和数组的第二个元素交换位置，如此循环，直到整个数组排序完成。 至于选大还是选小，这个都无所谓，你也可以每次选择最大的拎出来排，也可以每次选择最小的拎出来的排，只要你的排序的手段是这种方式，都叫选择排序。代码实现 1234567891011121314public static void sort(int arr[])&#123; for( int i = 0;i &lt; arr.length ; i++ )&#123; int min = i;//最小元素的下标 for(int j = i + 1;j &lt; arr.length ; j++ )&#123; if(arr[j] &lt; arr[min])&#123; min = j;//找最小值 &#125; &#125; //交换位置 int temp = arr[i]; arr[i] = arr[min]; arr[min] = temp; &#125;&#125; 双层循环，时间复杂度和冒泡一模一样，都是O(n2)。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>选择排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[希尔排序]]></title>
    <url>%2F2017%2F09%2F02%2F%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[希尔排序这个名字，来源于它的发明者希尔，也称作“缩小增量排序”，是插入排序的一种更高效的改进版本。 我们知道，插入排序对于大规模的乱序数组的时候效率是比较慢的，因为它每次只能将数据移动一位，希尔排序为了加快插入的速度，让数据移动的时候可以实现跳跃移动，节省了一部分的时间开支。 代码实现 123456789101112131415161718192021public static void sort(int[] arr) &#123; int length = arr.length; //区间 int gap = 1; while (gap &lt; length) &#123; gap = gap * 3 + 1; &#125; while (gap &gt; 0) &#123; for (int i = gap; i &lt; length; i++) &#123; int tmp = arr[i]; int j = i - gap; //跨区间排序 while (j &gt;= 0 &amp;&amp; arr[j] &gt; tmp) &#123; arr[j + gap] = arr[j]; j -= gap; &#125; arr[j + gap] = tmp; &#125; gap = gap / 3; &#125;&#125; 可能你会问为什么区间要以 gap = gap*3 + 1 去计算，其实最优的区间计算方法是没有答案的，这是一个长期未解决的问题，不过差不多都会取在二分之一到三分之一附近。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>希尔排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆排序]]></title>
    <url>%2F2017%2F09%2F01%2F%E5%A0%86%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[堆排序顾名思义，是利用堆这种数据结构来进行排序的算法。 如果你不了解堆这种数据结构，可以查看小吴之前的数据结构系列文章—看动画轻松理解堆 如果你了解堆这种数据结构，你应该知道堆是一种优先队列，两种实现，最大堆和最小堆，由于我们这里排序按升序排，所以就直接以最大堆来说吧。 我们完全可以把堆（以下全都默认为最大堆）看成一棵完全二叉树，但是位于堆顶的元素总是整棵树的最大值，每个子节点的值都比父节点小，由于堆要时刻保持这样的规则特性，所以一旦堆里面的数据发生变化，我们必须对堆重新进行一次构建。 既然堆顶元素永远都是整棵树中的最大值，那么我们将数据构建成堆后，只需要从堆顶取元素不就好了吗？ 第一次取的元素，是否取的就是最大值？取完后把堆重新构建一下，然后再取堆顶的元素，是否取的就是第二大的值？ 反复的取，取出来的数据也就是有序的数据。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public static void sort(int[] arr) &#123; int length = arr.length; //构建堆 buildHeap(arr， length); for ( int i = length - 1; i &gt; 0; i-- ) &#123; //将堆顶元素与末位元素调换 int temp = arr[0]; arr[0] = arr[i]; arr[i] = temp; //数组长度-1 隐藏堆尾元素 length--; //将堆顶元素下沉 目的是将最大的元素浮到堆顶来 sink(arr， 0， length); &#125;&#125;private static void buildHeap(int[] arr， int length) &#123; for (int i = length / 2; i &gt;= 0; i--) &#123; sink(arr， i， length); &#125;&#125;/** * 下沉调整 * @param arr 数组 * @param index 调整位置 * @param length 数组范围 */private static void sink(int[] arr， int index， int length) &#123; int leftChild = 2 * index + 1;//左子节点下标 int rightChild = 2 * index + 2;//右子节点下标 int present = index;//要调整的节点下标 //下沉左边 if (leftChild &lt; length &amp;&amp; arr[leftChild] &gt; arr[present]) &#123; present = leftChild; &#125; //下沉右边 if (rightChild &lt; length &amp;&amp; arr[rightChild] &gt; arr[present]) &#123; present = rightChild; &#125; //如果下标不相等 证明调换过了 if (present != index) &#123; //交换值 int temp = arr[index]; arr[index] = arr[present]; arr[present] = temp; //继续下沉 sink(arr， present， length); &#125;&#125; 堆排序和快速排序的时间复杂度都一样是 O(nlogn)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>堆排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[冒泡排序]]></title>
    <url>%2F2017%2F09%2F01%2F%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[冒泡排序 冒泡排序无疑是最为出名的排序算法之一，从序列的一端开始往另一端冒泡（你可以从左往右冒泡，也可以从右往左冒泡，看心情），依次比较相邻的两个数的大小（到底是比大还是比小也看你心情）。 冒泡的代码还是相当简单的，两层循环，外层冒泡轮数，里层依次比较，江湖中人人尽皆知。 123456789101112public static void sort(int arr[])&#123; for( int i = 0 ; i &lt; arr.length - 1 ; i++ )&#123; for(int j = 0;j &lt; arr.length - 1 - i ; j++)&#123; int temp = 0; if(arr[j] &lt; arr[j + 1])&#123; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; &#125;&#125; 我们看到嵌套循环，应该立马就可以得出这个算法的时间复杂度为O(n2)。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>冒泡排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序]]></title>
    <url>%2F2017%2F09%2F01%2F%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[快速排序的核心思想也是分治法，分而治之。它的实现方式是每次从序列中选出一个基准值，其他数依次和基准值做比较，比基准值大的放右边，比基准值小的放左边，然后再对左边和右边的两组数分别选出一个基准值，进行同样的比较移动，重复步骤，直到最后都变成单个元素，整个数组就成了有序的序列。 快速排序的关键之处在于切分，切分的同时要进行比较和移动，这里介绍一种叫做单边扫描的做法。 我们随意抽取一个数作为基准值，同时设定一个标记 mark 代表左边序列最右侧的下标位置，当然初始为 0 ，接下来遍历数组，如果元素大于基准值，无操作，继续遍历，如果元素小于基准值，则把 mark + 1 ，再将 mark 所在位置的元素和遍历到的元素交换位置，mark 这个位置存储的是比基准值小的数据，当遍历结束后，将基准值与 mark 所在元素交换位置即可。 代码实现：1234567891011121314151617181920212223242526272829303132public static void sort(int[] arr) &#123; sort(arr， 0， arr.length - 1);&#125;private static void sort(int[] arr， int startIndex， int endIndex) &#123; if (endIndex &lt;= startIndex) &#123; return; &#125; //切分 int pivotIndex = partitionV2(arr， startIndex， endIndex); sort(arr， startIndex， pivotIndex-1); sort(arr， pivotIndex+1， endIndex);&#125;private static int partition(int[] arr， int startIndex， int endIndex) &#123; int pivot = arr[startIndex];//取基准值 int mark = startIndex;//Mark初始化为起始下标 for(int i=startIndex+1; i&lt;=endIndex; i++)&#123; if(arr[i]&lt;pivot)&#123; //小于基准值 则mark+1，并交换位置。 mark ++; int p = arr[mark]; arr[mark] = arr[i]; arr[i] = p; &#125; &#125; //基准值与mark对应元素调换位置 arr[startIndex] = arr[mark]; arr[mark] = pivot; return mark;&#125; 极端情况快速排序的时间复杂度和归并排序一样，O(n log n)，但这是建立在每次切分都能把数组一刀切两半差不多大的前提下，如果出现极端情况，比如排一个有序的序列，如[ 9，8，7，6，5，4，3，2，1 ]，选取基准值 9 ，那么需要切分 n - 1 次才能完成整个快速排序的过程，这种情况下，时间复杂度就退化成了 O(n2)，当然极端情况出现的概率也是比较低的。 所以说，快速排序的时间复杂度是 O(nlogn)，极端情况下会退化成 O(n2)，为了避免极端情况的发生，选取基准值应该做到随机选取，或者是打乱一下数组再选取。 另外，快速排序的空间复杂度为 O(1)。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>快速排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker概览]]></title>
    <url>%2F2017%2F09%2F01%2FDocker%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[本文只是对Docker的概念做了较为详细的介绍，并不涉及一些像Docker环境的安装以及Docker的一些常见操作和命令。 Docker 是世界领先的软件容器平台，所以想要搞懂Docker的概念我们必须先从容器开始说起。 一 先从认识容器开始1.1 什么是容器?先来看看容器较为官方的解释一句话概括容器：容器就是将软件打包成标准化单元，以用于开发、交付和部署。 容器镜像是轻量的、可执行的独立软件包 ，包含软件运行所需的所有内容：代码、运行时环境、系统工具、系统库和设置。 容器化软件适用于基于Linux和Windows的应用，在任何环境中都能够始终如一地运行。 容器赋予了软件独立性 ，使其免受外在环境差异（例如，开发和预演环境的差异）的影响，从而有助于减少团队间在相同基础设施上运行不同软件时的冲突。 再来看看容器较为通俗的解释如果需要通俗的描述容器的话，我觉得容器就是一个存放东西的地方，就像书包可以装各种文具、衣柜可以放各种衣服、鞋架可以放各种鞋子一样。我们现在所说的容器存放的东西可能更偏向于应用比如网站、程序甚至是系统环境。 1.2 图解物理机,虚拟机与容器关于虚拟机与容器的对比在后面会详细介绍到，这里只是通过网上的图片加深大家对于物理机、虚拟机与容器这三者的理解。 物理机 虚拟机： 容器： 通过上面这三张抽象图，我们可以大概可以通过类比概括出： 容器虚拟化的是操作系统而不是硬件，容器之间是共享同一套操作系统资源的。虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统。因此容器的隔离级别会稍低一些。 相信通过上面的解释大家对于容器这个既陌生又熟悉的概念有了一个初步的认识，下面我们就来谈谈Docker的一些概念。 二 再来谈谈 Docker 的一些概念 2.1 什么是 Docker?说实话关于Docker是什么并太好说，下面我通过四点向你说明Docker到底是个什么东西。 Docker 是世界领先的软件容器平台。 Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核 的cgroup，namespace，以及AUFS类的UnionFS等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。 由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。Docke最初实现是基于 LXC. Docker 能够自动执行重复性任务，例如搭建和配置开发环境，从而解放了开发人员以便他们专注在真正重要的事情上：构建杰出的软件。 用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 2.2 Docker 思想 集装箱 标准化： ①运输方式 ② 存储方式 ③ API接口 隔离 2.3 Docker 容器的特点 轻量在一台机器上运行的多个 Docker 容器可以共享这台机器的操作系统内核；它们能够迅速启动，只需占用很少的计算和内存资源。镜像是通过文件系统层进行构造的，并共享一些公共文件。这样就能尽量降低磁盘用量，并能更快地下载镜像。 标准Docker 容器基于开放式标准，能够在所有主流 Linux 版本、Microsoft Windows 以及包括 VM、裸机服务器和云在内的任何基础设施上运行。 安全Docker 赋予应用的隔离性不仅限于彼此隔离，还独立于底层的基础设施。Docker 默认提供最强的隔离，因此应用出现问题，也只是单个容器的问题，而不会波及到整台机器。 2.4 为什么要用 Docker ? Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 “这段代码在我机器上没问题啊” 这类问题；——一致的运行环境 可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。——更快速的启动时间 避免公用的服务器，资源会容易受到其他用户的影响。——隔离性 善于处理集中爆发的服务器使用压力；——弹性伸缩，快速扩展 可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。——迁移方便 使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。——持续交付和部署 每当说起容器，我们不得不将其与虚拟机做一个比较。就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。 三 容器 VS 虚拟机 简单来说： 容器和虚拟机具有相似的资源隔离和分配优势，但功能有所不同，因为容器虚拟化的是操作系统，而不是硬件，因此容器更容易移植，效率也更高。 3.1 两者对比图 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便. 3.2 容器与虚拟机总结 容器是一个应用层抽象，用于将代码和依赖资源打包在一起。 多个容器可以在同一台机器上运行，共享操作系统内核，但各自作为独立的进程在用户空间中运行 。与虚拟机相比， 容器占用的空间较少（容器镜像大小通常只有几十兆），瞬间就能完成启动 。 虚拟机 (VM) 是一个物理硬件层抽象，用于将一台服务器变成多台服务器。 管理程序允许多个 VM 在一台机器上运行。每个VM都包含一整套操作系统、一个或多个应用、必要的二进制文件和库资源，因此 占用大量空间 。而且 VM 启动也十分缓慢 。 通过Docker官网，我们知道了这么多Docker的优势，但是大家也没有必要完全否定虚拟机技术，因为两者有不同的使用场景。虚拟机更擅长于彻底隔离整个运行环境。例如，云服务提供商通常采用虚拟机技术隔离不同的用户。而 Docker通常用于隔离不同的应用 ，例如前端，后端以及数据库。 3.3 容器与虚拟机两者是可以共存的就我而言，对于两者无所谓谁会取代谁，而是两者可以和谐共存。 Docker中非常重要的三个基本概念，理解了这三个概念，就理解了 Docker 的整个生命周期。 四 Docker基本概念Docker 包括三个基本概念 镜像（Image） 容器（Container） 仓库（Repository） 理解了这三个概念，就理解了 Docker 的整个生命周期 4.1 镜像(Image):一个特殊的文件系统 操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而Docker 镜像（Image），就相当于是一个 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像不包含任何动态数据，其内容在构建之后也不会被改变。 Docker 设计时，就充分利用 Union FS的技术，将其设计为 分层存储的架构 。 镜像实际是由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。 比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 4.2 容器(Container):镜像运行时的实体 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等 。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。前面讲过镜像使用的是分层存储，容器也是如此。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据 ，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主(或网络存储)发生读写，其性能和稳定性更高。数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此， 使用数据卷后，容器可以随意删除、重新 run ，数据却不会丢失。 4.3仓库(Repository):集中存放镜像文件的地方 镜像构建完成后，可以很容易的在当前宿主上运行，但是， 如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry就是这样的服务。 一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。所以说：镜像仓库是Docker用来集中存放镜像文件的地方类似于我们之前常用的代码仓库。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本 。我们可以通过&lt;仓库名&gt;:&lt;标签&gt;的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签.。 这里补充一下Docker Registry 公开服务和私有 Docker Registry的概念： Docker Registry 公开服务 是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 最常使用的 Registry 公开服务是官方的 Docker Hub ，这也是默认的 Registry，并拥有大量的高质量的官方镜像，网址为：https://hub.docker.com/ 。在国内访问Docker Hub 可能会比较慢国内也有一些云服务商提供类似于 Docker Hub 的公开服务。比如 时速云镜像库、网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库等。 除了使用公开服务外，用户还可以在 本地搭建私有 Docker Registry 。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。开源的 Docker Registry 镜像只提供了 Docker Registry API 的服务端实现，足以支持 docker 命令，不影响使用。但不包含图形界面，以及镜像维护、用户管理、访问控制等高级功能。 Docker的概念基本上已经讲完，最后我们谈谈：Build, Ship, and Run。 五 最后谈谈:Build Ship and Run如果你搜索Docker官网，会发现如下的字样：“Docker - Build, Ship, and Run Any App, Anywhere”。那么Build, Ship, and Run到底是在干什么呢？ Build（构建镜像） ： 镜像就像是集装箱包括文件以及运行环境等等资源。 Ship（运输镜像） ：主机和仓库间运输，这里的仓库就像是超级码头一样。 Run （运行镜像） ：运行的镜像就是一个容器，容器就是运行程序的地方。 Docker 运行过程也就是去仓库把镜像拉到本地，然后用一条命令把镜像运行起来变成容器。所以，我们也常常将Docker称为码头工人或码头装卸工，这和Docker的中文翻译搬运工人如出一辙。 六 总结本文主要把Docker中的一些常见概念做了详细的阐述，但是并不涉及Docker的安装、镜像的使用、容器的操作等内容。这部分东西，希望读者自己可以通过阅读书籍与官方文档的形式掌握。如果觉得官方文档阅读起来很费力的话，这里推荐一本书籍《Docker技术入门与实战第二版》。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[插入排序]]></title>
    <url>%2F2017%2F09%2F01%2F%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[思想 插入排序的思想和我们打扑克摸牌的时候一样，从牌堆里一张一张摸起来的牌都是乱序的，我们会把摸起来的牌插入到左手中合适的位置，让左手中的牌时刻保持一个有序的状态。 那如果我们不是从牌堆里摸牌，而是左手里面初始化就是一堆乱牌呢？ 一样的道理，我们把牌往手的右边挪一挪，把手的左边空出一点位置来，然后在乱牌中抽一张出来，插入到左边，再抽一张出来，插入到左边，再抽一张，插入到左边，每次插入都插入到左边合适的位置，时刻保持左边的牌是有序的，直到右边的牌抽完，则排序完毕。 代码实现 123456789101112131415public static void sort(int[] arr) &#123; int n = arr.length; for (int i = 1; i &lt; n; ++i) &#123; int value = arr[i]; int j = 0;//插入的位置 for (j = i-1; j &gt;= 0; j--) &#123; if (arr[j] &gt; value) &#123; arr[j+1] = arr[j];//移动数据 &#125; else &#123; break; &#125; &#125; arr[j+1] = value; //插入数据 &#125;&#125; 从代码里我们可以看出，如果找到了合适的位置，就不会再进行比较了，就好比牌堆里抽出的一张牌本身就比我手里的牌都小，那么我只需要直接放在末尾就行了，不用一个一个去移动数据腾出位置插入到中间。 所以说，最好情况的时间复杂度是 O(n)，最坏情况的时间复杂度是 O(n2)，然而时间复杂度这个指标看的是最坏的情况，而不是最好的情况，所以插入排序的时间复杂度是 O(n2)。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>插入排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring中用到了哪些设计模式？]]></title>
    <url>%2F2017%2F08%2F05%2FSpring%E4%B8%AD%E7%94%A8%E5%88%B0%E4%BA%86%E5%93%AA%E4%BA%9B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[JDK 中用到了那些设计模式?Spring 中用到了那些设计模式?这两个问题，在面试中比较常见。我在网上搜索了一下关于 Spring 中设计模式的讲解几乎都是千篇一律，而且大部分都年代久远。所以，花了几天时间自己总结了一下，由于我的个人能力有限，文中如有任何错误各位都可以指出。另外，文章篇幅有限，对于设计模式以及一些源码的解读我只是一笔带过，这篇文章的主要目的是回顾一下 Spring 中的设计模式。 Design Patterns(设计模式) 表示面向对象软件开发中最好的计算机编程实践。 Spring 框架中广泛使用了不同类型的设计模式，下面我们来看看到底有哪些设计模式? 控制反转(IoC)和依赖注入(DI)IoC(Inversion of Control,控制翻转) 是Spring 中一个非常非常重要的概念，它不是什么技术，而是一种解耦的设计思想。它的主要目的是借助于“第三方”(Spring 中的 IOC 容器) 实现具有依赖关系的对象之间的解耦(IOC容易管理对象，你只管使用即可)，从而降低代码之间的耦合度。IOC 是一个原则，而不是一个模式，以下模式（但不限于）实现了IoC原则。 Spring IOC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 IOC 容器负责创建对象，将对象连接在一起，配置这些对象，并从创建中处理这些对象的整个生命周期，直到它们被完全销毁。 在实际项目中一个 Service 类如果有几百甚至上千个类作为它的底层，我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IOC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。关于Spring IOC 的理解，推荐看这一下知乎的一个回答：https://www.zhihu.com/question/23277575/answer/169698662 ，非常不错。 控制翻转怎么理解呢? 举个例子：”对象a 依赖了对象 b，当对象 a 需要使用 对象 b的时候必须自己去创建。但是当系统引入了 IOC 容器后， 对象a 和对象 b 之前就失去了直接的联系。这个时候，当对象 a 需要使用 对象 b的时候， 我们可以指定 IOC 容器去创建一个对象b注入到对象 a 中”。 对象 a 获得依赖对象 b 的过程,由主动行为变为了被动行为，控制权翻转，这就是控制反转名字的由来。 DI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去。 工厂设计模式Spring使用工厂模式可以通过 BeanFactory 或 ApplicationContext 创建 bean 对象。 两者对比： BeanFactory ：延迟注入(使用到某个 bean 的时候才会注入),相比于BeanFactory 来说会占用更少的内存，程序启动速度更快。 ApplicationContext ：容器启动的时候，不管你用没用到，一次性创建所有 bean 。BeanFactory 仅提供了最基本的依赖注入支持，ApplicationContext 扩展了 BeanFactory ,除了有BeanFactory的功能还有额外更多功能，所以一般开发人员使用ApplicationContext会更多。 ApplicationContext的三个实现类： ClassPathXmlApplication：把上下文文件当成类路径资源。 FileSystemXmlApplication：从文件系统中的 XML 文件载入上下文定义信息。 XmlWebApplicationContext：从Web系统中的XML文件载入上下文定义信息。 Example: 123456789101112import org.springframework.context.ApplicationContext;import org.springframework.context.support.FileSystemXmlApplicationContext; public class App &#123; public static void main(String[] args) &#123; ApplicationContext context = new FileSystemXmlApplicationContext( "C:/work/IOC Containers/springframework.applicationcontext/src/main/resources/bean-factory-config.xml"); HelloApplicationContext obj = (HelloApplicationContext) context.getBean("helloApplicationContext"); obj.getMsg(); &#125;&#125; 单例设计模式在我们的系统中，有一些对象其实我们只需要一个，比如说：线程池、缓存、对话框、注册表、日志对象、充当打印机、显卡等设备驱动程序的对象。事实上，这一类对象只能有一个实例，如果制造出多个实例就可能会导致一些问题的产生，比如：程序的行为异常、资源使用过量、或者不一致性的结果。 使用单例模式的好处: 对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销； 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。 Spring 中 bean 的默认作用域就是 singleton(单例)的。 除了 singleton 作用域，Spring 中 bean 还有下面几种作用域： prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。 session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。 global-session： 全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。Portlet是能够生成语义代码(例如：HTML)片段的小型Java Web插件。它们基于portlet容器，可以像servlet一样处理HTTP请求。但是，与 servlet 不同，每个 portlet 都有不同的会话 Spring 实现单例的方式： xml : &lt;bean id=&quot;userService&quot; class=&quot;top.snailclimb.UserService&quot; scope=&quot;singleton&quot;/&gt; 注解：@Scope(value = &quot;singleton&quot;) Spring 通过 ConcurrentHashMap 实现单例注册表的特殊方式实现单例模式。Spring 实现单例的核心代码如下 12345678910111213141516171819202122232425262728// 通过 ConcurrentHashMap（线程安全） 实现单例注册表private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;String, Object&gt;(64);public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(beanName, "'beanName' must not be null"); synchronized (this.singletonObjects) &#123; // 检查缓存中是否存在实例 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) &#123; //...省略了很多代码 try &#123; singletonObject = singletonFactory.getObject(); &#125; //...省略了很多代码 // 如果实例对象在不存在，我们注册到单例注册表中。 addSingleton(beanName, singletonObject); &#125; return (singletonObject != NULL_OBJECT ? singletonObject : null); &#125; &#125; //将对象添加到单例注册表 protected void addSingleton(String beanName, Object singletonObject) &#123; synchronized (this.singletonObjects) &#123; this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT)); &#125; &#125;&#125; 代理设计模式代理模式在 AOP 中的应用AOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 Spring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么Spring AOP会使用JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候Spring AOP会使用Cglib ，这时候Spring AOP会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示： 当然你也可以使用 AspectJ ,Spring AOP 已经集成了AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。 使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。 Spring AOP 和 AspectJ AOP 有什么区别?Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多。 模板方法模板方法模式是一种行为设计模式，它定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。 模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤的实现方式。 1234567891011121314151617181920212223242526272829public abstract class Template &#123; //这是我们的模板方法 public final void TemplateMethod()&#123; PrimitiveOperation1(); PrimitiveOperation2(); PrimitiveOperation3(); &#125; protected void PrimitiveOperation1()&#123; //当前类实现 &#125; //被子类实现的方法 protected abstract void PrimitiveOperation2(); protected abstract void PrimitiveOperation3();&#125;public class TemplateImpl extends Template &#123; @Override public void PrimitiveOperation2() &#123; //当前类实现 &#125; @Override public void PrimitiveOperation3() &#123; //当前类实现 &#125;&#125; Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。一般情况下，我们都是使用继承的方式来实现模板模式，但是 Spring 并没有使用这种方式，而是使用Callback 模式与模板方法模式配合，既达到了代码复用的效果，同时增加了灵活性。 观察者模式观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，这个对象所依赖的对象也会做出反应。Spring 事件驱动模型就是观察者模式很经典的一个应用。Spring 事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题。 Spring 事件驱动模型中的三种角色事件角色 ApplicationEvent (org.springframework.context包下)充当事件的角色,这是一个抽象类，它继承了java.util.EventObject并实现了 java.io.Serializable接口。 Spring 中默认存在以下事件，他们都是对 ApplicationContextEvent 的实现(继承自ApplicationContextEvent)： ContextStartedEvent：ApplicationContext 启动后触发的事件; ContextStoppedEvent：ApplicationContext 停止后触发的事件; ContextRefreshedEvent：ApplicationContext 初始化或刷新完成后触发的事件; ContextClosedEvent：ApplicationContext 关闭后触发的事件。 事件监听者角色ApplicationListener 充当了事件监听者角色，它是一个接口，里面只定义了一个 onApplicationEvent（）方法来处理ApplicationEvent。ApplicationListener接口类源码如下，可以看出接口定义看出接口中的事件只要实现了 ApplicationEvent就可以了。所以，在 Spring中我们只要实现 ApplicationListener 接口实现 onApplicationEvent() 方法即可完成监听事件 123456package org.springframework.context;import java.util.EventListener;@FunctionalInterfacepublic interface ApplicationListener&lt;E extends ApplicationEvent&gt; extends EventListener &#123; void onApplicationEvent(E var1);&#125; 事件发布者角色ApplicationEventPublisher 充当了事件的发布者，它也是一个接口。 12345678@FunctionalInterfacepublic interface ApplicationEventPublisher &#123; default void publishEvent(ApplicationEvent event) &#123; this.publishEvent((Object)event); &#125; void publishEvent(Object var1);&#125; ApplicationEventPublisher 接口的publishEvent（）这个方法在AbstractApplicationContext类中被实现，阅读这个方法的实现，你会发现实际上事件真正是通过ApplicationEventMulticaster来广播出去的。具体内容过多，就不在这里分析了，后面可能会单独写一篇文章提到。 Spring 的事件流程总结 定义一个事件: 实现一个继承自 ApplicationEvent，并且写相应的构造函数； 定义一个事件监听者：实现 ApplicationListener 接口，重写 onApplicationEvent() 方法； 使用事件发布者发布消息: 可以通过 ApplicationEventPublisher 的 publishEvent() 方法发布消息。 Example: 12345678910111213141516171819202122232425262728293031323334353637383940// 定义一个事件,继承自ApplicationEvent并且写相应的构造函数public class DemoEvent extends ApplicationEvent&#123; private static final long serialVersionUID = 1L; private String message; public DemoEvent(Object source,String message)&#123; super(source); this.message = message; &#125; public String getMessage() &#123; return message; &#125; // 定义一个事件监听者,实现ApplicationListener接口，重写 onApplicationEvent() 方法；@Componentpublic class DemoListener implements ApplicationListener&lt;DemoEvent&gt;&#123; //使用onApplicationEvent接收消息 @Override public void onApplicationEvent(DemoEvent event) &#123; String msg = event.getMessage(); System.out.println("接收到的信息是："+msg); &#125;&#125;// 发布事件，可以通过ApplicationEventPublisher 的 publishEvent() 方法发布消息。@Componentpublic class DemoPublisher &#123; @Autowired ApplicationContext applicationContext; public void publish(String message)&#123; //发布事件 applicationContext.publishEvent(new DemoEvent(this, message)); &#125;&#125; 当调用 DemoPublisher 的 publish() 方法的时候，比如 demoPublisher.publish(&quot;你好&quot;) ，控制台就会打印出:接收到的信息是：你好 。 适配器模式适配器模式(Adapter Pattern) 将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作，其别名为包装器(Wrapper)。 spring AOP中的适配器模式我们知道 Spring AOP 的实现是基于代理模式，但是 Spring AOP 的增强或通知(Advice)使用到了适配器模式，与之相关的接口是AdvisorAdapter 。Advice 常用的类型有：BeforeAdvice（目标方法调用前,前置通知）、AfterAdvice（目标方法调用后,后置通知）、AfterReturningAdvice(目标方法执行结束后，return之前)等等。每个类型Advice（通知）都有对应的拦截器:MethodBeforeAdviceInterceptor、AfterReturningAdviceAdapter、AfterReturningAdviceInterceptor。Spring预定义的通知要通过对应的适配器，适配成 MethodInterceptor接口(方法拦截器)类型的对象（如：MethodBeforeAdviceInterceptor 负责适配 MethodBeforeAdvice）。 spring MVC中的适配器模式在Spring MVC中，DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由HandlerAdapter 适配器处理。HandlerAdapter 作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller 作为需要适配的类。 为什么要在 Spring MVC 中使用适配器模式？ Spring MVC 中的 Controller 种类众多，不同类型的 Controller 通过不同的方法来对请求进行处理。如果不利用适配器模式的话，DispatcherServlet 直接获取对应类型的 Controller，需要的自行来判断，像下面这段代码一样： 1234567if(mappedHandler.getHandler() instanceof MultiActionController)&#123; ((MultiActionController)mappedHandler.getHandler()).xxx &#125;else if(mappedHandler.getHandler() instanceof XXX)&#123; ... &#125;else if(...)&#123; ... &#125; 假如我们再增加一个 Controller类型就要在上面代码中再加入一行 判断语句，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则 – 对扩展开放，对修改关闭。 装饰者模式装饰者模式可以动态地给对象添加一些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个Decorator套在原有代码外面。其实在 JDK 中就有很多地方用到了装饰者模式，比如 InputStream家族，InputStream 类下有 FileInputStream (读取文件)、BufferedInputStream (增加缓存,使读取文件速度大大提升)等子类都在不修改InputStream 代码的情况下扩展了它的功能。 Spring 中配置 DataSource 的时候，DataSource 可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下动态切换不同的数据源？这个时候就要用到装饰者模式(这一点我自己还没太理解具体原理)。Spring 中用到的包装器模式在类名上含有 Wrapper或者 Decorator。这些类基本上都是动态地给一个对象添加一些额外的职责 总结Spring 框架中用到了哪些设计模式？ 工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 …… 参考 《Spring技术内幕》 https://blog.eduonix.com/java-programming-2/learn-design-patterns-used-spring-framework/ http://blog.yeamin.top/2018/03/27/单例模式-Spring单例实现原理分析/ https://www.tutorialsteacher.com/ioc/inversion-of-control https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.html https://juejin.im/post/5a8eb261f265da4e9e307230 https://juejin.im/post/5ba28986f265da0abc2b6084]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBean的作用域与生命周期]]></title>
    <url>%2F2017%2F08%2F01%2FSpringBean%E7%9A%84%E4%BD%9C%E7%94%A8%E5%9F%9F%E4%B8%8E%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[前言在 Spring 中，那些组成应用程序的主体及由 Spring IOC 容器所管理的对象，被称之为 bean。简单地讲，bean 就是由 IOC 容器初始化、装配及管理的对象，除此之外，bean 就与应用程序中的其他对象没有什么区别了。而 bean 的定义以及 bean 相互间的依赖关系将通过配置元数据来描述。 Spring中的bean默认都是单例的，这些单例Bean在多线程程序下如何保证线程安全呢？ 例如对于Web应用来说，Web容器对于每个用户请求都创建一个单独的Sevlet线程来处理请求，引入Spring框架之后，每个Action都是单例的，那么对于Spring托管的单例Service Bean，如何保证其安全呢？ Spring的单例是基于BeanFactory也就是Spring容器的，单例Bean在此容器内只有一个，Java的单例是基于 JVM，每个 JVM 内只有一个实例。 在大多数情况下。单例 bean 是很理想的方案。不过，有时候你可能会发现你所使用的类是易变的，它们会保持一些状态，因此重用是不安全的。在这种情况下，将 class 声明为单例的就不是那么明智了。因为对象会被污染，稍后重用的时候会出现意想不到的问题。所以 Spring 定义了多种作用域的bean。 一 bean的作用域创建一个bean定义，其实质是用该bean定义对应的类来创建真正实例的“配方”。把bean定义看成一个配方很有意义，它与class很类似，只根据一张“处方”就可以创建多个实例。不仅可以控制注入到对象中的各种依赖和配置值，还可以控制该对象的作用域。这样可以灵活选择所建对象的作用域，而不必在Java Class级定义作用域。Spring Framework支持五种作用域，分别阐述如下表。 五种作用域中，request、session 和 global session 三种作用域仅在基于web的应用中使用（不必关心你所采用的是什么web应用框架），只能用在基于 web 的 Spring ApplicationContext 环境。 1. singleton——唯一 bean 实例当一个 bean 的作用域为 singleton，那么Spring IoC容器中只会存在一个共享的 bean 实例，并且所有对 bean 的请求，只要 id 与该 bean 定义相匹配，则只会返回bean的同一实例。 singleton 是单例类型(对应于单例模式)，就是在创建起容器时就同时自动创建了一个bean的对象，不管你是否使用，但我们可以指定Bean节点的 lazy-init=”true” 来延迟初始化bean，这时候，只有在第一次获取bean时才会初始化bean，即第一次请求该bean时才初始化。 每次获取到的对象都是同一个对象。注意，singleton 作用域是Spring中的缺省作用域。要在XML中将 bean 定义成 singleton ，可以这样配置： 1&lt;bean id="ServiceImpl" class="cn.csdn.service.ServiceImpl" scope="singleton"&gt; 也可以通过 @Scope 注解（它可以显示指定bean的作用范围。）的方式 12345@Service@Scope("singleton")public class ServiceImpl&#123;&#125; 2. prototype——每次请求都会创建一个新的 bean 实例当一个bean的作用域为 prototype，表示一个 bean 定义对应多个对象实例。 prototype 作用域的 bean 会导致在每次对该 bean 请求（将其注入到另一个 bean 中，或者以程序的方式调用容器的 getBean() 方法）时都会创建一个新的 bean 实例。prototype 是原型类型，它在我们创建容器的时候并没有实例化，而是当我们获取bean的时候才会去创建一个对象，而且我们每次获取到的对象都不是同一个对象。根据经验，对有状态的 bean 应该使用 prototype 作用域，而对无状态的 bean 则应该使用 singleton 作用域。 在 XML 中将 bean 定义成 prototype ，可以这样配置： 123&lt;bean id="account" class="com.foo.DefaultAccount" scope="prototype"/&gt; 或者&lt;bean id="account" class="com.foo.DefaultAccount" singleton="false"/&gt; 通过 @Scope 注解的方式实现就不做演示了。 3. request——每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效request只适用于Web程序，每一次 HTTP 请求都会产生一个新的bean，同时该bean仅在当前HTTP request内有效，当请求结束后，该对象的生命周期即告结束。 在 XML 中将 bean 定义成 request ，可以这样配置： 1&lt;bean id="loginAction" class=cn.csdn.LoginAction" scope="request"/&gt; 4. session——每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效session只适用于Web程序，session 作用域表示该针对每一次 HTTP 请求都会产生一个新的 bean，同时该 bean 仅在当前 HTTP session 内有效.与request作用域一样，可以根据需要放心的更改所创建实例的内部状态，而别的 HTTP session 中根据 userPreferences 创建的实例，将不会看到这些特定于某个 HTTP session 的状态变化。当HTTP session最终被废弃的时候，在该HTTP session作用域内的bean也会被废弃掉。 1&lt;bean id="userPreferences" class="com.foo.UserPreferences" scope="session"/&gt; 5. globalSessionglobal session 作用域类似于标准的 HTTP session 作用域，不过仅仅在基于 portlet 的 web 应用中才有意义。Portlet 规范定义了全局 Session 的概念，它被所有构成某个 portlet web 应用的各种不同的 portle t所共享。在global session 作用域中定义的 bean 被限定于全局portlet Session的生命周期范围内。 1&lt;bean id="user" class="com.foo.Preferences "scope="globalSession"/&gt; 二 bean的生命周期Spring Bean是Spring应用中最最重要的部分了。所以来看看Spring容器在初始化一个bean的时候会做那些事情，顺序是怎样的，在容器关闭的时候，又会做哪些事情。 spring版本：4.2.3.RELEASE鉴于Spring源码是用gradle构建的，我也决定舍弃我大maven，尝试下洪菊推荐过的gradle。运行beanLifeCycle模块下的junit test即可在控制台看到如下输出，可以清楚了解Spring容器在创建，初始化和销毁Bean的时候依次做了那些事情。 12345678910111213141516171819202122232425Spring容器初始化=====================================调用GiraffeService无参构造函数GiraffeService中利用set方法设置属性值调用setBeanName:: Bean Name defined in context=giraffeService调用setBeanClassLoader,ClassLoader Name = sun.misc.Launcher$AppClassLoader调用setBeanFactory,setBeanFactory:: giraffe bean singleton=true调用setEnvironment调用setResourceLoader:: Resource File Name=spring-beans.xml调用setApplicationEventPublisher调用setApplicationContext:: Bean Definition Names=[giraffeService, org.springframework.context.annotation.CommonAnnotationBeanPostProcessor#0, com.giraffe.spring.service.GiraffeServicePostProcessor#0]执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName=giraffeService调用PostConstruct注解标注的方法执行InitializingBean接口的afterPropertiesSet方法执行配置的init-method执行BeanPostProcessor的postProcessAfterInitialization方法,beanName=giraffeServiceSpring容器初始化完毕=====================================从容器中获取Beangiraffe Name=李光洙=====================================调用preDestroy注解标注的方法执行DisposableBean接口的destroy方法执行配置的destroy-methodSpring容器关闭 先来看看，Spring在Bean从创建到销毁的生命周期中可能做得事情。 initialization 和 destroy有时我们需要在Bean属性值set好之后和Bean销毁之前做一些事情，比如检查Bean中某个属性是否被正常的设置好值了。Spring框架提供了多种方法让我们可以在Spring Bean的生命周期中执行initialization和pre-destroy方法。 1.实现InitializingBean和DisposableBean接口 这两个接口都只包含一个方法。通过实现InitializingBean接口的afterPropertiesSet()方法可以在Bean属性值设置好之后做一些操作，实现DisposableBean接口的destroy()方法可以在销毁Bean之前做一些操作。 例子如下： 12345678910public class GiraffeService implements InitializingBean,DisposableBean &#123; @Override public void afterPropertiesSet() throws Exception &#123; System.out.println("执行InitializingBean接口的afterPropertiesSet方法"); &#125; @Override public void destroy() throws Exception &#123; System.out.println("执行DisposableBean接口的destroy方法"); &#125;&#125; 这种方法比较简单，但是不建议使用。因为这样会将Bean的实现和Spring框架耦合在一起。 2.在bean的配置文件中指定init-method和destroy-method方法 Spring允许我们创建自己的 init 方法和 destroy 方法，只要在 Bean 的配置文件中指定 init-method 和 destroy-method 的值就可以在 Bean 初始化时和销毁之前执行一些操作。 例子如下： 12345678910public class GiraffeService &#123; //通过&lt;bean&gt;的destroy-method属性指定的销毁方法 public void destroyMethod() throws Exception &#123; System.out.println("执行配置的destroy-method"); &#125; //通过&lt;bean&gt;的init-method属性指定的初始化方法 public void initMethod() throws Exception &#123; System.out.println("执行配置的init-method"); &#125;&#125; 配置文件中的配置： 12&lt;bean name=&quot;giraffeService&quot; class=&quot;com.giraffe.spring.service.GiraffeService&quot; init-method=&quot;initMethod&quot; destroy-method=&quot;destroyMethod&quot;&gt;&lt;/bean&gt; 需要注意的是自定义的init-method和post-method方法可以抛异常但是不能有参数。 这种方式比较推荐，因为可以自己创建方法，无需将Bean的实现直接依赖于spring的框架。 3.使用@PostConstruct和@PreDestroy注解 除了xml配置的方式，Spring 也支持用 @PostConstruct和 @PreDestroy注解来指定 init 和 destroy 方法。这两个注解均在javax.annotation 包中。为了注解可以生效，需要在配置文件中定义org.springframework.context.annotation.CommonAnnotationBeanPostProcessor或context:annotation-config 例子如下： 12345678910public class GiraffeService &#123; @PostConstruct public void initPostConstruct()&#123; System.out.println("执行PostConstruct注解标注的方法"); &#125; @PreDestroy public void preDestroy()&#123; System.out.println("执行preDestroy注解标注的方法"); &#125;&#125; 配置文件： 12 &lt;bean class="org.springframework.context.annotation.CommonAnnotationBeanPostProcessor" /&gt; 实现*Aware接口 在Bean中使用Spring框架的一些对象有些时候我们需要在 Bean 的初始化中使用 Spring 框架自身的一些对象来执行一些操作，比如获取 ServletContext 的一些参数，获取 ApplicaitionContext 中的 BeanDefinition 的名字，获取 Bean 在容器中的名字等等。为了让 Bean 可以获取到框架自身的一些对象，Spring 提供了一组名为*Aware的接口。 这些接口均继承于org.springframework.beans.factory.Aware标记接口，并提供一个将由 Bean 实现的set*方法,Spring通过基于setter的依赖注入方式使相应的对象可以被Bean使用。网上说，这些接口是利用观察者模式实现的，类似于servlet listeners，目前还不明白，不过这也不在本文的讨论范围内。介绍一些重要的Aware接口： ApplicationContextAware: 获得ApplicationContext对象,可以用来获取所有Bean definition的名字。 BeanFactoryAware:获得BeanFactory对象，可以用来检测Bean的作用域。 BeanNameAware:获得Bean在配置文件中定义的名字。 ResourceLoaderAware:获得ResourceLoader对象，可以获得classpath中某个文件。 ServletContextAware:在一个MVC应用中可以获取ServletContext对象，可以读取context中的参数。 ServletConfigAware： 在一个MVC应用中可以获取ServletConfig对象，可以读取config中的参数。 12345678910111213141516171819202122232425262728293031323334353637383940public class GiraffeService implements ApplicationContextAware, ApplicationEventPublisherAware, BeanClassLoaderAware, BeanFactoryAware, BeanNameAware, EnvironmentAware, ImportAware, ResourceLoaderAware&#123; @Override public void setBeanClassLoader(ClassLoader classLoader) &#123; System.out.println("执行setBeanClassLoader,ClassLoader Name = " + classLoader.getClass().getName()); &#125; @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; System.out.println("执行setBeanFactory,setBeanFactory:: giraffe bean singleton=" + beanFactory.isSingleton("giraffeService")); &#125; @Override public void setBeanName(String s) &#123; System.out.println("执行setBeanName:: Bean Name defined in context=" + s); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; System.out.println("执行setApplicationContext:: Bean Definition Names=" + Arrays.toString(applicationContext.getBeanDefinitionNames())); &#125; @Override public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) &#123; System.out.println("执行setApplicationEventPublisher"); &#125; @Override public void setEnvironment(Environment environment) &#123; System.out.println("执行setEnvironment"); &#125; @Override public void setResourceLoader(ResourceLoader resourceLoader) &#123; Resource resource = resourceLoader.getResource("classpath:spring-beans.xml"); System.out.println("执行setResourceLoader:: Resource File Name=" + resource.getFilename()); &#125; @Override public void setImportMetadata(AnnotationMetadata annotationMetadata) &#123; System.out.println("执行setImportMetadata"); &#125;&#125; BeanPostProcessor上面的*Aware接口是针对某个实现这些接口的Bean定制初始化的过程，Spring同样可以针对容器中的所有Bean，或者某些Bean定制初始化过程，只需提供一个实现BeanPostProcessor接口的类即可。 该接口中包含两个方法，postProcessBeforeInitialization和postProcessAfterInitialization。 postProcessBeforeInitialization方法会在容器中的Bean初始化之前执行， postProcessAfterInitialization方法在容器中的Bean初始化之后执行。 例子如下： 123456789101112public class CustomerBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("执行BeanPostProcessor的postProcessBeforeInitialization方法,beanName=" + beanName); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; System.out.println("执行BeanPostProcessor的postProcessAfterInitialization方法,beanName=" + beanName); return bean; &#125;&#125; 要将BeanPostProcessor的Bean像其他Bean一样定义在配置文件中 1&lt;bean class="com.giraffe.spring.service.CustomerBeanPostProcessor"/&gt; 总结所以。。。结合第一节控制台输出的内容，Spring Bean的生命周期是这样纸的： Bean容器找到配置文件中 Spring Bean 的定义。 Bean容器利用Java Reflection API创建一个Bean的实例。 如果涉及到一些属性值 利用set方法设置一些属性值。 如果Bean实现了BeanNameAware接口，调用setBeanName()方法，传入Bean的名字。 如果Bean实现了BeanClassLoaderAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 如果Bean实现了BeanFactoryAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 与上面的类似，如果实现了其他*Aware接口，就调用相应的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessBeforeInitialization()方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果Bean在配置文件中的定义包含init-method属性，执行指定的方法。 如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessAfterInitialization()方法 当要销毁Bean的时候，如果Bean实现了DisposableBean接口，执行destroy()方法。 当要销毁Bean的时候，如果Bean在配置文件中的定义包含destroy-method属性，执行指定的方法。 用图表示一下(图来源:http://www.jianshu.com/p/d00539babca5)： 与之比较类似的中文版本: 其实很多时候我们并不会真的去实现上面说描述的那些接口，那么下面我们就除去那些接口，针对bean的单例和非单例来描述下bean的生命周期： 单例管理的对象当scope=”singleton”，即默认情况下，会在启动容器时（即实例化容器时）时实例化。但我们可以指定Bean节点的lazy-init=”true”来延迟初始化bean，这时候，只有在第一次获取bean时才会初始化bean，即第一次请求该bean时才初始化。如下配置： 1&lt;bean id="ServiceImpl" class="cn.csdn.service.ServiceImpl" lazy-init="true"/&gt; 如果想对所有的默认单例bean都应用延迟初始化，可以在根节点beans设置default-lazy-init属性为true，如下所示： 1&lt;beans default-lazy-init="true" …&gt; 默认情况下，Spring 在读取 xml 文件的时候，就会创建对象。在创建对象的时候先调用构造器，然后调用 init-method 属性值中所指定的方法。对象在被销毁的时候，会调用 destroy-method 属性值中所指定的方法（例如调用Container.destroy()方法的时候）。写一个测试类，代码如下： 1234567891011121314151617181920212223public class LifeBean &#123; private String name; public LifeBean()&#123; System.out.println("LifeBean()构造函数"); &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; System.out.println("setName()"); this.name = name; &#125; public void init()&#123; System.out.println("this is init of lifeBean"); &#125; public void destory()&#123; System.out.println("this is destory of lifeBean " + this); &#125; &#125; life.xml配置如下： 12&lt;bean id="life_singleton" class="com.bean.LifeBean" scope="singleton" init-method="init" destroy-method="destory" lazy-init="true"/&gt; 测试代码： 12345678910public class LifeTest &#123; @Test public void test() &#123; AbstractApplicationContext container = new ClassPathXmlApplicationContext("life.xml"); LifeBean life1 = (LifeBean)container.getBean("life"); System.out.println(life1); container.close(); &#125;&#125; 运行结果： 12345LifeBean()构造函数this is init of lifeBeancom.bean.LifeBean@573f2bb1……this is destory of lifeBean com.bean.LifeBean@573f2bb1 非单例管理的对象当scope=”prototype”时，容器也会延迟初始化 bean，Spring 读取xml 文件的时候，并不会立刻创建对象，而是在第一次请求该 bean 时才初始化（如调用getBean方法时）。在第一次请求每一个 prototype 的bean 时，Spring容器都会调用其构造器创建这个对象，然后调用init-method属性值中所指定的方法。对象销毁的时候，Spring 容器不会帮我们调用任何方法，因为是非单例，这个类型的对象有很多个，Spring容器一旦把这个对象交给你之后，就不再管理这个对象了。 为了测试prototype bean的生命周期life.xml配置如下： 1&lt;bean id="life_prototype" class="com.bean.LifeBean" scope="prototype" init-method="init" destroy-method="destory"/&gt; 测试程序： 123456789101112public class LifeTest &#123; @Test public void test() &#123; AbstractApplicationContext container = new ClassPathXmlApplicationContext("life.xml"); LifeBean life1 = (LifeBean)container.getBean("life_singleton"); System.out.println(life1); LifeBean life3 = (LifeBean)container.getBean("life_prototype"); System.out.println(life3); container.close(); &#125;&#125; 运行结果： 12345678LifeBean()构造函数this is init of lifeBeancom.bean.LifeBean@573f2bb1LifeBean()构造函数this is init of lifeBeancom.bean.LifeBean@5ae9a829……this is destory of lifeBean com.bean.LifeBean@573f2bb1 可以发现，对于作用域为 prototype 的 bean ，其destroy方法并没有被调用。如果 bean 的 scope 设为prototype时，当容器关闭时，destroy 方法不会被调用。对于 prototype 作用域的 bean，有一点非常重要，那就是 Spring不能对一个 prototype bean 的整个生命周期负责：容器在初始化、配置、装饰或者是装配完一个prototype实例后，将它交给客户端，随后就对该prototype实例不闻不问了。 不管何种作用域，容器都会调用所有对象的初始化生命周期回调方法。但对prototype而言，任何配置好的析构生命周期回调方法都将不会被调用。清除prototype作用域的对象并释放任何prototype bean所持有的昂贵资源，都是客户端代码的职责（让Spring容器释放被prototype作用域bean占用资源的一种可行方式是，通过使用bean的后置处理器，该处理器持有要被清除的bean的引用）。谈及prototype作用域的bean时，在某些方面你可以将Spring容器的角色看作是Java new操作的替代者，任何迟于该时间点的生命周期事宜都得交由客户端来处理。 Spring 容器可以管理 singleton 作用域下 bean 的生命周期，在此作用域下，Spring 能够精确地知道bean何时被创建，何时初始化完成，以及何时被销毁。而对于 prototype 作用域的bean，Spring只负责创建，当容器创建了 bean 的实例后，bean 的实例就交给了客户端的代码管理，Spring容器将不再跟踪其生命周期，并且不会管理那些被配置成prototype作用域的bean的生命周期。 三 说明本文的完成结合了下面两篇文章，并做了相应修改： https://blog.csdn.net/fuzhongmin05/article/details/73389779 https://yemengying.com/2016/07/14/spring-bean-life-cycle/ 由于本文非本人独立原创，所以未声明为原创！在此说明！]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Bean</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BIO,NIO,AIO 总结]]></title>
    <url>%2F2017%2F08%2F01%2FBIO-NIO-AIO%2F</url>
    <content type="text"><![CDATA[BIO,NIO,AIO 总结 Java 中的 BIO、NIO和 AIO 理解为是 Java 语言对操作系统的各种 IO 模型的封装。程序员在使用这些 API 的时候，不需要关心操作系统层面的知识，也不需要根据不同操作系统编写不同的代码。只需要使用Java的API就可以了。 在讲 BIO,NIO,AIO 之前先来回顾一下这样几个概念：同步与异步，阻塞与非阻塞。 同步与异步 同步： 同步就是发起一个调用后，被调用者未处理完请求之前，调用不返回。 异步： 异步就是发起一个调用后，立刻得到被调用者的回应表示已接收到请求，但是被调用者并没有返回结果，此时我们可以处理其他的请求，被调用者通常依靠事件，回调等机制来通知调用者其返回结果。 同步和异步的区别最大在于异步的话调用者不需要等待处理结果，被调用者会通过回调等机制来通知调用者其返回结果。 阻塞和非阻塞 阻塞： 阻塞就是发起一个请求，调用者一直等待请求结果返回，也就是当前线程会被挂起，无法从事其他任务，只有当条件就绪才能继续。 非阻塞： 非阻塞就是发起一个请求，调用者不用一直等着结果返回，可以先去干其他事情。 举个生活中简单的例子，你妈妈让你烧水，小时候你比较笨啊，在那里傻等着水开（同步阻塞）。等你稍微再长大一点，你知道每次烧水的空隙可以去干点其他事，然后只需要时不时来看看水开了没有（同步非阻塞）。后来，你们家用上了水开了会发出声音的壶，这样你就只需要听到响声后就知道水开了，在这期间你可以随便干自己的事情，你需要去倒水了（异步非阻塞）。 1. BIO (Blocking I/O)同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。 1.1 传统 BIOBIO通信（一请求一应答）模型图如下(图源网络，原出处不明)： 采用 BIO 通信模型 的服务端，通常由一个独立的 Acceptor 线程负责监听客户端的连接。我们一般通过在while(true) 循环中服务端会调用 accept() 方法等待接收客户端的连接的方式监听请求，请求一旦接收到一个连接请求，就可以建立通信套接字在这个通信套接字上进行读写操作，此时不能再接收其他客户端连接请求，只能等待同当前连接的客户端的操作执行完成， 不过可以通过多线程来支持多个客户端的连接，如上图所示。 如果要让 BIO 通信模型 能够同时处理多个客户端请求，就必须使用多线程（主要原因是socket.accept()、socket.read()、socket.write() 涉及的三个主要函数都是同步阻塞的），也就是说它在接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理，处理完成之后，通过输出流返回应答给客户端，线程销毁。这就是典型的 一请求一应答通信模型 。我们可以设想一下如果这个连接不做任何事情的话就会造成不必要的线程开销，不过可以通过 线程池机制 改善，线程池还可以让线程的创建和回收成本相对较低。使用FixedThreadPool 可以有效的控制了线程的最大数量，保证了系统有限的资源的控制，实现了N(客户端请求数量):M(处理客户端请求的线程数量)的伪异步I/O模型（N 可以远远大于 M），下面一节”伪异步 BIO”中会详细介绍到。 我们再设想一下当客户端并发访问量增加后这种模型会出现什么问题？ 在 Java 虚拟机中，线程是宝贵的资源，线程的创建和销毁成本很高，除此之外，线程的切换成本也是很高的。尤其在 Linux 这样的操作系统中，线程本质上就是一个进程，创建和销毁线程都是重量级的系统函数。如果并发访问量增加会导致线程数急剧膨胀可能会导致线程堆栈溢出、创建新线程失败等问题，最终导致进程宕机或者僵死，不能对外提供服务。 1.2 伪异步 IO为了解决同步阻塞I/O面临的一个链路需要一个线程处理的问题，后来有人对它的线程模型进行了优化一一一后端通过一个线程池来处理多个客户端的请求接入，形成客户端个数M：线程池最大线程数N的比例关系，其中M可以远远大于N.通过线程池可以灵活地调配线程资源，设置线程的最大值，防止由于海量并发接入导致线程耗尽。 伪异步IO模型图(图源网络，原出处不明)： 采用线程池和任务队列可以实现一种叫做伪异步的 I/O 通信框架，它的模型图如上图所示。当有新的客户端接入时，将客户端的 Socket 封装成一个Task（该任务实现java.lang.Runnable接口）投递到后端的线程池中进行处理，JDK 的线程池维护一个消息队列和 N 个活跃线程，对消息队列中的任务进行处理。由于线程池可以设置消息队列的大小和最大线程数，因此，它的资源占用是可控的，无论多少个客户端并发访问，都不会导致资源的耗尽和宕机。 伪异步I/O通信框架采用了线程池实现，因此避免了为每个请求都创建一个独立线程造成的线程资源耗尽问题。不过因为它的底层仍然是同步阻塞的BIO模型，因此无法从根本上解决问题。 1.3 代码示例下面代码中演示了BIO通信（一请求一应答）模型。我们会在客户端创建多个线程依次连接服务端并向其发送”当前时间+:hello world”，服务端会为每个客户端线程创建一个线程来处理。代码示例出自闪电侠的博客，原地址如下： https://www.jianshu.com/p/a4e03835921a 客户端 123456789101112131415161718192021222324252627/** * * @author 闪电侠 * @date 2018年10月14日 * @Description:客户端 */public class IOClient &#123; public static void main(String[] args) &#123; // TODO 创建多个线程，模拟多个客户端连接服务端 new Thread(() -&gt; &#123; try &#123; Socket socket = new Socket("127.0.0.1", 3333); while (true) &#123; try &#123; socket.getOutputStream().write((new Date() + ": hello world").getBytes()); Thread.sleep(2000); &#125; catch (Exception e) &#123; &#125; &#125; &#125; catch (IOException e) &#123; &#125; &#125;).start(); &#125;&#125; 服务端 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @author 闪电侠 * @date 2018年10月14日 * @Description: 服务端 */public class IOServer &#123; public static void main(String[] args) throws IOException &#123; // TODO 服务端处理客户端连接请求 ServerSocket serverSocket = new ServerSocket(3333); // 接收到客户端连接请求之后为每个客户端创建一个新的线程进行链路处理 new Thread(() -&gt; &#123; while (true) &#123; try &#123; // 阻塞方法获取新的连接 Socket socket = serverSocket.accept(); // 每一个新的连接都创建一个线程，负责读取数据 new Thread(() -&gt; &#123; try &#123; int len; byte[] data = new byte[1024]; InputStream inputStream = socket.getInputStream(); // 按字节流方式读取数据 while ((len = inputStream.read(data)) != -1) &#123; System.out.println(new String(data, 0, len)); &#125; &#125; catch (IOException e) &#123; &#125; &#125;).start(); &#125; catch (IOException e) &#123; &#125; &#125; &#125;).start(); &#125;&#125; 1.4 总结在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 2. NIO (New I/O)2.1 NIO 简介 NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了NIO框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。 NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发。 2.2 NIO的特性/NIO与IO区别如果是在面试中回答这个问题，我觉得首先肯定要从 NIO 流是非阻塞 IO 而 IO 流是阻塞 IO 说起。然后，可以从 NIO 的3个核心组件/特性为 NIO 带来的一些改进来分析。如果，你把这些都回答上了我觉得你对于 NIO 就有了更为深入一点的认识，面试官问到你这个问题，你也能很轻松的回答上来了。 1)Non-blocking IO（非阻塞IO）IO流是阻塞的，NIO流是不阻塞的。 Java NIO使我们可以进行非阻塞IO操作。比如说，单线程中从通道读取数据到buffer，同时可以继续做别的事情，当数据读取到buffer中后，线程再继续处理数据。写数据也是一样的。另外，非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 Java IO的各种流是阻塞的。这意味着，当一个线程调用 read() 或 write() 时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了 2)Buffer(缓冲区)IO 面向流(Stream oriented)，而 NIO 面向缓冲区(Buffer oriented)。 Buffer是一个对象，它包含一些要写入或者要读出的数据。在NIO类库中加入Buffer对象，体现了新库与原I/O的一个重要区别。在面向流的I/O中·可以将数据直接写入或者将数据直接读到 Stream 对象中。虽然 Stream 中也有 Buffer 开头的扩展类，但只是流的包装类，还是从流读到缓冲区，而 NIO 却是直接读到 Buffer 中进行操作。 在NIO厍中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的; 在写入数据时，写入到缓冲区中。任何时候访问NIO中的数据，都是通过缓冲区进行操作。 最常用的缓冲区是 ByteBuffer,一个 ByteBuffer 提供了一组功能用于操作 byte 数组。除了ByteBuffer,还有其他的一些缓冲区，事实上，每一种Java基本类型（除了Boolean类型）都对应有一种缓冲区。 3)Channel (通道)NIO 通过Channel（通道） 进行读写。 通道是双向的，可读也可写，而流的读写是单向的。无论读写，通道只能和Buffer交互。因为 Buffer，通道可以异步地读写。 4)Selectors(选择器)NIO有选择器，而IO没有。 选择器用于使用单个线程处理多个通道。因此，它需要较少的线程来处理这些通道。线程之间的切换对于操作系统来说是昂贵的。 因此，为了提高系统效率选择器是有用的。 2.3 NIO 读数据和写数据方式通常来说NIO中的所有IO都是从 Channel（通道） 开始的。 从通道进行数据读取 ：创建一个缓冲区，然后请求通道读取数据。 从通道进行数据写入 ：创建一个缓冲区，填充数据，并要求通道写入数据。 数据读取和写入操作图示： 2.4 NIO核心组件简单介绍NIO 包含下面几个核心的组件： Channel(通道) Buffer(缓冲区) Selector(选择器) 整个NIO体系包含的类远远不止这三个，只能说这三个是NIO体系的“核心API”。我们上面已经对这三个概念进行了基本的阐述，这里就不多做解释了。 2.5 代码示例代码示例出自闪电侠的博客，原地址如下： https://www.jianshu.com/p/a4e03835921a 客户端 IOClient.java 的代码不变，我们对服务端使用 NIO 进行改造。以下代码较多而且逻辑比较复杂，大家看看就好。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * * @author 闪电侠 * @date 2019年2月21日 * @Description: NIO 改造后的服务端 */public class NIOServer &#123; public static void main(String[] args) throws IOException &#123; // 1. serverSelector负责轮询是否有新的连接，服务端监测到新的连接之后，不再创建一个新的线程， // 而是直接将新连接绑定到clientSelector上，这样就不用 IO 模型中 1w 个 while 循环在死等 Selector serverSelector = Selector.open(); // 2. clientSelector负责轮询连接是否有数据可读 Selector clientSelector = Selector.open(); new Thread(() -&gt; &#123; try &#123; // 对应IO编程中服务端启动 ServerSocketChannel listenerChannel = ServerSocketChannel.open(); listenerChannel.socket().bind(new InetSocketAddress(3333)); listenerChannel.configureBlocking(false); listenerChannel.register(serverSelector, SelectionKey.OP_ACCEPT); while (true) &#123; // 监测是否有新的连接，这里的1指的是阻塞的时间为 1ms if (serverSelector.select(1) &gt; 0) &#123; Set&lt;SelectionKey&gt; set = serverSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isAcceptable()) &#123; try &#123; // (1) // 每来一个新连接，不需要创建一个线程，而是直接注册到clientSelector SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept(); clientChannel.configureBlocking(false); clientChannel.register(clientSelector, SelectionKey.OP_READ); &#125; finally &#123; keyIterator.remove(); &#125; &#125; &#125; &#125; &#125; &#125; catch (IOException ignored) &#123; &#125; &#125;).start(); new Thread(() -&gt; &#123; try &#123; while (true) &#123; // (2) 批量轮询是否有哪些连接有数据可读，这里的1指的是阻塞的时间为 1ms if (clientSelector.select(1) &gt; 0) &#123; Set&lt;SelectionKey&gt; set = clientSelector.selectedKeys(); Iterator&lt;SelectionKey&gt; keyIterator = set.iterator(); while (keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if (key.isReadable()) &#123; try &#123; SocketChannel clientChannel = (SocketChannel) key.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(1024); // (3) 面向 Buffer clientChannel.read(byteBuffer); byteBuffer.flip(); System.out.println( Charset.defaultCharset().newDecoder().decode(byteBuffer).toString()); &#125; finally &#123; keyIterator.remove(); key.interestOps(SelectionKey.OP_READ); &#125; &#125; &#125; &#125; &#125; &#125; catch (IOException ignored) &#123; &#125; &#125;).start(); &#125;&#125; 为什么大家都不愿意用 JDK 原生 NIO 进行开发呢？从上面的代码中大家都可以看出来，是真的难用！除了编程复杂、编程模型难之外，它还有以下让人诟病的问题： JDK 的 NIO 底层由 epoll 实现，该实现饱受诟病的空轮询 bug 会导致 cpu 飙升 100% 项目庞大之后，自行实现的 NIO 很容易出现各类 bug，维护成本较高，上面这一坨代码我都不能保证没有 bug Netty 的出现很大程度上改善了 JDK 原生 NIO 所存在的一些让人难以忍受的问题。 3. AIO (Asynchronous I/O)AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。 AIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO操作本身是同步的。（除了 AIO 其他的 IO 类型都是同步的，这一点可以从底层IO线程模型解释，推荐一篇文章：《漫话：如何给女朋友解释什么是Linux的五种IO模型？》 ） 查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。 参考 《Netty 权威指南》第二版 https://zhuanlan.zhihu.com/p/23488863 (美团技术团队)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对象与实例的区别（有点意思）]]></title>
    <url>%2F2017%2F07%2F23%2F%E5%AF%B9%E8%B1%A1%E4%B8%8E%E5%AE%9E%E4%BE%8B%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%88%E6%9C%89%E7%82%B9%E6%84%8F%E6%80%9D%EF%BC%89%2F</url>
    <content type="text"><![CDATA[看到这个问题可以先回忆一下之前刚总结的：Java创建对象的几种方式创建的都叫对象，但不都是实例对象Java中万物皆对象实例是对象的一种比如Person p＝new Person这里的对象叫实例对象比如Class p＝Person.class这里的对象叫类对象]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring相关教程/资料汇总]]></title>
    <url>%2F2017%2F06%2F12%2FSpring%E7%9B%B8%E5%85%B3%E6%95%99%E7%A8%8B%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[Spring相关教程/资料官网相关 Spring官网 Spring系列主要项目 Spring官网指南 Spring Framework 4.3.17.RELEASE API 系统学习教程文档 极客学院Spring Wiki Spring W3Cschool教程 视频 网易云课堂——58集精通java教程Spring框架开发 慕课网相关视频 黑马视频 面试必备知识点SpringAOP,IOC实现原理AOP实现原理、动态代理和静态代理、Spring IOC的初始化过程、IOC原理、自己实现怎么实现一个IOC容器？这些东西都是经常会被问到的。 推荐阅读： 自己动手实现的 Spring IOC 和 AOP - 上篇 自己动手实现的 Spring IOC 和 AOP - 下篇 AOPAOP思想的实现一般都是基于 代理模式 ，在JAVA中一般采用JDK动态代理模式，但是我们都知道，JDK动态代理模式只能代理接口而不能代理类。因此，Spring AOP 会这样子来进行切换，因为Spring AOP 同时支持 CGLIB、ASPECTJ、JDK动态代理。 如果目标对象的实现类实现了接口，Spring AOP 将会采用 JDK 动态代理来生成 AOP 代理类； 如果目标对象的实现类没有实现接口，Spring AOP 将会采用 CGLIB 来生成 AOP 代理类——不过这个选择过程对开发者完全透明、开发者也无需关心。 推荐阅读： 静态代理、JDK动态代理、CGLIB动态代理讲解 ：我们知道AOP思想的实现一般都是基于 代理模式 ，所以在看下面的文章之前建议先了解一下静态代理以及JDK动态代理、CGLIB动态代理的实现方式。 Spring AOP 入门 ：带你入门的一篇文章。这篇文章主要介绍了AOP中的基本概念：5种类型的通知（Before，After，After-returning，After-throwing，Around）；Spring中对AOP的支持：AOP思想的实现一般都是基于代理模式，在Java中一般采用JDK动态代理模式，Spring AOP 同时支持 CGLIB、ASPECTJ、JDK动态代理， Spring AOP 基于AspectJ注解如何实现AOP ： AspectJ是一个AOP框架，它能够对java代码进行AOP编译（一般在编译期进行），让java代码具有AspectJ的AOP功能（当然需要特殊的编译器），可以这样说AspectJ是目前实现AOP框架中最成熟，功能最丰富的语言，更幸运的是，AspectJ与java程序完全兼容，几乎是无缝关联，因此对于有java编程基础的工程师，上手和使用都非常容易。Spring注意到AspectJ在AOP的实现方式上依赖于特殊编译器(ajc编译器)，因此Spring很机智回避了这点，转向采用动态代理技术的实现原理来构建Spring AOP的内部机制（动态织入），这是与AspectJ（静态织入）最根本的区别。Spring 只是使用了与 AspectJ 5 一样的注解，但仍然没有使用 AspectJ 的编译器，底层依是动态代理技术的实现，因此并不依赖于 AspectJ 的编译器。 Spring AOP虽然是使用了那一套注解，其实实现AOP的底层是使用了动态代理(JDK或者CGLib)来动态植入。至于AspectJ的静态植入，不是本文重点，所以只提一提。 探秘Spring AOP（慕课网视频，很不错）:慕课网视频，讲解的很不错，详细且深入 spring源码剖析（六）AOP实现原理剖析 :通过源码分析Spring AOP的原理 IOCSpring IOC的初始化过程： [Spring框架]Spring IOC的原理及详解。 Spring IOC核心源码学习 :比较简短，推荐阅读。 Spring IOC 容器源码分析 :强烈推荐，内容详尽，而且便于阅读。 Spring事务管理 可能是最漂亮的Spring事务管理详解 Spring编程式和声明式事务实例讲解 Spring单例与线程安全 Spring框架中的单例模式（源码解读）:单例模式是一种常用的软件设计模式。通过单例模式可以保证系统中一个类只有一个实例。spring依赖注入时，使用了 多重判断加锁 的单例模式。 Spring源码阅读阅读源码不仅可以加深我们对Spring设计思想的理解，提高自己的编码水品，还可以让自己在面试中如鱼得水。下面的是Github上的一个开源的Spring源码阅读，大家有时间可以看一下，当然你如果有时间也可以自己慢慢研究源码。 spring-core spring-aop spring-context spring-task spring-transaction spring-mvc guava-cache]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从输入url到显示网页发生了什么]]></title>
    <url>%2F2017%2F06%2F10%2F%E4%BB%8E%E8%BE%93%E5%85%A5url%E5%88%B0%E6%98%BE%E7%A4%BA%E7%BD%91%E9%A1%B5%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[在浏览器中输入url到显示网页主要包含两个部分： 网络通信+页面渲染 互联网内各网络设备间的通信都遵循TCP/IP协议，利用TCP/IP协议族进行网络通信时，会通过分层顺序与对方进行通信。分层由高到低分别为：应用层、传输层、网络层、数据链路层。发送端从应用层往下走，接收端从数据链路层网上走 1.浏览器的地址栏输入URL并按下回车 我们常见的RUL是这样的:www.baidu.com,域名通常由3部分组成：协议 域名 端口号 协议：主要是HTTP协议，HTTPS协议，FTP协议，FILe协议域名：url中间部分为域名或者IP端口号：通常默认都是隐藏的 http默认端口号为80 https默认端口号为443 涉及知识点： 跨域在前端进行数据请求时，由于浏览器的同源策略，协议，域名，端口号有一个不同会存在跨域请求，需要进行跨域处理，相关的跨域方法点击user-gold-cdn.xitu.io/2018/11/19/… 2.DNS域名解析 互联网上每一台计算机的唯一标识是它的IP地址，但是IP地址并不方便记忆。用户更喜欢用方便记忆的网址去寻找互联网上的其它计算机，也就是上面提到的百度的网址。所以互联网设计者需要在用户的方便性与可用性方面做一个权衡，这个权衡就是一个网址到IP地址的转换，这个过程就是DNS解析，即实现了网址到IP地址的转换解析过程DNS解析是一个递归查询的过程。 上述图片是查找www.google.com的IP地址过程。首先在本地域名服务器中查询IP地址，如果没有找到的情况下，本地域名服务器会向根域名服务器发送一个请求，如果根域名服务器也不存在该域名时，本地域名会向com顶级域名服务器发送一个请求，依次类推下去。直到最后本地域名服务器得到google的IP地址并把它缓存到本地，供下次查询使用。从上述过程中，可以看出网址的解析是一个从右向左的过程: com -&gt; google.com -&gt; www.google.com。但是你是否发现少了点什么，根域名服务器的解析过程呢？事实上，真正的网址是www.google.com.，并不是我多打了一个.，这个.对应的就是根域名服务器，默认情况下所有的网址的最后一位都是.，既然是默认情况下，为了方便用户，通常都会省略，浏览器在请求DNS的时候会自动加上，所有网址真正的解析过程为: . -&gt; .com -&gt; google.com. -&gt; www.google.com.。DNS优化DNS缓存和DNS负载均衡DNS缓存DNS存在着多级缓存，从离浏览器的距离排序的话，有以下几种: 浏览器缓存，系统缓存，路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存。 在你的chrome浏览器中输入:chrome://dns/，你可以看到chrome浏览器的DNS缓存。 系统缓存主要存在/etc/hosts(Linux系统)中: DNS负载均衡真实的互联网世界背后存在成千上百台服务器，大型的网站甚至更多。但是在用户的眼中，它需要的只是处理他的请求，哪台机器处理请求并不重要。DNS可以返回一个合适的机器的IP给用户，例如可以根据每台机器的负载量，该机器离用户地理位置的距离等等，这种过程就是DNS负载均衡，又叫做DNS重定向 3.建立TCP连接 在通过DNS域名解析后，获取到了服务器的IP地址，在获取到IP地址后，便会开始建立一次连接，这是由TCP协议完成的，主要通过三次握手进行连接。 第一次握手： 建立连接时，客户端发送syn包（syn=j）到服务器，并进入SYN_SENT状态，等待服务器确认；第二次握手： 服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态；第三次握手： 客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1），此包发送完毕，客户端和服务器进入ESTABLISHED（TCP连接成功）状态，完成三次握手。这里需要了解下ACK，SYN的意义 完成TCP连接后开使向服务器进行请求 4.向服务器发送请求 完整的HTTP请求包含请求起始行、请求头部、请求主体三部分。 5.服务器接受响应 服务器在收到浏览器发送的HTTP请求之后，会将收到的HTTP报文封装成HTTP的Request对象，并通过不同的Web服务器进行处理，处理完的结果以HTTP的Response对象返回，主要包括状态码，响应头，响应报文三个部分。状态码主要包括以下部分: 1xx：指示信息–表示请求已接收，继续处理。2xx：成功–表示请求已被成功接收、理解、接受。3xx：重定向–要完成请求必须进行更进一步的操作。4xx：客户端错误–请求有语法错误或请求无法实现。5xx：服务器端错误–服务器未能实现合法的请求。 响应头主要由Cache-Control、 Connection、Date、Pragma等组成。响应体为服务器返回给浏览器的信息，主要由HTML，css，js，图片文件组成。 6.页面渲染 如果说响应的内容是HTML文档的话，就需要浏览器进行解析渲染呈现给用户。整个过程涉及两个方面：解析和渲染。在渲染页面之前，需要构建DOM树和CSSOM树。 在浏览器还没接收到完整的 HTML 文件时，它就开始渲染页面了，在遇到外部链入的脚本标签或样式标签或图片时，会再次发送 HTTP 请求重复上述的步骤。在收到 CSS 文件后会对已经渲染的页面重新渲染，加入它们应有的样式，图片文件加载完立刻显示在相应位置。在这一过程中可能会触发页面的重绘或重排。这里就涉及了两个重要概念：Reflow和Repaint。 Reflow，也称作Layout，中文叫回流，一般意味着元素的内容、结构、位置或尺寸发生了变化，需要重新计算样式和渲染树，这个过程称为Reflow。Repaint，中文重绘，意味着元素发生的改变只是影响了元素的一些外观之类的时候（例如，背景色，边框颜色，文字颜色等），此时只需要应用新样式绘制这个元素就OK了，这个过程称为Repaint。 所以说Reflow的成本比Repaint的成本高得多的多。DOM树里的每个结点都会有reflow方法，一个结点的reflow很有可能导致子结点，甚至父点以及同级结点的reflow。 7.关闭TCP连接或继续保持连接 通过四次挥手关闭连接(FIN ACK, ACK, FIN ACK, ACK)。 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式]]></title>
    <url>%2F2017%2F06%2F07%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[单例模式是 确保一个类只有一个实例，自行实例化并向系统提供这个实例一个类只有一个实例对象，避免了重复实例的频繁创建和销毁降低了资源消耗并且共用一个对象有利于数据同步，例如WINDOWS的任务管理器、回收站、网站的计数器、线程池对象、配置文件的读取对象等两种创建方式：1.饿汉单例模式（最常用）： 单例实例在类装载时就构建，急切初始化。（预先加载法）特点：线程安全、在类加载的同时已经创建好一个静态对象，调用时反应速度快，有可能从没用到，有一点点的资源浪费 123456789101112//饿汉单例模式Demo public class SingletonTest &#123; //1.私有化该类的构造方法（不让别人new，只能自己new） private SingletonTest() &#123; &#125; //2.自己内部new一个对象 public static SingletonTest instance = new SingletonTest(); //3.给一个get方法，让外界取它 public SingletonTest getInstance() &#123; return instance; &#125; &#125; ​ 2.懒汉单例模式： 单例实例在第一次被使用时构建，延迟初始化。 12345678910111213141516//懒汉单例模式Demo public class SingletonTest2 &#123; //1.私有化该类的构造方法（不让别人new，只能自己new） private SingletonTest2() &#123; &#125; //2.自己内部维护一个null对象（只要被调用一次就不再是了） public static SingletonTest2 instance = null; //3.给一个get方法，让外界取它，只有有人用才会new一个对象出来 public SingletonTest2 getInstance() &#123; if (instance == null) &#123; //TODO 多线程下可能会出现重复new的情况 instance = new SingletonTest2(); &#125; return instance; &#125; &#125; ​总结： 两种模式各有所长 一种是时间换空间 一种是空间换时间 根据具体场景使用]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 中的 bean 生命周期]]></title>
    <url>%2F2017%2F05%2F09%2FSpring%20%E4%B8%AD%E7%9A%84%20bean%20%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[关于Spring 中的 bean 生命周期 网上有很多文章都讲到了，下面的内容整理自：https://yemengying.com/2016/07/14/spring-bean-life-cycle/ ，除了这篇文章，再推荐一篇很不错的文章 ：https://www.cnblogs.com/zrtqsk/p/3735273.html 。 Bean 容器找到配置文件中 Spring Bean 的定义。 Bean 容器利用 Java Reflection API 创建一个Bean的实例。 如果涉及到一些属性值 利用 set()方法设置一些属性值。 如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入Bean的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 如果Bean实现了 BeanFactoryAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoade r对象的实例。 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法 当要销毁 Bean 的时候，如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法。 图示： 与之比较类似的中文版本:]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Bean</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构学习资源整理]]></title>
    <url>%2F2017%2F05%2F07%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[Queue什么是队列队列是数据结构中比较重要的一种类型，它支持 FIFO，尾部添加、头部删除（先进队列的元素先出队列），跟我们生活中的排队类似。 队列的种类 单队列（单队列就是常见的队列, 每次添加元素时，都是添加到队尾，存在“假溢出”的问题也就是明明有位置却不能添加的情况） 循环队列（避免了“假溢出”的问题） Java 集合框架中的队列 QueueJava 集合中的 Queue 继承自 Collection 接口 ，Deque, LinkedList, PriorityQueue, BlockingQueue 等类都实现了它。Queue 用来存放 等待处理元素 的集合，这种场景一般用于缓冲、并发访问。除了继承 Collection 接口的一些方法，Queue 还添加了额外的 添加、删除、查询操作。 推荐文章 Java 集合深入理解（9）：Queue 队列 Set什么是 SetSet 继承于 Collection 接口，是一个不允许出现重复元素，并且无序的集合，主要 HashSet 和 TreeSet 两大实现类。 在判断重复元素的时候，Set 集合会调用 hashCode()和 equal()方法来实现。 补充：有序集合与无序集合说明 有序集合：集合里的元素可以根据 key 或 index 访问 (List、Map) 无序集合：集合里的元素只能遍历。（Set） HashSet 和 TreeSet 底层数据结构HashSet 是哈希表结构，主要利用 HashMap 的 key 来存储元素，计算插入元素的 hashCode 来获取元素在集合中的位置； TreeSet 是红黑树结构，每一个元素都是树中的一个节点，插入的元素都会进行排序； 推荐文章 Java集合–Set(基础) List什么是List在 List 中，用户可以精确控制列表中每个元素的插入位置，另外用户可以通过整数索引（列表中的位置）访问元素，并搜索列表中的元素。 与 Set 不同，List 通常允许重复的元素。 另外 List 是有序集合而 Set 是无序集合。 List的常见实现类ArrayList 是一个数组队列，相当于动态数组。它由数组实现，随机访问效率高，随机插入、随机删除效率低。 LinkedList 是一个双向链表。它也可以被当作堆栈、队列或双端队列进行操作。LinkedList随机访问效率低，但随机插入、随机删除效率高。 Vector 是矢量队列，和ArrayList一样，它也是一个动态数组，由数组实现。但是ArrayList是非线程安全的，而Vector是线程安全的。 Stack 是栈，它继承于Vector。它的特性是：先进后出(FILO, First In Last Out)。相关阅读：java数据结构与算法之栈（Stack）设计与实现 ArrayList 和 LinkedList 源码学习 ArrayList 源码学习 LinkedList 源码学习 推荐阅读 java 数据结构与算法之顺序表与链表深入分析 Map 集合框架源码学习之 HashMap(JDK1.8) ConcurrentHashMap 实现原理及源码分析 树 1 二叉树 二叉树（百度百科） (1)完全二叉树——若设二叉树的高度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数，第h层有叶子结点，并且叶子结点都是从左到右依次排布，这就是完全二叉树。 (2)满二叉树——除了叶结点外每一个结点都有左右子叶且叶子结点都处在最底层的二叉树。 (3)平衡二叉树——平衡二叉树又被称为AVL树（区别于AVL算法），它是一棵二叉排序树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。 2 完全二叉树完全二叉树（百度百科） 完全二叉树：叶节点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树。 3 满二叉树 满二叉树（百度百科，国内外的定义不同） 国内教程定义：一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是满二叉树。也就是说，如果一个二叉树的层数为K，且结点总数是(2^k) -1 ，则它就是满二叉树。 堆 数据结构之堆的定义 堆是具有以下性质的完全二叉树：每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆；或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。 4 二叉查找树（BST）浅谈算法和数据结构: 七 二叉查找树 二叉查找树的特点： 若任意节点的左子树不空，则左子树上所有结点的 值均小于它的根结点的值； 若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树； 没有键值相等的节点（no duplicate nodes）。 5 平衡二叉树（Self-balancing binary search tree） 平衡二叉树（百度百科，平衡二叉树的常用实现方法有红黑树、AVL、替罪羊树、Treap、伸展树等） 6 红黑树 红黑树特点: 每个节点非红即黑； 根节点总是黑色的； 每个叶子节点都是黑色的空节点（NIL节点）； 如果节点是红色的，则它的子节点必须是黑色的（反之不一定）； 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度）。 红黑树的应用： TreeMap、TreeSet以及JDK1.8之后的HashMap底层都用到了红黑树。 为什么要用红黑树 简单来说红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。详细了解可以查看 漫画：什么是红黑树？（也介绍到了二叉查找树，非常推荐） 推荐文章： 漫画：什么是红黑树？（也介绍到了二叉查找树，非常推荐） 寻找红黑树的操作手册（文章排版以及思路真的不错） 红黑树深入剖析及Java实现（美团点评技术团队） 7 B-，B+，B*树 二叉树学习笔记之B树、B+树、B*树 《B-树，B+树，B*树详解》 《B-树，B+树与B*树的优缺点比较》 B-树（或B树）是一种平衡的多路查找（又称排序）树，在文件系统中有所应用。主要用作文件的索引。其中的B就表示平衡(Balance) B+ 树的叶子节点链表结构相比于 B- 树便于扫库，和范围检索。 B+树支持range-query（区间查询）非常方便，而B树不支持。这是数据库选用B+树的最主要原因。 B*树 是B+树的变体，B*树分配新结点的概率比B+树要低，空间使用率更高； 8 LSM 树[HBase] LSM树 VS B+树 B+树最大的性能问题是会产生大量的随机IO 为了克服B+树的弱点，HBase引入了LSM树的概念，即Log-Structured Merge-Trees。 LSM树由来、设计思想以及应用到HBase的索引 图BFS及DFS 《使用BFS及DFS遍历树和图的思路及实现》]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条SQL语句在执行时，其底层经历了哪些过程？]]></title>
    <url>%2F2017%2F05%2F07%2F%E4%B8%80%E6%9D%A1sql%E6%89%A7%E8%A1%8C1.0%2F</url>
    <content type="text"><![CDATA[一条SQL语句在执行时，其底层经历了哪些过程？大体来讲，MySQL 可以分为 Server 层和存储引擎层两部分（当然，首先还得经过客户端）多个存储引擎共用一个server层 因此所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等 建表时如果不指定存储引擎则默认使用的是InnoDB存储引擎（MySQL5.5.5版本以前默认使用的是MyISAM引擎 TODO:二者区别后续讲解） 连接器一条SQL语句从客户端传过来首先会创建一个连接，用username和password认证身份连接完成后如果没有其他操作便处于空闲状态 默认8h自动断开空闲连接连接还分为长连接和短连接长连接：持续使用同一个连接处理请求短连接：一个连接仅执行几次后便断开，然后重新建立连接因为创建连接的过程比较复杂，所以建议尽量使用长连接但是长连接太多有时候MySQL占用内存涨的特别快，此时可以考虑以下两种方案：1.定期断开长连接或者执行一个占用内存大的查询后断开连接重新连接后继续下面的查询2.MySQL5.7以上版本可以使用mysql_reset_connection命令来初始化连接资源，此操作不需要重连以及登录验证，就可以将连接恢复到刚刚创建完的状态 查询缓存建立连接后先去查询缓存但是大家基本不用mysql的缓存功能因为只要有对该表数据更新，表上的所有缓存都会清空然后重新创建缓存所以一般默认不查缓存 但可以使用select SQL_CACHE * from T where ID = 1按需查询PS.MySQL8.0直接将查询缓存功能删掉了 分析器经过缓存器后来到分析器先做词法分析 分析出sql语句中的关键字然后做语法分析 判断是否有语法错误 优化器经过分析后MySQL知道你要做什么了，但是在实际执行之前还得经过优化器优化一下在表中有多个索引的时候，由优化器来决定使用哪个索引或者有多表关联（join）的时候决定连接顺序 选择效率高的方案TODO：MySQL根据什么选择索引呢？后续解答 执行器执行器开始执行之前会验证是否有读/写权限 没有则返回权限错误有的话就打开表调用指定的存储引擎接口获取执行结果集 返回给客户端TODO:存储引擎内部机制后续讲解]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法学习资源整理]]></title>
    <url>%2F2017%2F05%2F06%2F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[虽然平时的工作大都是在写业务，算法好像看起来没什么用但人还是要有梦想的 有空多刷刷算法 提升内功 不要做一个只会CRUD的底层码农 LeetCode LeetCode（中国）官网 如何高效地使用 LeetCode 牛客网 牛客网官网 剑指offer编程题 2017校招真题 华为机试题 公司真题 网易2018校园招聘编程题真题集合 网易2018校招内推编程题集合 2017年校招全国统一模拟笔试(第五场)编程题集合 2017年校招全国统一模拟笔试(第四场)编程题集合 2017年校招全国统一模拟笔试(第三场)编程题集合 2017年校招全国统一模拟笔试(第二场)编程题集合 2017年校招全国统一模拟笔试(第一场)编程题集合 百度2017春招笔试真题编程题集合 网易2017春招笔试真题编程题集合 网易2017秋招编程题集合 网易有道2017内推编程题 滴滴出行2017秋招笔试真题-编程题汇总 腾讯2017暑期实习生编程题 今日头条2017客户端工程师实习生笔试题 今日头条2017后端工程师实习生笔试题]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL语句汇总]]></title>
    <url>%2F2017%2F05%2F02%2F%E4%B8%80%E5%8D%83%E8%A1%8CMySQL%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[基本操作123456789/* Windows服务 */-- 启动MySQL net start mysql-- 创建Windows服务 sc create mysql binPath= mysqld_bin_path(注意：等号与值之间有空格)/* 连接与断开服务器 */mysql -h 地址 -P 端口 -u 用户名 -p 密码SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- 显示系统变量信息 数据库操作12345678910111213141516171819/* 数据库操作 */ -------------------- 查看当前数据库 SELECT DATABASE();-- 显示当前时间、用户名、数据库版本 SELECT now(), user(), version();-- 创建库 CREATE DATABASE[ IF NOT EXISTS] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name-- 查看已有库 SHOW DATABASES[ LIKE &apos;PATTERN&apos;]-- 查看当前库信息 SHOW CREATE DATABASE 数据库名-- 修改库的选项信息 ALTER DATABASE 库名 选项信息-- 删除库 DROP DATABASE[ IF EXISTS] 数据库名 同时删除该数据库相关的目录及其目录内容 表的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576-- 创建表 CREATE [TEMPORARY] TABLE[ IF NOT EXISTS] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 TEMPORARY 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT &apos;string&apos;]-- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 &#123;LOGS|STATUS&#125; -- 显示存储引擎的日志或状态信息 -- 自增起始数 AUTO_INCREMENT = 行数 -- 数据文件目录 DATA DIRECTORY = &apos;目录&apos; -- 索引文件目录 INDEX DIRECTORY = &apos;目录&apos; -- 表注释 COMMENT = &apos;string&apos; -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表 SHOW TABLES[ LIKE &apos;pattern&apos;] SHOW TABLES FROM 库名-- 查看表机构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE &apos;PATTERN&apos;] SHOW TABLE STATUS [FROM db_name] [LIKE &apos;pattern&apos;]-- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构（13.1.2. ALTER TABLE语法） ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段定义 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键-- 删除表 DROP TABLE[ IF EXISTS] 表名 ...-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 数据操作1234567891011121314151617/* 数据操作 */ -------------------- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...]-- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段-- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部-- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件] 字符集编码123456789101112131415161718/* 字符集编码 */ -------------------- MySQL、数据库、表、字段均可设置编码-- 数据编码与客户端编码不需一致SHOW VARIABLES LIKE &apos;character_set_%&apos; -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码SET 变量名 = 变量值 SET character_set_client = gbk; SET character_set_results = gbk; SET character_set_connection = gbk;SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE &apos;pattern&apos;]/SHOW CHARSET [LIKE &apos;pattern&apos;] 查看所有字符集 SHOW COLLATION [LIKE &apos;pattern&apos;] 查看所有校对集 CHARSET 字符集编码 设置字符集编码 COLLATE 校对集编码 设置校对集编码 数据类型(列类型)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/* 数据类型（列类型） */ ------------------1. 数值类型-- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数&apos;123&apos;，补填后为&apos;00123&apos; - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。-- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。-- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。2. 字符串类型-- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3-- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值-- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155datetime YYYY-MM-DD hh:mm:sstimestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmssdate YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDDtime hh:mm:ss hhmmss hhmmssyear YYYY YY YYYY YY4. 枚举和集合-- 枚举(enum) ----------enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。-- 集合（set） ----------set(val1, val2, val3...) create table tab ( gender set(&apos;男&apos;, &apos;女&apos;, &apos;无&apos;) ); insert into tab values (&apos;男, 女&apos;); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。 列属性(列约束)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/* 列属性（列约束） */ ------------------1. PRIMARY 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, &apos;val&apos;); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, &apos;val&apos;); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;6. COMMENT 注释 例：create table tab ( id int ) comment &apos;注释内容&apos;;7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。 建表规范1234567891011121314/* 建表规范 */ ------------------ -- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表 -- 1NF, 第一范式 字段不能再分，就满足第一范式。 -- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除符合主键就可以避免部分依赖。增加单列关键字。 -- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。 SELECT123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* SELECT */ ------------------SELECT [ALL|DISTINCT] select_expr FROM -&gt; WHERE -&gt; GROUP BY [合计函数] -&gt; HAVING -&gt; ORDER BY -&gt; LIMITa. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb;b. FROM 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 SELECT * FROM tb1 AS tt, tb2 AS bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 SELECT * FROM tb1, tb2; -- 向优化符提示如何选择索引 USE INDEX、IGNORE INDEX、FORCE INDEX SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3; SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3;c. WHERE 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 &lt;=&gt;与&lt;&gt;功能相同，&lt;=&gt;可用于null比较d. GROUP BY 子句, 分组子句 GROUP BY 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 GROUP BY 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。e. HAVING 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。f. ORDER BY 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。g. LIMIT 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数h. DISTINCT, ALL 选项 distinct 去除重复记录 默认为 all, 全部记录 UNION12345678/* UNION */ ------------------ 将多个select查询的结果组合成一个结果集合。 SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。 ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。 子查询1234567891011121314151617181920212223242526272829/* 子查询 */ ------------------ - 子查询需用括号包裹。-- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。 连接查询(join)123456789101112131415161718192021222324/* 连接查询(join) */ ------------------ 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2;-- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id; TRUNCATE123456789/* TRUNCATE */ ------------------TRUNCATE [TABLE] tbl_name清空数据删除重建表区别：1，truncate 是删除表再创建，delete 是逐条删除2，truncate 重置auto_increment的值。而delete不会3，truncate 不知道删除了几条，而delete知道。4，当被用于带分区的表时，truncate 会保留分区 备份与还原123456789101112131415161718192021/* 备份与还原 */ ------------------备份，将数据的结构与表内数据保存起来。利用 mysqldump 指令完成。-- 导出mysqldump [options] db_name [tables]mysqldump [options] ---database DB1 [DB2 DB3...]mysqldump [options] --all--database1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:/a.sql)2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:/a.sql)3. 导出所有表 mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:/a.sql)4. 导出一个库 mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 &gt; 文件名(D:/a.sql)可以-w携带WHERE条件-- 导入1. 在登录mysql的情况下： source 备份文件2. 在不登录的情况下 mysql -u用户名 -p密码 库名 &lt; 备份文件 视图1234567891011121314151617181920212223242526272829什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name-- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。 事务(transaction)123456789101112131415161718192021222324252627282930313233343536373839404142434445事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据晚自习方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。-- 事务提交 COMMIT;-- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。-- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务) 锁表1234567/* 锁表 */表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES 触发器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* 触发器 */ ------------------ 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.-- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。-- 字符连接函数concat(str1,str2,...])concat_ws(separator,str1,str2,...)-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert SQL编程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130/* SQL编程 */ --------------------// 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。-- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量）--// 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var = value;也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。select @var:=20;select @v1:=id, @v2=name from t1 limit 1;select * from tbl_name where @var:=30;select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb;-- 自定义变量名为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var=10; - 变量被定义后，在整个会话周期都有效（登录到退出）--// 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环--// 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) = 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) = 11floor(x) -- 向下取整 floor (10.1) = 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3=1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取日期部分time(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取时间部分date_format(&apos;yyyy-mm-dd hh:ii:ss&apos;, &apos;%d %y %a %d %m %b %j&apos;); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 其他常用函数md5();default();--// 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由&quot;参数名&quot;和&quot;参数类型&quot;组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE &apos;partten&apos; SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--// 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。 存储过程12345678910111213141516/* 存储过程 */ ------------------存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意- 没有返回值。- 只能单独调用，不可夹杂在其他语句中-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END 用户和权限管理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/* 用户和权限管理 */ -------------------- root密码重置1. 停止MySQL服务2. [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables &amp; [Windows] mysqld --skip-grant-tables3. use mysql;4. UPDATE `user` SET PASSWORD=PASSWORD(&quot;密码&quot;) WHERE `user` = &quot;root&quot;;5. FLUSH PRIVILEGES;用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES;-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 &apos;user_name&apos;@&apos;192.168.1.1&apos; - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD = PASSWORD(&apos;密码&apos;) -- 为当前用户设置密码SET PASSWORD FOR 用户名 = PASSWORD(&apos;密码&apos;) -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限/添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] &apos;password&apos;] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表 GRANT ALL PRIVILEGES ON `pms`.* TO &apos;pms&apos;@&apos;%&apos; IDENTIFIED BY &apos;pms0817&apos;;-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限 表维护12345678/* 表维护 */-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option = &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125;-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 杂项1234567891011121314/* 杂项 */ ------------------1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \&apos;5. CMD命令行内的语句结束符可以为 &quot;;&quot;, &quot;\G&quot;, &quot;\g&quot;，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：\c]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring相关知识点总结]]></title>
    <url>%2F2017%2F05%2F01%2FSpring%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Spring是什么 Spring是一个一站式轻量级的开源框架Spring Bean的三种配置方式：xml、注解和Java @Configuration public class BeanConfig { @Bean public BeanFactory beanFactory(){ return new BeanFactoryImpl(); } }Spring的核心： 控制反转（IoC）和面向切面（AOP） AOP代理主要分为静态代理和动态代理 AOP实现的关键在于 代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ； 动态代理则以Spring AOP为代表。 （1）AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类 因此也称为编译时增强，他会在编译阶段将AspectJ(切面)织入到Java字节码中 运行的时候就是增强之后的AOP对象。 （2）Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码 而是每次运行时在内存中临时为方法生成一个AOP对象 这个AOP对象包含了目标对象的全部方法 并且在特定的切点做了增强处理，并回调原对象的方法。Spring中AOP动态代理的两种实现方式： JDK动态代理和CGLIB动态代理 Spring的优点： Spring将对象之间的依赖关系交由框架处理，减低组件的耦合性； Spring对于主流的应用框架提供了非常方便的集成支持； Spring提供的AOP功能，方便进行面向切面的编程，许多不容易用传统OOP实现的功能可以通过AOP轻松应付； Spring框架设计精妙，Spring源码是经典的学习范例Spring的七大组成模块： Spring Core：核心类库，提供IOC服务； Spring Context：提供框架式的Bean访问方式，以及企业级功能（JNDI、定时任务等）； Spring AOP：AOP服务； Spring DAO：对JDBC的抽象，简化了数据访问异常的处理； Spring ORM：对现有的ORM框架的支持； Spring Web：提供了基本的面向Web的综合特性，例如多方文件上传； Spring MVC：提供面向Web应用的Model-View-Controller实现 Spring事务的实现方式和实现原理： Spring事务的本质其实就是数据库对事务的支持 没有数据库的事务支持，spring是无法提供事务功能的。 真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。Spring的IOC有三种注入方式 ： 构造器注入、setter方法注入、注解注入 Spring框架支持以下五种bean的作用域： singleton : bean在每个Spring ioc 容器中只有一个实例。 prototype：一个bean的定义可以有多个实例。 request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。 session：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。缺省的Spring bean 的作用域是Singleton. Spring支持两种类型的事务管理： 编程式事务管理：这意味你通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。 声明式事务管理：这意味着你可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。大多数Spring框架的用户选择声明式事务管理，因为它对应用代码的影响最小，因此更符合一个无侵入的轻量级容器的思想。声明式事务管理要优于编程式事务管理，虽然比编程式事务管理（这种方式允许你通过代码控制事务）少了一点灵活性。 SpringMVC 请求处理流程 1、 用户发送请求被前端控制器DispatcherServlet捕获 2、 DispatcherServlet收到请求调用处理器映射器HandlerMapping。 3、 处理器映射器找到具体的处理器(可以根据xml配置、注解进行查找)，生成映射信息 返回给DispatcherServlet。 4、 DispatcherServlet调用处理器适配器HandlerAdapter。 5、 处理器适配器根据映射信息适配调用具体的处理器(Controller，也叫后端控制器) Controller执行完成返回ModelAndView给DispatcherServlet。 6、 DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体视图view。 7、 DispatcherServlet根据视图View进行渲染视图并且响应给用户。 整个过程都是以DispatcherServlet为中心进行的。 你怎样定义类的作用域? 当定义一个 在Spring里，我们还能给这个bean声明一个作用域。它可以通过bean 定义中的scope属性来定义。如，当Spring要在需要的时候每次生产一个新的bean实例，bean的scope属性被指定为prototype。 另一方面，一个bean每次使用的时候必须返回同一个实例，这个bean的scope 属性 必须设为 singleton。解释Spring支持的几种bean的作用域 Spring框架支持以下五种bean的作用域： singleton : bean在每个Spring ioc 容器中只有一个实例。 prototype ：一个bean的定义可以有多个实例。 request ：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。 session ：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 global-session ：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。缺省的Spring bean 的作用域是Singleton. Spring框架中的单例bean是线程安全的吗? 不，Spring框架中的单例bean不是线程安全的。解释Spring框架中bean的生命周期 PS：可以借鉴Servlet的生命周期，实例化、初始init、接收请求service、销毁destroy。 Spring的核心容器会首先从XML 文件中读取bean的定义，并实例化bean。 然后根据bean的定义填充所有的属性。 然后根据bean内部实现了哪些接口依次调用一堆方法，最后初始化，最后的最后destroy。 如果bean实现了BeanNameAware 接口，Spring 传递bean 的ID 到 setBeanName方法。 如果Bean 实现了 BeanFactoryAware 接口， Spring传递beanfactory 给setBeanFactory 方法。 如果有任何与bean相关联的BeanPostProcessors，Spring会在postProcesserBeforeInitialization()方法内调用它们。 如果bean实现IntializingBean了，调用它的afterPropertySet方法， 如果bean声明了初始化方法，调用此初始化方法。 如果有BeanPostProcessors 和bean 关联，这些bean的postProcessAfterInitialization() 方法将被调用。 如果bean实现了 DisposableBean，它将调用destroy()方法。 哪些是重要的bean生命周期方法？ 你能重载它们吗？ 有两个重要的bean 生命周期方法，第一个是setup ， 它是在容器加载bean的时候被调用。第二个方法是 teardown 它是在容器卸载类的时候被调用。 The bean 标签有两个重要的属性（init-method和destroy-method）。 用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct和@PreDestroy）]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 设计模式学习汇总]]></title>
    <url>%2F2017%2F04%2F07%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Java 设计模式下面是自己学习设计模式的时候做的总结，有些是自己的原创文章，有些是网上写的比较好的文章，保存下来细细消化吧！ 系列文章推荐：https://design-patterns.readthedocs.io/zh_CN/latest/index.html 创建型模式创建型模式概述 创建型模式(Creational Pattern)对类的实例化过程进行了抽象，能够将软件模块中对象的创建和对象的使用分离。为了使软件的结构更加清晰，外界对于这些对象只需要知道它们共同的接口，而不清楚其具体的实现细节，使整个系统的设计更加符合单一职责原则。 创建型模式在创建什么(What)，由谁创建(Who)，何时创建(When)等方面都为软件设计者提供了尽可能大的灵活性。创建型模式隐藏了类的实例的创建细节，通过隐藏对象如何被创建和组合在一起达到使整个系统独立的目的。 常见创建型模式详解 单例模式： 深入理解单例模式——只有一个实例 工厂模式： 深入理解工厂模式——由对象工厂生成对象 建造者模式： 深入理解建造者模式 ——组装复杂的实例 原型模式： 深入理解原型模式 ——通过复制生成实例 结构型模式结构型模式概述 结构型模式(Structural Pattern)： 描述如何将类或者对象结合在一起形成更大的结构，就像搭积木，可以通过简单积木的组合形成复杂的、功能更为强大的结构 结构型模式可以分为类结构型模式和对象结构型模式： 类结构型模式关心类的组合，由多个类可以组合成一个更大的系统，在类结构型模式中一般只存在继承关系和实现关系。 对象结构型模式关心类与对象的组合，通过关联关系使得在一个类中定义另一个类的实例对象，然后通过该对象调用其方法。根据“合成复用原则”，在系统中尽量使用关联关系来替代继承关系，因此大部分结构型模式都是对象结构型模式。 常见结构型模式详解 适配器模式： 深入理解适配器模式——加个“适配器”以便于复用 适配器模式原理及实例介绍-IBM 桥接模式： 设计模式笔记16：桥接模式(Bridge Pattern) 组合模式： 大话设计模式—组合模式 装饰模式： java模式—装饰者模式、Java设计模式-装饰者模式 外观模式： java设计模式之外观模式（门面模式） 享元模式： 享元模式 代理模式： 代理模式原理及实例讲解 （IBM出品，很不错） 轻松学，Java 中的代理模式及动态代理 Java代理模式及其应用 行为型模式行为型模式概述 行为型模式(Behavioral Pattern)是对在不同的对象之间划分责任和算法的抽象化。 行为型模式不仅仅关注类和对象的结构，而且重点关注它们之间的相互作用。 通过行为型模式，可以更加清晰地划分类与对象的职责，并研究系统在运行时实例对象之间的交互。在系统运行时，对象并不是孤立的，它们可以通过相互通信与协作完成某些复杂功能，一个对象在运行时也将影响到其他对象的运行。 行为型模式分为类行为型模式和对象行为型模式两种： 类行为型模式： 类的行为型模式使用继承关系在几个类之间分配行为，类行为型模式主要通过多态等方式来分配父类与子类的职责。 对象行为型模式： 对象的行为型模式则使用对象的聚合关联关系来分配行为，对象行为型模式主要是通过对象关联等方式来分配两个或多个类的职责。根据“合成复用原则”，系统中要尽量使用关联关系来取代继承关系，因此大部分行为型设计模式都属于对象行为型设计模式。 职责链模式： Java设计模式之责任链模式、职责链模式 责任链模式实现的三种方式 命令模式： https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/command.html 在软件设计中，我们经常需要向某些对象发送请求，但是并不知道请求的接收者是谁，也不知道被请求的操作是哪个，我们只需在程序运行时指定具体的请求接收者即可，此时，可以使用命令模式来进行设计，使得请求发送者与请求接收者消除彼此之间的耦合，让对象之间的调用关系更加灵活。命令模式可以对发送者和接收者完全解耦，发送者与接收者之间没有直接引用关系，发送请求的对象只需要知道如何发送请求，而不必知道如何完成请求。这就是命令模式的模式动机。 解释器模式： 迭代器模式： 中介者模式： https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/mediator.html 备忘录模式： 观察者模式： https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.html、https://juejin.im/post/5c712ab56fb9a049a7127114 状态模式：https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/state.html 策略模式：https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/strategy.html 策略模式作为设计原则中开闭原则最典型的体现，也是经常使用的。下面这篇博客介绍了策略模式一般的组成部分和概念，并用了一个小demo去说明了策略模式的应用。 java设计模式之策略模式 模板方法模式： 访问者模式：]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK-JRE-JVM对比]]></title>
    <url>%2F2017%2F04%2F01%2FJDK-JRE-JVM%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[JDK（Java Development Kit）是针对Java开发员的产品，是整个Java的核心，包括了Java运行环境JRE、Java工具和Java基础类库。Java Runtime Environment（JRE）是运行JAVA程序所必须的环境的集合，包含JVM标准实现及Java核心类库。JVM是Java Virtual Machine（Java虚拟机）的缩写，是整个java实现跨平台的最核心的部分，能够运行以Java语言写作的软件程序。 JDK包括了Java运行环境JRE 、一堆Java工具（javac/java/jdb等）和Java基础的类库（即Java API 包括rt.jar）。 rt.jar：Java基础类库，也就是Java doc里面看到的所有的类的class文件。 tools.jar：是系统用来编译一个类的时候用到的，即执行javac的时候用到。 dt.jar：dt.jar是关于运行环境的类库，主要是swing包。JREJava运行环境，并不是一个开发环境，所以没有包含任何开发工具（如编译器和调试器），只是针对于使用Java程序的用户。JRE中包含了 JVM 、runtime class libraries和Java application launcher，这些是运行Java程序的必要组件。 JVM就是我们常说的java虚拟机，它是整个java实现跨平台的最核心的部分，所有的java程序会首先被编译为.class的类文件，这种类文件可以在虚拟机上执行。 也就是说class并不直接与机器的操作系统相对应，而是经过虚拟机间接与操作系统交互，由虚拟机将程序解释给本地系统执行。 只有JVM还不能成class的执行，因为在解释class的时候JVM需要调用解释所需要的类库lib，而jre包含lib类库。 JVM屏蔽了与具体操作系统平台相关的信息，使得Java程序只需生成在Java虚拟机上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>JRE</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾回收算法]]></title>
    <url>%2F2017%2F01%2F07%2F%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[垃圾回收算法垃圾回收算法有很多种，目前商业虚拟机常用的是分代回收算法，最初并不是用这个算法的所以需要先了解一下垃圾收集算法的背景知识： 要讨论垃圾回收算法首先得知道虚拟机是如何判断一个对象是“活”还是“死”的 这里涉及到 两种标记算法：1.引用计数算法引用计数算法非常简单且高效，当一个对象被引用一次则+1不再被引用则-1，当计数为0就是不可能在被使用的对象了，但是这种算法存在一个致命的缺陷：两个对象相互引用对方呢？ 2.可达性分析算法目前的标记算法主流实现都是用的可达性分析算法。就是以一个叫GC Roots的对象为起点，通过引用链向下搜索，如果一个对象通过引用链无法与GC Roots对象链接，就视为可回收对象，上面说的那种相互引用的情况自然也解决了。 扩展：即使是可达性分析中不可达的对象也并不是非死不可，只是暂处‘缓刑’，真正宣告一个对象死亡至少还要经历两次标记过程：当被判定不可达之后那么他被第一次标记并进行筛选，若对象没有覆盖finalize()方法或者finalize()方法已经被虚拟机调用过就‘放生’，如果被判定需要执行finalize()方法就会被放到一个叫F-Queue的队列中进行第二次标记对象被再次被引用就会放生，否则就会被回收。 finalize()方法finalize()是Object中的方法，当垃圾回收器将要回收对象所占内存之前被调用，即当一个对象被虚拟机宣告死亡时会先调用它finalize()方法，让此对象处理它生前的最后事情（这个对象可以趁这个时机挣脱死亡的命运） 引用说到这里敏锐的小伙伴可能以及察觉到了，上面都在说引用所以引用的定义就显得尤为关键了JDK1.2后Java对引用的概念进行了扩充，将引用分为：强引用、软引用、弱引用、虚引用四种 强引用：用处很大，无论如何都不会被GC回收 软引用：有一定用处但不大，内存实在不够才会在内存溢出之前回收掉 弱引用：比软引用强度更弱一些，GC会让它多活一轮，下一轮就回收 虚引用：必回收，唯一作用就是被GC回收时会收到一个系统通知 标记-清除算法最基础的垃圾回收算法，顾名思义，整个回收过程分两步：1.逐个标记2.统一回收该算法可以算是后来所有垃圾回收算法的基石（后续所有算法都有标记和清除这两步，只不过策略上有了一些优化） 复制算法前面说的标记-清除算法其实两个过程效率都很低，并且回收之后内存被‘抠出很多洞’内存碎片化严重，此时如果过来了一个较大的对象，找不到一整块连续的内存空间就不得不提前触发另外一次GC回收。而复制算法则选择将内存一分为二每次只使用其中一半，满了之后将存活的对象整齐复制到另一块干净的内存上，将剩下的碎片一次性擦除，简单高效。但是也存在一个很大的缺陷，那就是可用内存变为原来的一半了。 标记整理算法复制算法的高效性是建立在存活对象少、垃圾对象多的前提下的。这种情况在新生代经常发生，但是在老年代更常见的情况是大部分对象都是存活对象。如果依然使用复制算法，由于存活的对象较多，复制的成本也将很高。标记-整理算法是一种老年代的回收算法，它在标记-清除算法的基础上做了一些优化。首先也需要从根节点开始对所有可达对象做一次标记，但之后，它并不简单地清理未标记的对象，而是将所有的存活对象压缩到内存的一端。之后，清理边界外所有的空间。这种方法既避免了碎片的产生，又不需要两块相同的内存空间，因此，其性价比比较高。 分代收集算法1.年轻代：复制算法事实上后来IBM公司经过研究发现，98%的对象都是‘朝生夕死’，所以并不需要1:1的划分内存，即我们现在常用的分代收集算法：根据对象的存活周期将内存划分为两块，分别为新生代和老年代，然后对各代采用不同的回收算法，在新生代中大部分是‘朝生夕死’的对象，继续将新生代8:2划分为Eden区和survival区，其中survival区1:1分成s0和s1两块，采用之前说的复制算法，减少内存碎片的产生。新生代满了会进行一次minor GC ，minor GC 存活的对象转移到survival区，survival区满了就会将survival区进行回收，存活的survival区对象复制到另外一块survival区中，并且survival区对象每存活一轮年龄+1当到达一定年龄就会前往老年代。 2.年老代：标记-清除或标记-整理算法当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。 以上这种年轻代与年老代分别采用不同回收算法的方式称为”分代收集算法”，这也是当下企业使用的一种方式 垃圾回收机制年轻代分为Eden区和survivor区（两块儿：from和to），且Eden:from:to==8:1:1。 1）新产生的对象优先分配在Eden区（除非配置了-XX:PretenureSizeThreshold，大于该值的对象会直接进入年老代）； 2）当Eden区满了或放不下了，这时候其中存活的对象会复制到from区。 这里，需要注意的是，如果存活下来的对象from区都放不下，则这些存活下来的对象全部进入年老代。之后Eden区的内存全部回收掉。3）之后产生的对象继续分配在Eden区，当Eden区又满了或放不下了，这时候将会把Eden区和from区存活下来的对象复制到to区（同理，如果存活下来的对象to区都放不下，则这些存活下来的对象全部进入年老代），之后回收掉Eden区和from区的所有内存。 4）如上这样，会有很多对象会被复制很多次（每复制一次，对象的年龄就+1），默认情况下，当对象被复制了15次（这个次数可以通过-XX:MaxTenuringThreshold来配置），就会进入年老代了。 5）当年老代满了或者存放不下将要进入年老代的存活对象的时候，就会发生一次Full GC（这个是我们最需要减少的，因为耗时很严重）。 垃圾回收的两种类型：Minor GC 和 Full GC。1.Minor GC对新生代进行回收，不会影响到年老代。因为新生代的 Java 对象大多死亡频繁，所以 Minor GC 非常频繁，一般在这里使用速度快、效率高的算法，使垃圾回收能尽快完成。 2.Full GC也叫Major GC，对整个堆进行回收，包括新生代和老年代。由于Full GC需要对整个堆进行回收，所以比MinorGC要慢，因此应该尽可能减少Full GC的次数，导致FullGC的原因包括：老年代被写满、永久代（Perm）被写满和System.gc()被显式调用等。 最后：扩展01：JVM何时会进行全局GC 01.手动调用System.GC 但也不是立即调用 02.老年代空间不足 03.永生代空间不足 04.计算得知新生代前往老年代平均值大于老年代剩余空间 扩展02：在压力测试时，发现FullGC频率很高，如何解决 01.观察GC日志，判断是否有内存泄漏，或者存在内部不合理点 02.调整JVM参数，如新生代、老年代大小 S0+S1大小比例，选用不同的立即回收器 03.Dump内存，做进一步的对象分析 04.压测脚本的编写，性能问题解决前可以发现问题，并对解决方案进行验证]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java锁 总结]]></title>
    <url>%2F2017%2F01%2F07%2FJava%E9%94%81%20%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[锁的种类： synchronize自动锁（最常用） 可以给类、方法、代码块加锁 lock手动锁，只能锁代码块儿，且需要手动加锁解锁，忘记解锁会造成死锁 volatile轻量级锁，不会造成线程阻塞，只能修饰变量，且只能保证变量的修改可见性，无法保证原子性 解决死锁的方法： 1)尽量使用tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。 2)尽量使用java.util.concurrent(jdk 1.5以上)包的并发类代替手写控制并发，比较常用的是ConcurrentHashMap、ConcurrentLinkedQueue、AtomicBoolean等等，实际应用中java.util.concurrent.atomic十分有用，简单方便且效率比使用Lock更高 3)尽量降低锁的使用粒度，尽量不要几个功能用同一把锁 4)尽量减少同步的代码块 悲观锁与乐观锁 悲观锁用于线程冲突率高的场景，用提前加锁保证线程安全 乐观锁用于线程冲突率底的场景，用修改前后版本号是否一致保证线程安全 未完待续]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>锁</tag>
      </tags>
  </entry>
</search>
