<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[一条SQL语句在执行时，其底层经历了哪些过程？]]></title>
    <url>%2F2019%2F05%2F07%2F%E4%B8%80%E6%9D%A1sql%E6%89%A7%E8%A1%8C1.0%2F</url>
    <content type="text"><![CDATA[一条SQL语句在执行时，其底层经历了哪些过程？大体来讲，MySQL 可以分为 Server 层和存储引擎层两部分（当然，首先还得经过客户端）多个存储引擎共用一个server层 因此所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等 建表时如果不指定存储引擎则默认使用的是InnoDB存储引擎（MySQL5.5.5版本以前默认使用的是MyISAM引擎 TODO:二者区别后续讲解） 连接器一条SQL语句从客户端传过来首先会创建一个连接，用username和password认证身份连接完成后如果没有其他操作便处于空闲状态 默认8h自动断开空闲连接连接还分为长连接和短连接长连接：持续使用同一个连接处理请求短连接：一个连接仅执行几次后便断开，然后重新建立连接因为创建连接的过程比较复杂，所以建议尽量使用长连接但是长连接太多有时候MySQL占用内存涨的特别快，此时可以考虑以下两种方案：1.定期断开长连接或者执行一个占用内存大的查询后断开连接重新连接后继续下面的查询2.MySQL5.7以上版本可以使用mysql_reset_connection命令来初始化连接资源，此操作不需要重连以及登录验证，就可以将连接恢复到刚刚创建完的状态 查询缓存建立连接后先去查询缓存但是大家基本不用mysql的缓存功能因为只要有对该表数据更新，表上的所有缓存都会清空然后重新创建缓存所以一般默认不查缓存 但可以使用select SQL_CACHE * from T where ID = 1按需查询PS.MySQL8.0直接将查询缓存功能删掉了 分析器经过缓存器后来到分析器先做词法分析 分析出sql语句中的关键字然后做语法分析 判断是否有语法错误 优化器经过分析后MySQL知道你要做什么了，但是在实际执行之前还得经过优化器优化一下在表中有多个索引的时候，由优化器来决定使用哪个索引或者有多表关联（join）的时候决定连接顺序 选择效率高的方案TODO：MySQL根据什么选择索引呢？后续解答 执行器执行器开始执行之前会验证是否有读/写权限 没有则返回权限错误有的话就打开表调用指定的存储引擎接口获取执行结果集 返回给客户端TODO:存储引擎内部机制后续讲解]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一千行MySQL命令]]></title>
    <url>%2F2019%2F05%2F02%2F%E4%B8%80%E5%8D%83%E8%A1%8CMySQL%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[基本操作 数据库操作 表的操作 数据操作 字符集编码 数据类型(列类型) 列属性(列约束) 建表规范 SELECT UNION 子查询 连接查询(join) TRUNCATE 备份与还原 视图 事务(transaction) 锁表 触发器 SQL编程 存储过程 用户和权限管理 表维护 杂项 基本操作123456789/* Windows服务 */-- 启动MySQL net start mysql-- 创建Windows服务 sc create mysql binPath= mysqld_bin_path(注意：等号与值之间有空格)/* 连接与断开服务器 */mysql -h 地址 -P 端口 -u 用户名 -p 密码SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- 显示系统变量信息 数据库操作12345678910111213141516171819/* 数据库操作 */ -------------------- 查看当前数据库 SELECT DATABASE();-- 显示当前时间、用户名、数据库版本 SELECT now(), user(), version();-- 创建库 CREATE DATABASE[ IF NOT EXISTS] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name-- 查看已有库 SHOW DATABASES[ LIKE &apos;PATTERN&apos;]-- 查看当前库信息 SHOW CREATE DATABASE 数据库名-- 修改库的选项信息 ALTER DATABASE 库名 选项信息-- 删除库 DROP DATABASE[ IF EXISTS] 数据库名 同时删除该数据库相关的目录及其目录内容 表的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576-- 创建表 CREATE [TEMPORARY] TABLE[ IF NOT EXISTS] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 TEMPORARY 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT &apos;string&apos;]-- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 &#123;LOGS|STATUS&#125; -- 显示存储引擎的日志或状态信息 -- 自增起始数 AUTO_INCREMENT = 行数 -- 数据文件目录 DATA DIRECTORY = &apos;目录&apos; -- 索引文件目录 INDEX DIRECTORY = &apos;目录&apos; -- 表注释 COMMENT = &apos;string&apos; -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表 SHOW TABLES[ LIKE &apos;pattern&apos;] SHOW TABLES FROM 库名-- 查看表机构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE &apos;PATTERN&apos;] SHOW TABLE STATUS [FROM db_name] [LIKE &apos;pattern&apos;]-- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构（13.1.2. ALTER TABLE语法） ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段定义 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键-- 删除表 DROP TABLE[ IF EXISTS] 表名 ...-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 数据操作1234567891011121314151617/* 数据操作 */ -------------------- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...]-- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段-- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部-- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件] 字符集编码123456789101112131415161718/* 字符集编码 */ -------------------- MySQL、数据库、表、字段均可设置编码-- 数据编码与客户端编码不需一致SHOW VARIABLES LIKE &apos;character_set_%&apos; -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码SET 变量名 = 变量值 SET character_set_client = gbk; SET character_set_results = gbk; SET character_set_connection = gbk;SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE &apos;pattern&apos;]/SHOW CHARSET [LIKE &apos;pattern&apos;] 查看所有字符集 SHOW COLLATION [LIKE &apos;pattern&apos;] 查看所有校对集 CHARSET 字符集编码 设置字符集编码 COLLATE 校对集编码 设置校对集编码 数据类型(列类型)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/* 数据类型（列类型） */ ------------------1. 数值类型-- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数&apos;123&apos;，补填后为&apos;00123&apos; - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。-- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。-- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。2. 字符串类型-- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3-- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值-- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155datetime YYYY-MM-DD hh:mm:sstimestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmssdate YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDDtime hh:mm:ss hhmmss hhmmssyear YYYY YY YYYY YY4. 枚举和集合-- 枚举(enum) ----------enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。-- 集合（set） ----------set(val1, val2, val3...) create table tab ( gender set(&apos;男&apos;, &apos;女&apos;, &apos;无&apos;) ); insert into tab values (&apos;男, 女&apos;); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。 列属性(列约束)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/* 列属性（列约束） */ ------------------1. PRIMARY 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, &apos;val&apos;); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, &apos;val&apos;); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;6. COMMENT 注释 例：create table tab ( id int ) comment &apos;注释内容&apos;;7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。 建表规范1234567891011121314/* 建表规范 */ ------------------ -- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表 -- 1NF, 第一范式 字段不能再分，就满足第一范式。 -- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除符合主键就可以避免部分依赖。增加单列关键字。 -- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。 SELECT123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* SELECT */ ------------------SELECT [ALL|DISTINCT] select_expr FROM -&gt; WHERE -&gt; GROUP BY [合计函数] -&gt; HAVING -&gt; ORDER BY -&gt; LIMITa. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb;b. FROM 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 SELECT * FROM tb1 AS tt, tb2 AS bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 SELECT * FROM tb1, tb2; -- 向优化符提示如何选择索引 USE INDEX、IGNORE INDEX、FORCE INDEX SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3; SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3;c. WHERE 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 &lt;=&gt;与&lt;&gt;功能相同，&lt;=&gt;可用于null比较d. GROUP BY 子句, 分组子句 GROUP BY 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 GROUP BY 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。e. HAVING 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。f. ORDER BY 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。g. LIMIT 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数h. DISTINCT, ALL 选项 distinct 去除重复记录 默认为 all, 全部记录 UNION12345678/* UNION */ ------------------ 将多个select查询的结果组合成一个结果集合。 SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。 ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。 子查询1234567891011121314151617181920212223242526272829/* 子查询 */ ------------------ - 子查询需用括号包裹。-- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。 连接查询(join)123456789101112131415161718192021222324/* 连接查询(join) */ ------------------ 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2;-- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id; TRUNCATE123456789/* TRUNCATE */ ------------------TRUNCATE [TABLE] tbl_name清空数据删除重建表区别：1，truncate 是删除表再创建，delete 是逐条删除2，truncate 重置auto_increment的值。而delete不会3，truncate 不知道删除了几条，而delete知道。4，当被用于带分区的表时，truncate 会保留分区 备份与还原123456789101112131415161718192021/* 备份与还原 */ ------------------备份，将数据的结构与表内数据保存起来。利用 mysqldump 指令完成。-- 导出mysqldump [options] db_name [tables]mysqldump [options] ---database DB1 [DB2 DB3...]mysqldump [options] --all--database1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:/a.sql)2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:/a.sql)3. 导出所有表 mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:/a.sql)4. 导出一个库 mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 &gt; 文件名(D:/a.sql)可以-w携带WHERE条件-- 导入1. 在登录mysql的情况下： source 备份文件2. 在不登录的情况下 mysql -u用户名 -p密码 库名 &lt; 备份文件 视图1234567891011121314151617181920212223242526272829什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name-- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。 事务(transaction)123456789101112131415161718192021222324252627282930313233343536373839404142434445事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据晚自习方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。-- 事务提交 COMMIT;-- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。-- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务) 锁表1234567/* 锁表 */表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES 触发器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/* 触发器 */ ------------------ 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.-- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。-- 字符连接函数concat(str1,str2,...])concat_ws(separator,str1,str2,...)-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert SQL编程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130/* SQL编程 */ --------------------// 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。-- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量）--// 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var = value;也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。select @var:=20;select @v1:=id, @v2=name from t1 limit 1;select * from tbl_name where @var:=30;select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb;-- 自定义变量名为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var=10; - 变量被定义后，在整个会话周期都有效（登录到退出）--// 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环--// 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) = 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) = 11floor(x) -- 向下取整 floor (10.1) = 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3=1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取日期部分time(&apos;yyyy-mm-dd hh:ii:ss&apos;); -- 获取时间部分date_format(&apos;yyyy-mm-dd hh:ii:ss&apos;, &apos;%d %y %a %d %m %b %j&apos;); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 其他常用函数md5();default();--// 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由&quot;参数名&quot;和&quot;参数类型&quot;组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE &apos;partten&apos; SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--// 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。 存储过程12345678910111213141516/* 存储过程 */ ------------------存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意- 没有返回值。- 只能单独调用，不可夹杂在其他语句中-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END 用户和权限管理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/* 用户和权限管理 */ -------------------- root密码重置1. 停止MySQL服务2. [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables &amp; [Windows] mysqld --skip-grant-tables3. use mysql;4. UPDATE `user` SET PASSWORD=PASSWORD(&quot;密码&quot;) WHERE `user` = &quot;root&quot;;5. FLUSH PRIVILEGES;用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES;-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 &apos;user_name&apos;@&apos;192.168.1.1&apos; - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD = PASSWORD(&apos;密码&apos;) -- 为当前用户设置密码SET PASSWORD FOR 用户名 = PASSWORD(&apos;密码&apos;) -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限/添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] &apos;password&apos;] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表 GRANT ALL PRIVILEGES ON `pms`.* TO &apos;pms&apos;@&apos;%&apos; IDENTIFIED BY &apos;pms0817&apos;;-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限 表维护12345678/* 表维护 */-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option = &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125;-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... 杂项1234567891011121314/* 杂项 */ ------------------1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \&apos;5. CMD命令行内的语句结束符可以为 &quot;;&quot;, &quot;\G&quot;, &quot;\g&quot;，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：\c]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL高性能优化规范]]></title>
    <url>%2F2019%2F05%2F01%2FMySQL%E9%AB%98%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E8%A7%84%E8%8C%83%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[数据库命令规范 数据库基本设计规范 1. 所有表必须使用 Innodb 存储引擎 2. 数据库和表的字符集统一使用 UTF8 3. 所有表和字段都需要添加注释 4. 尽量控制单表数据量的大小,建议控制在 500 万以内。 5. 谨慎使用 MySQL 分区表 6.尽量做到冷热数据分离,减小表的宽度 7. 禁止在表中建立预留字段 8. 禁止在数据库中存储图片,文件等大的二进制数据 9. 禁止在线上做数据库压力测试 10. 禁止从开发环境,测试环境直接连接生成环境数据库 数据库字段设计规范 1. 优先选择符合存储需要的最小的数据类型 2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据 3. 避免使用 ENUM 类型 4. 尽可能把所有列定义为 NOT NULL 5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间 6. 同财务相关的金额类数据必须使用 decimal 类型 索引设计规范 1. 限制每张表上的索引数量,建议单张表索引不超过 5 个 2. 禁止给表中的每一列都建立单独的索引 3. 每个 Innodb 表必须有个主键 4. 常见索引列建议 5.如何选择索引列的顺序 6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） 7. 对于频繁的查询优先考虑使用覆盖索引 8.索引 SET 规范 数据库 SQL 开发规范 1. 建议使用预编译语句进行数据库操作 2. 避免数据类型的隐式转换 3. 充分利用表上已经存在的索引 4. 数据库设计时，应该要对以后扩展进行考虑 5. 程序连接不同的数据库使用不同的账号，进制跨库查询 6. 禁止使用 SELECT * 必须使用 SELECT &lt;字段列表&gt; 查询 7. 禁止使用不含字段列表的 INSERT 语句 8. 避免使用子查询，可以把子查询优化为 join 操作 9. 避免使用 JOIN 关联太多的表 10. 减少同数据库的交互次数 11. 对应同一列进行 or 判断时，使用 in 代替 or 12. 禁止使用 order by rand() 进行随机排序 13. WHERE 从句中禁止对列进行函数转换和计算 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION 15. 拆分复杂的大 SQL 为多个小 SQL 数据库操作行为规范 1. 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作 2. 对于大表使用 pt-online-schema-change 修改表结构 3. 禁止为程序使用的账号赋予 super 权限 4. 对于程序连接数据库账号,遵循权限最小原则 数据库命令规范 所有数据库对象名称必须使用小写字母并用下划线分割 所有数据库对象名称禁止使用 MySQL 保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来） 数据库对象的命名要能做到见名识意，并且最后不要超过 32 个字符 临时库表必须以 tmp_为前缀并以日期为后缀，备份表必须以 bak_为前缀并以日期 (时间戳) 为后缀 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低） 数据库基本设计规范1. 所有表必须使用 Innodb 存储引擎没有特殊要求（即 Innodb 无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用 Innodb 存储引擎（MySQL5.5 之前默认使用 Myisam，5.6 以后默认的为 Innodb）。 Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。 2. 数据库和表的字符集统一使用 UTF8兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储 emoji 表情的需要，字符集需要采用 utf8mb4 字符集。 3. 所有表和字段都需要添加注释使用 comment 从句添加表和列的备注，从一开始就进行数据字典的维护 4. 尽量控制单表数据量的大小,建议控制在 500 万以内。500 万并不是 MySQL 数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。 可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小 5. 谨慎使用 MySQL 分区表分区表在物理上表现为多个文件，在逻辑上表现为一个表； 谨慎选择分区键，跨分区查询效率可能更低； 建议采用物理分表的方式管理大数据。 6.尽量做到冷热数据分离,减小表的宽度 MySQL 限制每个表最多存储 4096 列，并且每一行数据的大小不能超过 65535 字节。 减少磁盘 IO,保证热数据的内存缓存命中率（表越宽，把表装载进内存缓冲池时所占用的内存也就越大,也会消耗更多的 IO）； 更有效的利用缓存，避免读入无用的冷数据； 经常一起使用的列放到一个表中（避免更多的关联操作）。 7. 禁止在表中建立预留字段预留字段的命名很难做到见名识义。 预留字段无法确认存储的数据类型，所以无法选择合适的类型。 对预留字段类型的修改，会对表进行锁定。 8. 禁止在数据库中存储图片,文件等大的二进制数据通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机 IO 操作，文件很大时，IO 操作很耗时。 通常存储于文件服务器，数据库只存储文件地址信息 9. 禁止在线上做数据库压力测试10. 禁止从开发环境,测试环境直接连接生产环境数据库 数据库字段设计规范1. 优先选择符合存储需要的最小的数据类型原因： 列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多，索引的性能也就越差。 方法： a.将字符串转换成数字类型存储,如:将 IP 地址转换成整形数据 MySQL 提供了两个方法来处理 ip 地址 inet_aton 把 ip 转为无符号整型 (4-8 位) inet_ntoa 把整型的 ip 转为地址 插入数据前，先用 inet_aton 把 ip 地址转为整型，可以节省空间，显示数据时，使用 inet_ntoa 把整型的 ip 地址转为地址显示即可。 b.对于非负型的数据 (如自增 ID,整型 IP) 来说,要优先使用无符号整型来存储 原因： 无符号相对于有符号可以多出一倍的存储空间 12SIGNED INT -2147483648~2147483647UNSIGNED INT 0~4294967295 VARCHAR(N) 中的 N 代表的是字符数，而不是字节数，使用 UTF8 存储 255 个汉字 Varchar(255)=765 个字节。过大的长度会消耗更多的内存。 2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中 MySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。 如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select * 而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。 2、TEXT 或 BLOB 类型只能使用前缀索引 因为MySQL 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的 3. 避免使用 ENUM 类型修改 ENUM 值需要使用 ALTER 语句 ENUM 类型的 ORDER BY 操作效率低，需要额外操作 禁止使用数值作为 ENUM 的枚举值 4. 尽可能把所有列定义为 NOT NULL原因： 索引 NULL 列需要额外的空间来保存，所以要占用更多的空间 进行比较和计算时要对 NULL 值做特别的处理 5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07 TIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高 超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储 经常会有人用字符串存储日期型的数据（不正确的做法） 缺点 1：无法用日期函数进行计算和比较 缺点 2：用字符串存储日期要占用更多的空间 6. 同财务相关的金额类数据必须使用 decimal 类型 非精准浮点：float,double 精准浮点：decimal Decimal 类型为精准浮点数，在计算时不会丢失精度 占用空间由定义的宽度决定，每 4 个字节可以存储 9 位数字，并且小数点要占用一个字节 可用于存储比 bigint 更大的整型数据 索引设计规范1. 限制每张表上的索引数量,建议单张表索引不超过 5 个索引并不是越多越好！索引可以提高效率同样可以降低效率。 索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。 因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。 2. 禁止给表中的每一列都建立单独的索引5.6 版本之前，一个 sql 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好。 3. 每个 Innodb 表必须有个主键Innodb 是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。 Innodb 是按照主键索引的顺序来组织表的 不要使用更新频繁的列作为主键，不适用多列主键（相当于联合索引） 不要使用 UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长） 主键建议使用自增 ID 值 4. 常见索引列建议 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列 包含在 ORDER BY、GROUP BY、DISTINCT 中的字段 并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好 多表 join 的关联列 5.如何选择索引列的顺序建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。 区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引） 6. 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） 重复索引示例：primary key(id)、index(id)、unique index(id) 冗余索引示例：index(a,b,c)、index(a,b)、index(a) 7. 对于频繁的查询优先考虑使用覆盖索引 覆盖索引：就是包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引 覆盖索引的好处： 避免 Innodb 表进行索引的二次查询: Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。 8.索引 SET 规范尽量避免使用外键约束 不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引 外键可用于保证数据的参照完整性，但建议在业务端实现 外键会影响父表和子表的写操作从而降低性能 数据库 SQL 开发规范1. 建议使用预编译语句进行数据库操作预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题。 只传参数，比传递 SQL 语句更高效。 相同语句可以一次解析，多次使用，提高处理效率。 2. 避免数据类型的隐式转换隐式转换会导致索引失效如: 1select name,phone from customer where id = &apos;111&apos;; 3. 充分利用表上已经存在的索引避免使用双%号的查询条件。如：a like &#39;%123%&#39;，（如果无前置%,只有后置%，是可以用到列上的索引的） 一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。 在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧，使用 left join 或 not exists 来优化 not in 操作，因为 not in 也通常会使用索引失效。 4. 数据库设计时，应该要对以后扩展进行考虑5. 程序连接不同的数据库使用不同的账号，禁止跨库查询 为数据库迁移和分库分表留出余地 降低业务耦合度 避免权限过大而产生的安全风险 6. 禁止使用 SELECT * 必须使用 SELECT &lt;字段列表&gt; 查询原因： 消耗更多的 CPU 和 IO 以网络带宽资源 无法使用覆盖索引 可减少表结构变更带来的影响 7. 禁止使用不含字段列表的 INSERT 语句如： 1insert into values (&apos;a&apos;,&apos;b&apos;,&apos;c&apos;); 应使用： 1insert into t(c1,c2,c3) values (&apos;a&apos;,&apos;b&apos;,&apos;c&apos;); 8. 避免使用子查询，可以把子查询优化为 join 操作通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。 子查询性能差的原因： 子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。 由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。 9. 避免使用 JOIN 关联太多的表对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。 在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。 如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。 同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。 10. 减少同数据库的交互次数数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。 11. 对应同一列进行 or 判断时，使用 in 代替 orin 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。 12. 禁止使用 order by rand() 进行随机排序order by rand() 会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的 CPU 和 IO 及内存资源。 推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。 13. WHERE 从句中禁止对列进行函数转换和计算对列进行函数转换或计算时会导致无法使用索引 不推荐： 1where date(create_time)=&apos;20190101&apos; 推荐： 1where create_time &gt;= &apos;20190101&apos; and create_time &lt; &apos;20190102&apos; 14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作 UNION ALL 不会再对结果集进行去重操作 15. 拆分复杂的大 SQL 为多个小 SQL 大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL MySQL 中，一个 SQL 只能使用一个 CPU 进行计算 SQL 拆分后可以通过并行执行来提高处理效率 数据库操作行为规范1. 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作大批量操作可能会造成严重的主从延迟 主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间，而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况 binlog 日志为 row 格式时会产生大量的日志 大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因 避免产生大事务操作 大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。 特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批 2. 对于大表使用 pt-online-schema-change 修改表结构 避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。 pt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。把原来一个 DDL 操作，分解成多个小的批次进行。 3. 禁止为程序使用的账号赋予 super 权限 当达到最大连接数限制时，还运行 1 个有 super 权限的用户连接 super 权限只能留给 DBA 处理问题的账号使用 4. 对于程序连接数据库账号,遵循权限最小原则 程序使用数据库账号只能在一个 DB 下使用，不准跨库 程序使用的账号原则上不准有 drop 权限]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP和HTTPS对比 以及HTTPS是如何保证安全性的]]></title>
    <url>%2F2019%2F04%2F29%2FHTTP%E5%92%8CHTTPS%E5%AF%B9%E6%AF%94%20%20HTTPS%E6%98%AF%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%AE%89%E5%85%A8%E6%80%A7%E7%9A%84%2F</url>
    <content type="text"><![CDATA[HTTP和HTTPS对比 HTTPS是如何保证安全性的HTTPS（全称：Hypertext Transfer Protocol over Secure Socket Layer）HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容就需要SSL Https的劣势对数据进行加解密决定了它比http慢需要进行非对称的加解密，且需要三次握手。首次连接比较慢点，当然现在也有很多的优化。出于安全考虑，浏览器不会在本地保存HTTPS缓存。当然，只要在HTTP头中使用特定命令，HTTPS也是可以缓存的。 HTTPS和HTTP的区别https协议需要到CA申请证书。http是超文本传输协议，信息是明文传输；https 则是具有安全性的ssl加密传输协议。http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。下面就是https的整个架构，现在的https基本都使用TLS了，因为更加安全，所以下图中的SSL应该换为SSL/TLS。 下面就上图中的知识点进行一个大概的介绍。 加解密相关知识对称加密对称加密(也叫私钥加密)指加密和解密使用相同密钥的加密算法。有时又叫传统密码算法，就是加密密钥能够从解密密钥中推算出来，同时解密密钥也可以从加密密钥中推算出来。而在大多数的对称算法中，加密密钥和解密密钥是相同的，所以也称这种加密算法为秘密密钥算法或单密钥算法。常见的对称加密有：DES（Data Encryption Standard）、AES（Advanced Encryption Standard）、RC4、IDEA 非对称加密与对称加密算法不同，非对称加密算法需要两个密钥：公开密钥（publickey）和私有密钥（privatekey）；并且加密密钥和解密密钥是成对出现的。非对称加密算法在加密和解密过程使用了不同的密钥，非对称加密也称为公钥加密，在密钥对中，其中一个密钥是对外公开的，所有人都可以获取到，称为公钥，其中一个密钥是不公开的称为私钥。 非对称加密算法对加密内容的长度有限制，不能超过公钥长度。比如现在常用的公钥长度是 2048 位，意味着待加密内容不能超过 256 个字节。 摘要算法数字摘要是采用单项Hash函数将需要加密的明文“摘要”成一串固定长度（128位）的密文，这一串密文又称为数字指纹，它有固定的长度，而且不同的明文摘要成密文，其结果总是不同的，而同样的明文其摘要必定一致。“数字摘要“是https能确保数据完整性和防篡改的根本原因。 数字签名数字签名技术就是对“非对称密钥加解密”和“数字摘要“两项技术的应用，它将摘要信息用发送者的私钥加密，与原文一起传送给接收者。接收者只有用发送者的公钥才能解密被加密的摘要信息，然后用HASH函数对收到的原文产生一个摘要信息，与解密的摘要信息对比。如果相同，则说明收到的信息是完整的，在传输过程中没有被修改，否则说明信息被修改过，因此数字签名能够验证信息的完整性。数字签名的过程如下： 明文 --&gt; hash运算 --&gt; 摘要 --&gt; 私钥加密 --&gt; 数字签名 数字签名有两种功效：一、能确定消息确实是由发送方签名并发出来的，因为别人假冒不了发送方的签名。二、数字签名能确定消息的完整性。 注意： 数字签名只能验证数据的完整性，数据本身是否加密不属于数字签名的控制范围 数字证书为什么要有数字证书？对于请求方来说，它怎么能确定它所得到的公钥一定是从目标主机那里发布的，而且没有被篡改过呢？亦或者请求的目标主机本本身就从事窃取用户信息的不正当行为呢？这时候，我们需要有一个权威的值得信赖的第三方机构(一般是由政府审核并授权的机构)来统一对外发放主机机构的公钥，只要请求方这种机构获取公钥，就避免了上述问题的发生。 数字证书的颁发过程用户首先产生自己的密钥对，并将公共密钥及部分个人身份信息传送给认证中心。认证中心在核实身份后，将执行一些必要的步骤，以确信请求确实由用户发送而来，然后，认证中心将发给用户一个数字证书，该证书内包含用户的个人信息和他的公钥信息，同时还附有认证中心的签名信息(根证书私钥签名)。用户就可以使用自己的数字证书进行相关的各种活动。数字证书由独立的证书发行机构发布，数字证书各不相同，每种证书可提供不同级别的可信度。 证书包含哪些内容 证书颁发机构的名称 证书本身的数字签名 证书持有者公钥 证书签名用到的Hash算法 验证证书的有效性浏览器默认都会内置CA根证书，其中根证书包含了CA的公钥 证书颁发的机构是伪造的：浏览器不认识，直接认为是危险证书 证书颁发的机构是确实存在的，于是根据CA名，找到对应内置的CA根证书、CA的公钥。用CA的公钥，对伪造的证书的摘要进行解密，发现解不了，认为是危险证书。 对于篡改的证书，使用CA的公钥对数字签名进行解密得到摘要A，然后再根据签名的Hash算法计算出证书的摘要B，对比A与B，若相等则正常，若不相等则是被篡改过的。 证书可在其过期前被吊销，通常情况是该证书的私钥已经失密。较新的浏览器如Chrome、Firefox、Opera和Internet Explorer都实现了在线证书状态协议（OCSP）以排除这种情形：浏览器将网站提供的证书的序列号通过OCSP发送给证书颁发机构，后者会告诉浏览器证书是否还是有效的。 1、2点是对伪造证书进行的，3是对于篡改后的证书验证，4是对于过期失效的验证。 SSL 与 TLSSSL (Secure Socket Layer，安全套接字层)SSL为Netscape所研发，用以保障在Internet上数据传输之安全，利用数据加密(Encryption)技术，可确保数据在网络上之传输过程中不会被截取，当前为3.0版本。 SSL协议可分为两层： SSL记录协议（SSL Record Protocol）：它建立在可靠的传输协议（如TCP）之上，为高层协议提供数据封装、压缩、加密等基本功能的支持。 SSL握手协议（SSL Handshake Protocol）：它建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等。 TLS (Transport Layer Security，传输层安全协议)用于两个应用程序之间提供保密性和数据完整性。TLS 1.0是IETF（Internet Engineering Task Force，Internet工程任务组）制定的一种新的协议，它建立在SSL 3.0协议规范之上，是SSL 3.0的后续版本，可以理解为SSL 3.1，它是写入了 RFC 的。该协议由两层组成： TLS 记录协议（TLS Record）和 TLS 握手协议（TLS Handshake）。较低的层为 TLS 记录协议，位于某个可靠的传输协议（例如 TCP）上面。 SSL/TLS协议作用： 认证用户和服务器，确保数据发送到正确的客户机和服务器； 加密数据以防止数据中途被窃取； 维护数据的完整性，确保数据在传输过程中不被改变。 TLS比SSL的优势 对于消息认证使用密钥散列法：TLS 使用“消息认证代码的密钥散列法”（HMAC），当记录在开放的网络（如因特网）上传送时，该代码确保记录不会被变更。SSLv3.0还提供键控消息认证，但HMAC比SSLv3.0使用的（消息认证代码）MAC 功能更安全。 增强的伪随机功能（PRF）：PRF生成密钥数据。在TLS中，HMAC定义PRF。PRF使用两种散列算法保证其安全性。如果任一算法暴露了，只要第二种算法未暴露，则数据仍然是安全的。 改进的已完成消息验证：TLS和SSLv3.0都对两个端点提供已完成的消息，该消息认证交换的消息没有被变更。然而，TLS将此已完成消息基于PRF和HMAC值之上，这也比SSLv3.0更安全。 一致证书处理：与SSLv3.0不同，TLS试图指定必须在TLS之间实现交换的证书类型。 特定警报消息：TLS提供更多的特定和附加警报，以指示任一会话端点检测到的问题。TLS还对何时应该发送某些警报进行记录。 SSL、TLS的握手过程SSL与TLS握手整个过程如下图所示，下面会详细介绍每一步的具体内容： 客户端首次发出请求由于客户端(如浏览器)对一些加解密算法的支持程度不一样，但是在TLS协议传输过程中必须使用同一套加解密算法才能保证数据能够正常的加解密。在TLS握手阶段，客户端首先要告知服务端，自己支持哪些加密算法，所以客户端需要将本地支持的加密套件(Cipher Suite)的列表传送给服务端。除此之外，客户端还要产生一个随机数，这个随机数一方面需要在客户端保存，另一方面需要传送给服务端，客户端的随机数需要跟服务端产生的随机数结合起来产生后面要讲到的 Master Secret 。 客户端需要提供如下信息： 支持的协议版本，比如TLS 1.0版 一个客户端生成的随机数，稍后用于生成”对话密钥” 支持的加密方法，比如RSA公钥加密 支持的压缩方法 服务端首次回应服务端在接收到客户端的Client Hello之后，服务端需要确定加密协议的版本，以及加密的算法，然后也生成一个随机数，以及将自己的证书发送给客户端一并发送给客户端，这里的随机数是整个过程的第二个随机数。 服务端需要提供的信息： 协议的版本 加密的算法 随机数 服务器证书 客户端再次回应客户端首先会对服务器下发的证书进行验证，验证通过之后，则会继续下面的操作，客户端再次产生一个随机数（第三个随机数），然后使用服务器证书中的公钥进行加密，以及放一个ChangeCipherSpec消息即编码改变的消息，还有整个前面所有消息的hash值，进行服务器验证，然后用新秘钥加密一段数据一并发送到服务器，确保正式通信前无误。客户端使用前面的两个随机数以及刚刚新生成的新随机数，使用与服务器确定的加密算法，生成一个Session Secret。 ChangeCipherSpecChangeCipherSpec是一个独立的协议，体现在数据包中就是一个字节的数据，用于告知服务端，客户端已经切换到之前协商好的加密套件（Cipher Suite）的状态，准备使用之前协商好的加密套件加密数据并传输了。 服务器再次响应服务端在接收到客户端传过来的第三个随机数的 加密数据之后，使用私钥对这段加密数据进行解密，并对数据进行验证，也会使用跟客户端同样的方式生成秘钥，一切准备好之后，也会给客户端发送一个 ChangeCipherSpec，告知客户端已经切换到协商过的加密套件状态，准备使用加密套件和 Session Secret加密数据了。之后，服务端也会使用 Session Secret 加密一段 Finish 消息发送给客户端，以验证之前通过握手建立起来的加解密通道是否成功。 后续客户端与服务器间通信确定秘钥之后，服务器与客户端之间就会通过商定的秘钥加密消息了，进行通讯了。整个握手过程也就基本完成了。 值得特别提出的是：SSL协议在握手阶段使用的是非对称加密，在传输阶段使用的是对称加密，也就是说在SSL上传送的数据是使用对称密钥加密的！因为非对称加密的速度缓慢，耗费资源。其实当客户端和主机使用非对称加密方式建立连接后，客户端和主机已经决定好了在传输过程使用的对称加密算法和关键的对称加密密钥，由于这个过程本身是安全可靠的，也即对称加密密钥是不可能被窃取盗用的，因此，保证了在传输过程中对数据进行对称加密也是安全可靠的，因为除了客户端和主机之外，不可能有第三方窃取并解密出对称加密密钥！如果有人窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两个。整个通话的安全，只取决于第三个随机数（Premaster secret）能不能被破解。 其他补充对于非常重要的保密数据，服务端还需要对客户端进行验证，以保证数据传送给了安全的合法的客户端。服务端可以向客户端发出 Cerficate Request 消息，要求客户端发送证书对客户端的合法性进行验证。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供USB密钥，里面就包含了一张客户端证书。 PreMaster secret前两个字节是TLS的版本号，这是一个比较重要的用来核对握手数据的版本号，因为在Client Hello阶段，客户端会发送一份加密套件列表和当前支持的SSL/TLS的版本号给服务端，而且是使用明文传送的，如果握手的数据包被破解之后，攻击者很有可能串改数据包，选择一个安全性较低的加密套件和版本给服务端，从而对数据进行破解。所以，服务端需要对密文中解密出来对的PreMaster版本号跟之前Client Hello阶段的版本号进行对比，如果版本号变低，则说明被串改，则立即停止发送任何消息。 session的恢复有两种方法可以恢复原来的session：一种叫做session ID，另一种叫做session ticket。 session IDsession ID的思想很简单，就是每一次对话都有一个编号（session ID）。如果对话中断，下次重连的时候，只要客户端给出这个编号，且服务器有这个编号的记录，双方就可以重新使用已有的”对话密钥”，而不必重新生成一把。 session ID是目前所有浏览器都支持的方法，但是它的缺点在于session ID往往只保留在一台服务器上。所以，如果客户端的请求发到另一台服务器，就无法恢复对话 session ticket客户端发送一个服务器在上一次对话中发送过来的session ticket。这个session ticket是加密的，只有服务器才能解密，其中包括本次对话的主要信息，比如对话密钥和加密方法。当服务器收到session ticket以后，解密后就不必重新生成对话密钥了。 目前只有Firefox和Chrome浏览器支持。 总结https实际就是在TCP层与http层之间加入了SSL/TLS来为上层的安全保驾护航，主要用到对称加密、非对称加密、证书，等技术进行客户端与服务器的数据加密传输，最终达到保证整个通信的安全性。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>HTTPS</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java性能调优：利用VisualVM进行性能分析]]></title>
    <url>%2F2019%2F04%2F01%2FJava%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-%E5%88%A9%E7%94%A8VisualVM%E8%BF%9B%E8%A1%8C%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[JVisualVM 简介VisualVM 是Netbeans的profile子项目，已在JDK6.0 update 7 中自带，能够监控线程，内存情况，查看方法的CPU时间和内存中的对 象，已被GC的对象，反向查看分配的堆栈(如100个String对象分别由哪几个对象分配出来的)。在JDK_HOME/bin(默认是C:\Program Files\Java\jdk1.6.0_13\bin)目录下面，有一个jvisualvm.exe文件，双击打开，从UI上来看，这个软件是基于NetBeans开发的了。 VisualVM 提供了一个可视界面，用于查看 Java 虚拟机 (Java Virtual Machine, JVM) 上运行的基于 Java 技术的应用程序的详细信息。VisualVM 对 Java Development Kit (JDK) 工具所检索的 JVM 软件相关数据进行组织，并通过一种使您可以快速查看有关多个 Java 应用程序的数据的方式提供该信息。您可以查看本地应用程序或远程主机上运行的应用程序的相关数据。此外，还可以捕获有关 JVM 软件实例的数据，并将该数据保存到本地系统，以供后期查看或与其他用户共享。 双击启动 jvisualvm.exe，启动起来后和jconsole 一样同样可以选择本地和远程，如果需要监控远程同样需要配置相关参数。 主界面如下：VisualVM可以根据需要安装不同的插件，每个插件的关注点都不同，有的主要监控GC，有的主要监控内存，有的监控线程等。如何安装： 1、从主菜单中选择“工具”&gt;“插件”。 2、在“可用插件”标签中，选中该插件的“安装”复选框。单击“安装”。 3、逐步完成插件安装程序。 我这里以 Eclipse(pid 22296)为例，双击后直接展开，主界面展示了系统和jvm两大块内容，点击右下方jvm参数和系统属性可以参考详细的参数信息.因为VisualVM的插件太多，我这里主要介绍三个我主要使用几个：监控、线程、Visual GC 监控的主页其实也就是，cpu、内存、类、线程的图表线程和jconsole功能没有太大的区别Visual GC 是常常使用的一个功能，可以明显的看到年轻代、老年代的内存变化，以及gc频率、gc的时间等。以上的功能其实jconsole几乎也有，VisualVM更全面更直观一些，另外VisualVM非常多的其它功能，可以分析dump的内存快照， dump出来的线程快照并且进行分析等，还有其它很多的插件大家可以去探索 案例分析准备模拟内存泄漏样例1、定义静态变量HashMap2、分段循环创建对象，并加入HashMap3、配置jvm参数如下： -Xms512m -Xmx512m -XX:-UseGCOverheadLimit -XX:MaxPermSize=50m 4、运行程序并打卡visualvm监控代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.util.HashMap;import java.util.Map;public class CyclicDependencies &#123; //声明缓存对象 private static final Map map = new HashMap(); public static void main(String args[])&#123; try &#123; Thread.sleep(10000);//给打开visualvm时间 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //循环添加对象到缓存 for(int i=0; i&lt;1000000;i++)&#123; TestMemory t = new TestMemory(); map.put("key"+i,t); &#125; System.out.println("first"); //为dump出堆提供时间 try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; for(int i=0; i&lt;1000000;i++)&#123; TestMemory t = new TestMemory(); map.put("key"+i,t); &#125; System.out.println("second"); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; for(int i=0; i&lt;3000000;i++)&#123; TestMemory t = new TestMemory(); map.put("key"+i,t); &#125; System.out.println("third"); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; for(int i=0; i&lt;4000000;i++)&#123; TestMemory t = new TestMemory(); map.put("key"+i,t); &#125; System.out.println("forth"); try &#123; Thread.sleep(Integer.MAX_VALUE); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("qqqq"); &#125;&#125; 使用JVisualVM分析内存泄漏 1、查看Visual GC标签，内容如下，这是输出first的截图 这是输出forth的截图：通过2张图对比发现：老生代一直在gc，当程序继续运行可以发现老生代gc还在继续：增加到了7次，但是老生代的内存并没有减少。说明存在无法被回收的对象，可能是内存泄漏了。如何分析是那个对象泄漏了呢？打开抽样器标签：点击后如下图：按照程序输出进行堆dump，当输出second时，dump一次，当输出forth时dump一次。进入最后dump出来的堆标签，点击类： 比较结果如下：可以看出在两次间隔时间内TestMemory对象实例一直在增加并且多了，说明该对象引用的方法可能存在内存泄漏。如何查看对象引用关系呢？右键选择类TestMemory，选择“在实例视图中显示”，如下所示：左侧是创建的实例总数，右侧上部为该实例的结构，下面为引用说明，从图中可以看出在类CyclicDependencies里面被引用了，并且被HashMap引用。如此可以确定泄漏的位置，进而根据实际情况进行分析解决。 JVisualVM 远程监控 Tomcat1、修改远程tomcat的catalina.sh配置文件，在其中增加： JAVA_OPTS=&quot;$JAVA_OPTS -Djava.rmi.server.hostname=192.168.122.128 -Dcom.sun.management.jmxremote.port=18999 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false&quot;这次配置先不走权限校验。只是打开jmx端口。2、打开jvisualvm，右键远程，选择添加远程主机： 3、输入主机的名称，直接写ip，如下： 右键新建的主机，选择添加JMX连接，输入在tomcat中配置的端口即可。4、双击打开。完毕！]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>VisualVM</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[垃圾回收算法]]></title>
    <url>%2F2019%2F01%2F07%2F%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[垃圾回收算法垃圾回收算法有很多种，目前商业虚拟机常用的是分代回收算法，最初并不是用这个算法的所以需要先了解一下垃圾收集算法的背景知识： 要讨论垃圾回收算法首先得知道虚拟机是如何判断一个对象是“活”还是“死”的 这里涉及到 两种标记算法：1.引用计数算法引用计数算法非常简单且高效，当一个对象被引用一次则+1不再被引用则-1，当计数为0就是不可能在被使用的对象了，但是这种算法存在一个致命的缺陷：两个对象相互引用对方呢？ 2.可达性分析算法目前的标记算法主流实现都是用的可达性分析算法。就是以一个叫GC Roots的对象为起点，通过引用链向下搜索，如果一个对象通过引用链无法与GC Roots对象链接，就视为可回收对象，上面说的那种相互引用的情况自然也解决了。 扩展：即使是可达性分析中不可达的对象也并不是非死不可，只是暂处‘缓刑’，真正宣告一个对象死亡至少还要经历两次标记过程：当被判定不可达之后那么他被第一次标记并进行筛选，若对象没有覆盖finalize()方法或者finalize()方法已经被虚拟机调用过就‘放生’，如果被判定需要执行finalize()方法就会被放到一个叫F-Queue的队列中进行第二次标记对象被再次被引用就会放生，否则就会被回收。 finalize()方法finalize()是Object中的方法，当垃圾回收器将要回收对象所占内存之前被调用，即当一个对象被虚拟机宣告死亡时会先调用它finalize()方法，让此对象处理它生前的最后事情（这个对象可以趁这个时机挣脱死亡的命运） 引用说到这里敏锐的小伙伴可能以及察觉到了，上面都在说引用所以引用的定义就显得尤为关键了JDK1.2后Java对引用的概念进行了扩充，将引用分为：强引用、软引用、弱引用、虚引用四种 强引用：用处很大，无论如何都不会被GC回收 软引用：有一定用处但不大，内存实在不够才会在内存溢出之前回收掉 弱引用：比软引用强度更弱一些，GC会让它多活一轮，下一轮就回收 虚引用：必回收，唯一作用就是被GC回收时会收到一个系统通知 标记-清除算法最基础的垃圾回收算法，顾名思义，整个回收过程分两步：1.逐个标记2.统一回收该算法可以算是后来所有垃圾回收算法的基石（后续所有算法都有标记和清除这两步，只不过策略上有了一些优化） 复制算法前面说的标记-清除算法其实两个过程效率都很低，并且回收之后内存被‘抠出很多洞’内存碎片化严重，此时如果过来了一个较大的对象，找不到一整块连续的内存空间就不得不提前触发另外一次GC回收。而复制算法则选择将内存一分为二每次只使用其中一半，满了之后将存活的对象整齐复制到另一块干净的内存上，将剩下的碎片一次性擦除，简单高效。但是也存在一个很大的缺陷，那就是可用内存变为原来的一半了。 标记整理算法复制算法的高效性是建立在存活对象少、垃圾对象多的前提下的。这种情况在新生代经常发生，但是在老年代更常见的情况是大部分对象都是存活对象。如果依然使用复制算法，由于存活的对象较多，复制的成本也将很高。标记-整理算法是一种老年代的回收算法，它在标记-清除算法的基础上做了一些优化。首先也需要从根节点开始对所有可达对象做一次标记，但之后，它并不简单地清理未标记的对象，而是将所有的存活对象压缩到内存的一端。之后，清理边界外所有的空间。这种方法既避免了碎片的产生，又不需要两块相同的内存空间，因此，其性价比比较高。 分代收集算法1.年轻代：复制算法事实上后来IBM公司经过研究发现，98%的对象都是‘朝生夕死’，所以并不需要1:1的划分内存，即我们现在常用的分代收集算法：根据对象的存活周期将内存划分为两块，分别为新生代和老年代，然后对各代采用不同的回收算法，在新生代中大部分是‘朝生夕死’的对象，继续将新生代8:2划分为Eden区和survival区，其中survival区1:1分成s0和s1两块，采用之前说的复制算法，减少内存碎片的产生。新生代满了会进行一次minor GC ，minor GC 存活的对象转移到survival区，survival区满了就会将survival区进行回收，存活的survival区对象复制到另外一块survival区中，并且survival区对象每存活一轮年龄+1当到达一定年龄就会前往老年代。 2.年老代：标记-清除或标记-整理算法当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。 以上这种年轻代与年老代分别采用不同回收算法的方式称为”分代收集算法”，这也是当下企业使用的一种方式 垃圾回收机制年轻代分为Eden区和survivor区（两块儿：from和to），且Eden:from:to==8:1:1。 1）新产生的对象优先分配在Eden区（除非配置了-XX:PretenureSizeThreshold，大于该值的对象会直接进入年老代）； 2）当Eden区满了或放不下了，这时候其中存活的对象会复制到from区。 这里，需要注意的是，如果存活下来的对象from区都放不下，则这些存活下来的对象全部进入年老代。之后Eden区的内存全部回收掉。3）之后产生的对象继续分配在Eden区，当Eden区又满了或放不下了，这时候将会把Eden区和from区存活下来的对象复制到to区（同理，如果存活下来的对象to区都放不下，则这些存活下来的对象全部进入年老代），之后回收掉Eden区和from区的所有内存。 4）如上这样，会有很多对象会被复制很多次（每复制一次，对象的年龄就+1），默认情况下，当对象被复制了15次（这个次数可以通过-XX:MaxTenuringThreshold来配置），就会进入年老代了。 5）当年老代满了或者存放不下将要进入年老代的存活对象的时候，就会发生一次Full GC（这个是我们最需要减少的，因为耗时很严重）。 垃圾回收的两种类型：Minor GC 和 Full GC。1.Minor GC对新生代进行回收，不会影响到年老代。因为新生代的 Java 对象大多死亡频繁，所以 Minor GC 非常频繁，一般在这里使用速度快、效率高的算法，使垃圾回收能尽快完成。 2.Full GC也叫Major GC，对整个堆进行回收，包括新生代和老年代。由于Full GC需要对整个堆进行回收，所以比MinorGC要慢，因此应该尽可能减少Full GC的次数，导致FullGC的原因包括：老年代被写满、永久代（Perm）被写满和System.gc()被显式调用等。 最后：扩展01：JVM何时会进行全局GC 01.手动调用System.GC 但也不是立即调用 02.老年代空间不足 03.永生代空间不足 04.计算得知新生代前往老年代平均值大于老年代剩余空间 扩展02：在压力测试时，发现FullGC频率很高，如何解决 01.观察GC日志，判断是否有内存泄漏，或者存在内部不合理点 02.调整JVM参数，如新生代、老年代大小 S0+S1大小比例，选用不同的立即回收器 03.Dump内存，做进一步的对象分析 04.压测脚本的编写，性能问题解决前可以发现问题，并对解决方案进行验证]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java锁 总结]]></title>
    <url>%2F2019%2F01%2F07%2FJava%E9%94%81%20%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[锁的种类： synchronize自动锁（最常用） 可以给类、方法、代码块加锁 lock手动锁，只能锁代码块儿，且需要手动加锁解锁，忘记解锁会造成死锁 volatile轻量级锁，不会造成线程阻塞，只能修饰变量，且只能保证变量的修改可见性，无法保证原子性 解决死锁的方法： 1)尽量使用tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。 2)尽量使用java.util.concurrent(jdk 1.5以上)包的并发类代替手写控制并发，比较常用的是ConcurrentHashMap、ConcurrentLinkedQueue、AtomicBoolean等等，实际应用中java.util.concurrent.atomic十分有用，简单方便且效率比使用Lock更高 3)尽量降低锁的使用粒度，尽量不要几个功能用同一把锁 4)尽量减少同步的代码块 悲观锁与乐观锁 悲观锁用于线程冲突率高的场景，用提前加锁保证线程安全 乐观锁用于线程冲突率底的场景，用修改前后版本号是否一致保证线程安全 未完待续]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap为什么不是线程安全的？]]></title>
    <url>%2F2018%2F12%2F13%2FHashMap%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E6%98%AF%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E7%9A%84%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[当Hashmap的容量不够时，需要扩容扩容包含扩容和ReHash两个步骤，ReHash在并发的情况下可能会形成链表环。具体细节参考：漫画：高并发下的HashMap(https://mp.weixin.qq.com/s/dzNq50zBQ4iDrOAhM4a70A) 程序员小灰]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Java</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【Redis】pipeline管道打包处理模式]]></title>
    <url>%2F2018%2F11%2F01%2F%E3%80%90Redis%E3%80%91pipeline%E7%AE%A1%E9%81%93%E6%89%93%E5%8C%85%E5%A4%84%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Redis本身是一个cs模式的tcp server, client可以通过一个socket连续发起多个请求命令。 每个请求命令发出后client通常会阻塞并等待redis服务端处理，redis服务端处理完后将结果返回给client。 由于我们知道Redis非常快，这种发送模式中的性能瓶颈其实在于请求传输速度，就算redis server端有很强的处理能力，也由于收到的client消息少，而造成吞吐量小。我们可以修改一种处理模式：通过pipeline方式将client端命令一起发出，redis server会处理完多条命令后，将结果一起打包返回client,从而节省大量的网络延迟开销。下面以Java的客户端jedis来测试pipeline的效果。 12345678Pipeline pipeline = jedis.pipelined(); long start = System.currentTimeMillis(); for (int i = 0; i &lt; 10000; i++) &#123; pipeline.hset("server", "" + i, "" + i); &#125; List&lt;Object&gt; results = pipeline.execute(); long end = System.currentTimeMillis(); System.out.println("Pipelined SET: " + ((end - start)/1000.0) + " seconds"); 测试的结果采用pipeline方式，效率几乎与mset一样，每秒插入约15万数据，但内存占用仅为mset的1/3.]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【消息队列】几种常见的MQ总结对比]]></title>
    <url>%2F2018%2F10%2F01%2F%E3%80%90%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E3%80%91%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84MQ%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[提问： 为什么使用消息队列？ 消息队列有什么优点和缺点？ Kafka、ActiveMQ、RabbitMQ、RocketMQ 都有什么区别，以及适合哪些场景？面试官心理分析 其实面试官主要是想看看： 第一，你知不知道你们系统里为什么要用消息队列这个东西？ 不少候选人，说自己项目里用了 Redis、MQ，但是其实他并不知道自己为什么要用这个东西。其实说白了，就是为了用而用，或者是别人设计的架构，他从头到尾都没思考过。 没有对自己的架构问过为什么的人，一定是平时没有思考的人，面试官对这类候选人印象通常很不好。因为面试官担心你进了团队之后只会木头木脑的干呆活儿，不会自己思考。 第二，你既然用了消息队列这个东西，你知不知道用了有什么好处&amp;坏处？ 你要是没考虑过这个，那你盲目弄个 MQ 进系统里，后面出了问题你是不是就自己溜了给公司留坑？你要是没考虑过引入一个技术可能存在的弊端和风险，面试官把这类候选人招进来了，基本可能就是挖坑型选手。就怕你干 1 年挖一堆坑，自己跳槽了，给公司留下无穷后患。 第三，既然你用了 MQ，可能是某一种 MQ，那么你当时做没做过调研？ 你别傻乎乎的自己拍脑袋看个人喜好就瞎用了一个 MQ，比如 Kafka，甚至都从没调研过业界流行的 MQ 到底有哪几种。每一个 MQ 的优点和缺点是什么。每一个 MQ 没有绝对的好坏，但是就是看用在哪个场景可以扬长避短，利用其优势，规避其劣势。 如果是一个不考虑技术选型的候选人招进了团队，leader 交给他一个任务，去设计个什么系统，他在里面用一些技术，可能都没考虑过选型，最后选的技术可能并不一定合适，一样是留坑。面试题剖析为什么使用消息队列 其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么？ 面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用 MQ 可能会很麻烦，但是你现在用了 MQ 之后带给了你很多的好处。 先说一下消息队列常见的使用场景吧，其实场景有很多，但是比较核心的有 3 个：解耦、异步、削峰。解耦 看这么个场景。A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃…… mq-1 在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！ 如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。 mq-2 总结：通过一个 MQ，Pub/Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。 面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用 MQ 给它异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个 MQ 去进行系统的解耦。在简历中体现出来这块东西，用 MQ 作解耦。异步 再来看一个场景，A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 = 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。 mq-3 一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。 如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了，爽！网站做得真好，真快！ mq-4削峰 每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。 一般的 MySQL，扛到每秒 2k 个请求就差不多了，如果每秒请求到 5k 的话，可能就直接把 MySQL 给打死了，导致系统崩溃，用户也就没法再使用系统了。 但是高峰期一过，到了下午的时候，就成了低峰期，可能也就 1w 的用户同时在网站上操作，每秒中的请求数量可能也就 50 个请求，对整个系统几乎没有任何的压力。 mq-5 如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。 mq-6 这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。消息队列有什么优缺点 优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰。 缺点有以下几个： 系统可用性降低 系统引入的外部依赖越多，越容易挂掉。本来你就是 A 系统调用 BCD 三个系统的接口就好了，人 ABCD 四个系统好好的，没啥问题，你偏加个 MQ 进来，万一 MQ 挂了咋整，MQ 一挂，整套系统崩溃的，你不就完了？如何保证消息队列的高可用，可以点击这里查看。 系统复杂度提高 硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 一致性问题 A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，咋整？你这数据就不一致了。所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，做好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了 10 倍。但是关键时刻，用，还是得用的。 如何选择MQ 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 怎么保证消息没有重复消费 1.如果是拿这个消息做数据库insert操作（事实上update和delete重复也不影响）给这个消息做一个唯一主键，那么就算出现重复消费的情况，就会导致主键冲突，避免数据库出现脏数据。 2.当拿到这个消息做redis的set的操作，那就容易了，不用解决，因为你无论set几次结果都是一样的，set操作本来就算幂等操作。 3.如果上面两种情况还不行，准备一个第三方存储,来做消费记录。以redis为例，给消息分配一个全局id，只要消费过该消息，将&lt;id,message&gt;以K-V形式写入redis。那消费者开始消费前，先去redis中查询有没消费记录即可。 怎么处理消息丢失的情况怎么保证消息传递的顺序性怎么保证多系统消息一致性]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【设计模式】工厂模式]]></title>
    <url>%2F2018%2F10%2F01%2F%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[工厂模式分为三种：简单工厂模式、工厂模式、抽象工厂模式从实现上看，代码复杂度依次上升简单工厂模式：采用switch语句根据传入的参数不同返回不同的对象，缺点是必须得传参，传参有问题会导致调用不成功，且后期扩展不方便，如果要加一个对象需要改动原来的代码工厂模式：将每个对象写作不同的方法，不用传参，且后期扩展方便，直接加方法就行抽象工厂模式：将方法加上了static修饰，使用的时候直接调用，不用实例化，更方便]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>工厂模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu环境搭建]]></title>
    <url>%2F2018%2F06%2F07%2Fubuntu%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1.重装系统2.设置root用户密码：​ sudo passwd root3.切换 root 用户：​ su root4.允许远程连接root用户： apt-get install vim vim /etc/ssh/sshd_config 添加 PermitRootLogin yes 允许远程连接 skip-grant-tables 跳过远程连接时的权限验证5.安装mysql： sudo apt-get update sudo apt-get install mysql-server6.配置mysql：​ sudo mysql_secure_installation​ （密码安全级别 0-密码 root-是否确认密码 Y-删除匿名用户 Y-是否禁止远程登录 N-是否删除test数据库-Y -是否重新加载权限表Y）7.安装JDK: sudo apt-get update sudo apt-get install openjdk-8-jdk java -version]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java四种常用线程池]]></title>
    <url>%2F2018%2F03%2F01%2FJava%E5%9B%9B%E7%A7%8D%E5%B8%B8%E7%94%A8%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[创建线程池时通过设置线程池各项参数可以创建性能各异的线程池 1234567new ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) 但官方也给我们设置好了四种常用的线程池，可以直接调用 Java通过Executors提供四种线程池，分别为： newCachedThreadPool 可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 1. newCachedThreadPool 可缓存线程池创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。示例代码如下： 123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; try &#123; Thread.sleep(index * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; cachedThreadPool.execute(new Runnable() &#123; public void run() &#123; System.out.println(index); &#125; &#125;); &#125; &#125; &#125; 线程池为无限大，当执行第二个任务时第一个任务已经完成，会复用执行第一个任务的线程，而不用每次新建线程。 2. newFixedThreadPool 定长线程池创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。示例代码如下： 123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService fixedThreadPool = Executors.newFixedThreadPool(3); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; fixedThreadPool.execute(new Runnable() &#123; public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; &#125; 因为线程池大小为3，每个任务输出index后sleep 2秒，所以每两秒打印3个数字。定长线程池的大小最好根据系统资源进行设置。如Runtime.getRuntime().availableProcessors() 3. newScheduledThreadPool 定时线程池创建一个定长线程池，支持定时及周期性任务执行。延迟执行示例代码如下： 1234567891011121314151617181920212223242526272829package test; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.schedule(new Runnable() &#123; public void run() &#123; System.out.println("延迟3秒执行"); &#125; &#125;, 3, TimeUnit.SECONDS); &#125; &#125; package test; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.scheduleAtFixedRate(new Runnable() &#123; public void run() &#123; System.out.println("延迟1秒后每3秒执行一次"); &#125; &#125;, 1, 3, TimeUnit.SECONDS); &#125; &#125; 4. newSingleThreadExecutor 单线程线程池创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。示例代码如下： 123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; singleThreadExecutor.execute(new Runnable() &#123; public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; &#125;]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【消息队列】RabbitMQ如何处理消息丢失]]></title>
    <url>%2F2018%2F03%2F01%2F%E3%80%90%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E3%80%91RabbitMQ%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1%2F</url>
    <content type="text"><![CDATA[首先明确一点 一条消息的传送流程：生产者-&gt;MQ-&gt;消费者 所以有三个地方都会丢失数据： 生产者发送给MQ的途中出现网络问题 MQ自己没保管好弄丢了 消费者拿到数据后出错了没有最终完成任务依次分析 1）生产者弄丢了数据 生产者将数据发送到rabbitmq的时候，可能因为网络问题导致数据就在半路给搞丢了。 1.使用事务（性能差）可以选择用rabbitmq提供的事务功能，在生产者发送数据之前开启rabbitmq事务（channel.txSelect），然后发送消息，如果消息没有成功被rabbitmq接收到，那么生产者会收到异常报错，此时就可以回滚事务（channel.txRollback），然后重试发送消息；如果收到了消息，那么可以提交事务（channel.txCommit）。但是问题是，开始rabbitmq事务机制，基本上吞吐量会下来，因为太耗性能。 2.发送回执确认（推荐）可以开启confirm模式，在生产者那里设置开启confirm模式之后，你每次写的消息都会分配一个唯一的id，然后如果写入了rabbitmq中，rabbitmq会给你回传一个ack消息，告诉你说这个消息ok了。如果rabbitmq没能处理这个消息，会回调你一个nack接口，告诉你这个消息接收失败，你可以重试。而且你可以结合这个机制自己在内存里维护每个消息id的状态，如果超过一定时间还没接收到这个消息的回调，那么你可以重发。 事务机制和cnofirm机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息rabbitmq接收了之后会异步回调你一个接口通知你这个消息接收到了。 所以一般在生产者这块避免数据丢失，都是用confirm机制的。 2）RabbitMQ弄丢了数据-开启RabbitMQ的数据持久化 为了防止rabbitmq自己弄丢了数据，这个你必须开启rabbitmq的持久化，就是消息写入之后会持久化到磁盘，哪怕是rabbitmq自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。除非极其罕见的是，rabbitmq还没持久化，自己就挂了，可能导致少量数据会丢失的，但是这个概率较小。 设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证rabbitmq持久化queue的元数据，但是不会持久化queue里的数据；第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时rabbitmq就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，rabbitmq哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。 而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，rabbitmq挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。 若生产者那边的confirm机制未开启的情况下，哪怕是你给rabbitmq开启了持久化机制，也有一种可能，就是这个消息写到了rabbitmq中，但是还没来得及持久化到磁盘上，结果不巧，此时rabbitmq挂了，就会导致内存里的一点点数据会丢失。 3）消费端弄丢了数据 主要是因为你消费的时候，刚消费到，还没处理，结果进程挂了比如重启了，那么就尴尬了，RabbitMQ认为你都消费了，这数据就丢了。或者消费者拿到数据之后挂了，这时候需要MQ重新指派另一个消费者去执行任务（一块肉，刚用筷子夹起来，发地震抖了一下，肉掉了） 这个时候得用RabbitMQ提供的ack机制，也是一种处理完成发送回执确认的机制。如果MQ等待一段时间后你没有发送过来处理完成 那么RabbitMQ就认为你还没处理完，这个时候RabbitMQ会把这个消费分配给别的consumer去处理，消息是不会丢的。]]></content>
      <categories>
        <category>消息中间件</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>消息中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[过滤器与拦截器的区别]]></title>
    <url>%2F2018%2F02%2F11%2F%E8%BF%87%E6%BB%A4%E5%99%A8%E4%B8%8E%E6%8B%A6%E6%88%AA%E5%99%A8%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[-----《Spring源码深度解析》]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>过滤器</tag>
        <tag>拦截器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[让 hexo 博客在后台跑起来]]></title>
    <url>%2F2018%2F01%2F01%2F%E8%AE%A9%20hexo%20%E5%8D%9A%E5%AE%A2%E5%9C%A8%E5%90%8E%E5%8F%B0%E8%B7%91%E8%B5%B7%E6%9D%A5%2F</url>
    <content type="text"><![CDATA[让 hexo 博客在后台跑起来 前几天利用 hexo 搭建了一个博客，于是很高兴地用 $ hexo server 在我的 Linux 服务器上跑了起来。 但是发现，只要关闭了进程，博客也就跟着关闭了。想着，这样的博客给谁看啊，总不能我本地一直开着进程吧。 于是我开始搜索相关 hexo 后台运行的方法，貌似很多人也有同样的问题，官方给出的方法是$ hexo s &amp;,试了一下进程还是莫名其妙中断了。于是乎看到一篇文章说使用 forever 可以解决，这个给了我启发，那么就用 pm2 运行我的博客不就行了。 那么开始操作： 安装pm21npm install -g pm2 写一个执行脚本学习一下 Node.js 官方文档的 child_process.exec(command[, options][, callback]) 部分 参考官方文档 在博客根目录下创建一个文件 run.js 12345678910//run.jsconst &#123; exec &#125; = require('child_process')exec('hexo server',(error, stdout, stderr) =&gt; &#123; if(error)&#123; console.log(`exec error: $&#123;error&#125;`) return &#125; console.log(`stdout: $&#123;stdout&#125;`); console.log(`stderr: $&#123;stderr&#125;`);&#125;) 运行脚本进入博客根目录 1pm2 start run.js]]></content>
      <categories>
        <category>个人博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>个人博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后端程序员必备的Linux基础知识]]></title>
    <url>%2F2018%2F01%2F01%2F%E5%90%8E%E7%AB%AF%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BF%85%E5%A4%87%E7%9A%84Linux%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[一 从认识操作系统开始 1.1 操作系统简介 1.2 操作系统简单分类 二 初探Linux 2.1 Linux简介 2.2 Linux诞生简介 2.3 Linux的分类 三 Linux文件系统概览 3.1 Linux文件系统简介 3.2 文件类型与目录结构 四 Linux基本命令 4.1 目录切换命令 4.2 目录的操作命令（增删改查） 4.3 文件的操作命令（增删改查） 4.4 压缩文件的操作命令 4.5 Linux的权限命令 4.6 Linux 用户管理 4.7 Linux系统用户组的管理 4.8 其他常用命令 推荐一个Github开源的Linux学习指南(Java工程师向)：https://github.com/judasn/Linux-Tutorial 学习Linux之前，我们先来简单的认识一下操作系统。 一 从认识操作系统开始1.1 操作系统简介我通过以下四点介绍什么操作系统： 操作系统（Operation System，简称OS）是管理计算机硬件与软件资源的程序，是计算机系统的内核与基石； 操作系统本质上是运行在计算机上的软件程序 ； 为用户提供一个与系统交互的操作界面 ； 操作系统分内核与外壳（我们可以把外壳理解成围绕着内核的应用程序，而内核就是能操作硬件的程序）。 1.2 操作系统简单分类 Windows: 目前最流行的个人桌面操作系统 ，不做多的介绍，大家都清楚。 Unix： 最早的多用户、多任务操作系统 .按照操作系统的分类，属于分时操作系统。Unix 大多被用在服务器、工作站，现在也有用在个人计算机上。它在创建互联网、计算机网络或客户端/服务器模型方面发挥着非常重要的作用。 Linux: Linux是一套免费使用和自由传播的类Unix操作系统.Linux存在着许多不同的Linux版本，但它们都使用了 Linux内核 。Linux可安装在各种计算机硬件设备中，比如手机、平板电脑、路由器、视频游戏控制台、台式计算机、大型机和超级计算机。严格来讲，Linux这个词本身只表示Linux内核，但实际上人们已经习惯了用Linux来形容整个基于Linux内核，并且使用GNU 工程各种工具和数据库的操作系统。 二 初探Linux2.1 Linux简介我们上面已经介绍到了Linux，我们这里只强调三点。 类Unix系统： Linux是一种自由、开放源码的类似Unix的操作系统 Linux内核： 严格来说，Linux这个词本身只表示Linux内核 Linux之父： 一个编程领域的传奇式人物。他是Linux内核的最早作者，随后发起了这个开源项目，担任Linux内核的首要架构师与项目协调者，是当今世界最著名的电脑程序员、黑客之一。他还发起了Git这个开源项目，并为主要的开发者。 2.2 Linux诞生简介 1991年，芬兰的业余计算机爱好者Linus Torvalds编写了一款类似Minix的系统（基于微内核架构的类Unix操作系统）被ftp管理员命名为Linux 加入到自由软件基金的GNU计划中; Linux以一只可爱的企鹅作为标志，象征着敢作敢为、热爱生活。 2.3 Linux的分类Linux根据原生程度，分为两种： 内核版本： Linux不是一个操作系统，严格来讲，Linux只是一个操作系统中的内核。内核是什么？内核建立了计算机软件与硬件之间通讯的平台，内核提供系统服务，比如文件管理、虚拟内存、设备I/O等； 发行版本： 一些组织或公司在内核版基础上进行二次开发而重新发行的版本。Linux发行版本有很多种（ubuntu和CentOS用的都很多，初学建议选择CentOS），如下图所示： 三 Linux文件系统概览3.1 Linux文件系统简介在Linux操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。 也就是说在LINUX系统中有一个重要的概念：一切都是文件。其实这是UNIX哲学的一个体现，而Linux是重写UNIX而来，所以这个概念也就传承了下来。在UNIX系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。 3.2 文件类型与目录结构Linux支持5种文件类型 ： Linux的目录结构如下： Linux文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： 常见目录说明： /bin： 存放二进制可执行文件(ls、cat、mkdir等)，常用命令一般都在这里； /etc： 存放系统管理和配置文件； /home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示； /usr ： 用于存放系统应用程序； /opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里； /proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息； /root： 超级用户（系统管理员）的主目录（特权阶级^o^）； /sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等； /dev： 用于存放设备文件； /mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统； /boot： 存放用于系统引导时使用的各种文件； /lib ： 存放着和系统运行相关的库文件 ； /tmp： 用于存放各种临时文件，是公用的临时文件存储点； /var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等； /lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。 四 Linux基本命令下面只是给出了一些比较常用的命令。推荐一个Linux命令快查网站，非常不错，大家如果遗忘某些命令或者对某些命令不理解都可以在这里得到解决。 Linux命令大全：http://man.linuxde.net/ 4.1 目录切换命令 cd usr： 切换到该目录下usr目录 cd ..（或cd../）： 切换到上一层目录 cd /： 切换到系统根目录 cd ~： 切换到用户主目录 cd -： 切换到上一个操作所在目录 4.2 目录的操作命令(增删改查) mkdir 目录名称： 增加目录 ls或者ll（ll是ls -l的别名，ll命令可以看到该目录下的所有目录和文件的详细信息）：查看目录信息 find 目录 参数： 寻找目录（查） 示例： 列出当前目录及子目录下所有文件和文件夹: find . 在/home目录下查找以.txt结尾的文件名:find /home -name &quot;*.txt&quot; 同上，但忽略大小写: find /home -iname &quot;*.txt&quot; 当前目录及子目录下查找所有以.txt和.pdf结尾的文件:find . \( -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot; \)或find . -name &quot;*.txt&quot; -o -name &quot;*.pdf&quot; mv 目录名称 新目录名称： 修改目录的名称（改） 注意：mv的语法不仅可以对目录进行重命名而且也可以对各种文件，压缩包等进行 重命名的操作。mv命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。后面会介绍到mv命令的另一个用法。 mv 目录名称 目录的新位置： 移动目录的位置—剪切（改） 注意：mv语法不仅可以对目录进行剪切操作，对文件和压缩包等都可执行剪切操作。另外mv与cp的结果不同，mv好像文件“搬家”，文件个数并未增加。而cp对文件进行复制，文件个数增加了。 cp -r 目录名称 目录拷贝的目标位置： 拷贝目录（改），-r代表递归拷贝 注意：cp命令不仅可以拷贝目录还可以拷贝文件，压缩包等，拷贝文件和压缩包时不 用写-r递归 rm [-rf] 目录: 删除目录（删） 注意：rm不仅可以删除目录，也可以删除其他文件或压缩包，为了增强大家的记忆， 无论删除任何目录或文件，都直接使用rm -rf 目录/文件/压缩包 4.3 文件的操作命令(增删改查) touch 文件名称: 文件的创建（增） cat/more/less/tail 文件名称 文件的查看（查） cat： 查看显示文件内容 more： 可以显示百分比，回车可以向下一行， 空格可以向下一页，q可以退出查看 less： 可以使用键盘上的PgUp和PgDn向上 和向下翻页，q结束查看 tail-10 ： 查看文件的后10行，Ctrl+C结束 注意：命令 tail -f 文件 可以对某个文件进行动态监控，例如tomcat的日志文件， 会随着程序的运行，日志会变化，可以使用tail -f catalina-2016-11-11.log 监控 文 件的变化 vim 文件： 修改文件的内容（改） vim编辑器是Linux中的强大组件，是vi编辑器的加强版，vim编辑器的命令和快捷方式有很多，但此处不一一阐述，大家也无需研究的很透彻，使用vim编辑修改文件的方式基本会使用就可以了。 在实际开发中，使用vim编辑器主要作用就是修改配置文件，下面是一般步骤： vim 文件——&gt;进入文件—–&gt;命令模式——&gt;按i进入编辑模式—–&gt;编辑文件 ——-&gt;按Esc进入底行模式—–&gt;输入：wq/q! （输入wq代表写入内容并退出，即保存；输入q!代表强制退出不保存。） rm -rf 文件： 删除文件（删） 同目录删除：熟记 rm -rf 文件 即可 4.4 压缩文件的操作命令1）打包并压缩文件： Linux中的打包文件一般是以.tar结尾的，压缩的命令一般是以.gz结尾的。 而一般情况下打包和压缩是一起进行的，打包并压缩后的文件的后缀名一般.tar.gz。命令：tar -zcvf 打包压缩后的文件名 要打包压缩的文件其中： z：调用gzip压缩命令进行压缩 c：打包文件 v：显示运行过程 f：指定文件名 比如：加入test目录下有三个文件分别是：aaa.txt bbb.txt ccc.txt，如果我们要打包test目录并指定压缩后的压缩包名称为test.tar.gz可以使用命令：tar -zcvf test.tar.gz aaa.txt bbb.txt ccc.txt或：tar -zcvf test.tar.gz /test/ 2）解压压缩包： 命令：tar [-xvf] 压缩文件 其中：x：代表解压 示例： 1 将/test下的test.tar.gz解压到当前目录下可以使用命令：tar -xvf test.tar.gz 2 将/test下的test.tar.gz解压到根目录/usr下:tar -xvf xxx.tar.gz -C /usr（- C代表指定解压的位置） 4.5 Linux的权限命令 操作系统中每个文件都拥有特定的权限、所属用户和所属组。权限是操作系统用来限制资源访问的机制，在Linux中权限一般分为读(readable)、写(writable)和执行(excutable)，分为三组。分别对应文件的属主(owner)，属组(group)和其他用户(other)，通过这样的机制来限制哪些用户、哪些组可以对特定的文件进行什么样的操作。通过 ls -l 命令我们可以 查看某个目录下的文件或目录的权限 示例：在随意某个目录下ls -l 第一列的内容的信息解释如下： 下面将详细讲解文件的类型、Linux中权限以及文件有所有者、所在组、其它组具体是什么？ 文件的类型： d： 代表目录 -： 代表文件 l： 代表软链接（可以认为是window中的快捷方式） Linux中权限分为以下几种： r：代表权限是可读，r也可以用数字4表示 w：代表权限是可写，w也可以用数字2表示 x：代表权限是可执行，x也可以用数字1表示 文件和目录权限的区别： 对文件和目录而言，读写执行表示不同的意义。 对于文件： 权限名称 可执行操作 r 可以使用cat查看文件的内容 w 可以修改文件的内容 x 可以将其运行为二进制文件 对于目录： 权限名称 可执行操作 r 可以查看目录下列表 w 可以创建和删除目录下文件 x 可以使用cd进入目录 需要注意的是超级用户可以无视普通用户的权限，即使文件目录权限是000，依旧可以访问。在linux中的每个用户必须属于一个组，不能独立于组外。在linux中每个文件有所有者、所在组、其它组的概念。 所有者 一般为文件的创建者，谁创建了该文件，就天然的成为该文件的所有者，用ls ‐ahl命令可以看到文件的所有者 也可以使用chown 用户名 文件名来修改文件的所有者 。 文件所在组 当某个用户创建了一个文件后，这个文件的所在组就是该用户所在的组 用ls ‐ahl命令可以看到文件的所有组 也可以使用chgrp 组名 文件名来修改文件所在的组。 其它组 除开文件的所有者和所在组的用户外，系统的其它用户都是文件的其它组 我们再来看看如何修改文件/目录的权限。 修改文件/目录的权限的命令：chmod 示例：修改/test下的aaa.txt的权限为属主有全部权限，属主所在的组有读写权限，其他用户只有读的权限 chmod u=rwx,g=rw,o=r aaa.txt 上述示例还可以使用数字表示： chmod 764 aaa.txt 补充一个比较常用的东西: 假如我们装了一个zookeeper，我们每次开机到要求其自动启动该怎么办？ 新建一个脚本zookeeper 为新建的脚本zookeeper添加可执行权限，命令是:chmod +x zookeeper 把zookeeper这个脚本添加到开机启动项里面，命令是：chkconfig --add zookeeper 如果想看看是否添加成功，命令是：chkconfig --list 4.6 Linux 用户管理Linux系统是一个多用户多任务的分时操作系统，任何一个要使用系统资源的用户，都必须首先向系统管理员申请一个账号，然后以这个账号的身份进入系统。 用户的账号一方面可以帮助系统管理员对使用系统的用户进行跟踪，并控制他们对系统资源的访问；另一方面也可以帮助用户组织文件，并为用户提供安全性保护。 Linux用户管理相关命令: useradd 选项 用户名:添加用户账号 userdel 选项 用户名:删除用户帐号 usermod 选项 用户名:修改帐号 passwd 用户名:更改或创建用户的密码 passwd -S 用户名 :显示用户账号密码信息 passwd -d 用户名: 清除用户密码 useradd命令用于Linux中创建的新的系统用户。useradd可用来建立用户帐号。帐号建好之后，再用passwd设定帐号的密码．而可用userdel删除帐号。使用useradd指令所建立的帐号，实际上是保存在/etc/passwd文本文件中。 passwd命令用于设置用户的认证信息，包括用户密码、密码过期时间等。系统管理者则能用它管理系统用户的密码。只有管理者可以指定用户名称，一般用户只能变更自己的密码。 4.7 Linux系统用户组的管理每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 Linux系统用户组的管理相关命令: groupadd 选项 用户组 :增加一个新的用户组 groupdel 用户组:要删除一个已有的用户组 groupmod 选项 用户组 : 修改用户组的属性 4.8 其他常用命令 pwd： 显示当前所在位置 grep 要搜索的字符串 要搜索的文件 --color： 搜索命令，–color代表高亮显示 ps -ef/ps -aux： 这两个命令都是查看当前系统正在运行进程，两者的区别是展示格式不同。如果想要查看特定的进程可以使用这样的格式：ps aux|grep redis （查看包括redis字符串的进程），也可使用 pgrep redis -a。 注意：如果直接用ps（（Process Status））命令，会显示所有进程的状态，通常结合grep命令查看某进程的状态。 kill -9 进程的pid： 杀死进程（-9 表示强制终止。） 先用ps查找进程，然后用kill杀掉 网络通信命令： 查看当前系统的网卡信息：ifconfig 查看与某台机器的连接情况：ping 查看当前系统的端口使用：netstat -an net-tools 和 iproute2 ： net-tools起源于BSD的TCP/IP工具箱，后来成为老版本Linux内核中配置网络功能的工具。但自2001年起，Linux社区已经对其停止维护。同时，一些Linux发行版比如Arch Linux和CentOS/RHEL 7则已经完全抛弃了net-tools，只支持iproute2。linux ip命令类似于ifconfig，但功能更强大，旨在替代它。更多详情请阅读如何在Linux中使用IP命令和示例 shutdown： shutdown -h now： 指定现在立即关机；shutdown +5 &quot;System will shutdown after 5 minutes&quot;：指定5分钟后关机，同时送出警告信息给登入用户。 reboot： reboot： 重开机。reboot -w： 做个重开机的模拟（只有纪录并不会真的重开机）。]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>服务器</tag>
        <tag>操作系统</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务的隔离级别]]></title>
    <url>%2F2017%2F11%2F11%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[事务 就是要保证一组数据库操作，要么全部成功，要么全部失败。 在MySQL中，事务支持是在引擎层实现的 MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如MySQL原生的MyISAM引擎就不支持事务，这也是MyISAM被InnoDB取代的重要原因之一。事务的四大特性：ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。 SQL标准的事务隔离级别包括：（从上到下越来越严实） 读未提交（read uncommitted） 读提交（read committed） 可重复读（repeatable read） 串行化（serializable ）逐一解释： 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。这会带来脏读、幻读、不可重复读问题。（基本没用） 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。避免了脏读，但仍然存在不可重复读和幻读问题。 可重复读是指，一个事务执行过程中看到的数据，这个事务自己看到的数据始终不变（当然他可能已经被其他事务改变了）在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 避免了脏读和不可重复读问题，但幻读依然存在。 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行，避免了以上所有问题。（基本没用） 直接看文字描述可能不太好理解，那我们来看图吧 我们来看看在不同的隔离级别下，事务A会有哪些不同的返回结果，也就是图里面V1、V2、V3的返回值分别是什么。 若隔离级别是“读未提交”， 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2。 若隔离级别是“读提交”，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2。 若隔离级别是“可重复读”，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2。在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle数据库的默认隔离级别其实就是“读提交”，因此对于一些从Oracle迁移到MySQL的应用，为保证数据库隔离级别的一致，你一定要记得将MySQL的隔离级别设置为“读提交”。 总结来说，存在即合理，哪个隔离级别都有它自己的使用场景，你要根据自己的业务情况来定。我想你可能会问那什么时候需要“可重复读”的场景呢？我们来看一个数据校对逻辑的案例。 假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。 这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。 悲观锁与乐观锁 悲观锁，正如它的名字那样，数据库总是认为别人会去修改它所要操作的数据，因此在数据库处理过程中将数据加锁。其实现依靠数据库底层。 乐观锁，如它的名字那样，总是认为别人不会去修改，只有在提交更新的时候去检查数据的状态。通常是给数据增加一个字段来标识数据的版本。 MySQL的MVCC（多版本并发控制） 我们知道，MySQL的innodb采用的是行锁，而且采用了多版本并发控制来提高读操作的性能。 什么是多版本并发控制呢 ？其实就是在每一行记录的后面增加两个隐藏列，记录创建版本号和删除版本号， 而每一个事务在启动的时候，都有一个唯一的递增的版本号。 1、在插入操作时 ： 记录的创建版本号就是事务版本号。 比如我插入一条记录, 事务id 假设是1 ，那么记录如下：也就是说，创建版本号就是事务版本号。 2、在更新操作的时候，采用的是先标记旧的那行记录为已删除，并且删除版本号是事务版本号，然后插入一行新的记录的方式。 比如，针对上面那行记录，事务Id为2 要把name字段更新 update table set name= ‘new_value’ where id=1; 3、删除操作的时候，就把事务版本号作为删除版本号。比如 delete from table where id=1; 4、查询操作： 从上面的描述可以看到，在查询时要符合以下两个条件的记录才能被事务查询出来： 1) 删除版本号 大于 当前事务版本号，就是说删除操作是在当前事务启动之后做的。 2) 创建版本号 小于或者等于 当前事务版本号 ，就是说记录创建是在事务中（等于的情况）或者事务启动之前。 这样就保证了各个事务互不影响。从这里也可以体会到一种提高系统性能的思路，就是： 通过版本号来减少锁的争用。 另外，只有read-committed和 repeatable-read 两种事务隔离级别才能使用mVcc read-uncommited由于是读到未提交的，所以不存在版本的问题 而serializable 则会对所有读取的行加锁。 问题：那我们用什么办法能看到两个隐藏列呢？]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【设计模式】动态代理]]></title>
    <url>%2F2017%2F11%2F05%2F%E3%80%90%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%91%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[个人理解：一个工厂生产冰箱，冰箱从工厂生产出来到消费者手中一般还有有一个代理商，提供一些服务，比如附赠一些小礼品，送货上门等等Spring中就使用了动态代理的思想，比如Spring的反向代理加依赖注入，就相当于送货上门Spring里的AOF面向切面编程就使用了动态代理达到事务控制、日志打印功能，就相当于附赠一些小礼品。 按照代理的创建时期，代理类可以分为两种： 静态代理：由程序员创建代理类或特定工具自动生成源代码再对其编译。在程序运行前代理类的.class文件就已经存在了。 动态代理：在程序运行时运用反射机制动态创建而成。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>动态代理</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从输入url到显示网页发生了什么]]></title>
    <url>%2F2017%2F06%2F10%2F%E4%BB%8E%E8%BE%93%E5%85%A5url%E5%88%B0%E6%98%BE%E7%A4%BA%E7%BD%91%E9%A1%B5%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[在浏览器中输入url到显示网页主要包含两个部分： 网络通信+页面渲染 互联网内各网络设备间的通信都遵循TCP/IP协议，利用TCP/IP协议族进行网络通信时，会通过分层顺序与对方进行通信。分层由高到低分别为：应用层、传输层、网络层、数据链路层。发送端从应用层往下走，接收端从数据链路层网上走 1.浏览器的地址栏输入URL并按下回车 我们常见的RUL是这样的:www.baidu.com,域名通常由3部分组成：协议 域名 端口号 协议：主要是HTTP协议，HTTPS协议，FTP协议，FILe协议域名：url中间部分为域名或者IP端口号：通常默认都是隐藏的 http默认端口号为80 https默认端口号为443 涉及知识点： 跨域在前端进行数据请求时，由于浏览器的同源策略，协议，域名，端口号有一个不同会存在跨域请求，需要进行跨域处理，相关的跨域方法点击user-gold-cdn.xitu.io/2018/11/19/… 2.DNS域名解析 互联网上每一台计算机的唯一标识是它的IP地址，但是IP地址并不方便记忆。用户更喜欢用方便记忆的网址去寻找互联网上的其它计算机，也就是上面提到的百度的网址。所以互联网设计者需要在用户的方便性与可用性方面做一个权衡，这个权衡就是一个网址到IP地址的转换，这个过程就是DNS解析，即实现了网址到IP地址的转换解析过程DNS解析是一个递归查询的过程。 上述图片是查找www.google.com的IP地址过程。首先在本地域名服务器中查询IP地址，如果没有找到的情况下，本地域名服务器会向根域名服务器发送一个请求，如果根域名服务器也不存在该域名时，本地域名会向com顶级域名服务器发送一个请求，依次类推下去。直到最后本地域名服务器得到google的IP地址并把它缓存到本地，供下次查询使用。从上述过程中，可以看出网址的解析是一个从右向左的过程: com -&gt; google.com -&gt; www.google.com。但是你是否发现少了点什么，根域名服务器的解析过程呢？事实上，真正的网址是www.google.com.，并不是我多打了一个.，这个.对应的就是根域名服务器，默认情况下所有的网址的最后一位都是.，既然是默认情况下，为了方便用户，通常都会省略，浏览器在请求DNS的时候会自动加上，所有网址真正的解析过程为: . -&gt; .com -&gt; google.com. -&gt; www.google.com.。DNS优化DNS缓存和DNS负载均衡DNS缓存DNS存在着多级缓存，从离浏览器的距离排序的话，有以下几种: 浏览器缓存，系统缓存，路由器缓存，IPS服务器缓存，根域名服务器缓存，顶级域名服务器缓存，主域名服务器缓存。 在你的chrome浏览器中输入:chrome://dns/，你可以看到chrome浏览器的DNS缓存。 系统缓存主要存在/etc/hosts(Linux系统)中: DNS负载均衡真实的互联网世界背后存在成千上百台服务器，大型的网站甚至更多。但是在用户的眼中，它需要的只是处理他的请求，哪台机器处理请求并不重要。DNS可以返回一个合适的机器的IP给用户，例如可以根据每台机器的负载量，该机器离用户地理位置的距离等等，这种过程就是DNS负载均衡，又叫做DNS重定向 3.建立TCP连接 在通过DNS域名解析后，获取到了服务器的IP地址，在获取到IP地址后，便会开始建立一次连接，这是由TCP协议完成的，主要通过三次握手进行连接。 第一次握手： 建立连接时，客户端发送syn包（syn=j）到服务器，并进入SYN_SENT状态，等待服务器确认；第二次握手： 服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态；第三次握手： 客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1），此包发送完毕，客户端和服务器进入ESTABLISHED（TCP连接成功）状态，完成三次握手。这里需要了解下ACK，SYN的意义 完成TCP连接后开使向服务器进行请求 4.向服务器发送请求 完整的HTTP请求包含请求起始行、请求头部、请求主体三部分。 5.服务器接受响应 服务器在收到浏览器发送的HTTP请求之后，会将收到的HTTP报文封装成HTTP的Request对象，并通过不同的Web服务器进行处理，处理完的结果以HTTP的Response对象返回，主要包括状态码，响应头，响应报文三个部分。状态码主要包括以下部分: 1xx：指示信息–表示请求已接收，继续处理。2xx：成功–表示请求已被成功接收、理解、接受。3xx：重定向–要完成请求必须进行更进一步的操作。4xx：客户端错误–请求有语法错误或请求无法实现。5xx：服务器端错误–服务器未能实现合法的请求。 响应头主要由Cache-Control、 Connection、Date、Pragma等组成。响应体为服务器返回给浏览器的信息，主要由HTML，css，js，图片文件组成。 6.页面渲染 如果说响应的内容是HTML文档的话，就需要浏览器进行解析渲染呈现给用户。整个过程涉及两个方面：解析和渲染。在渲染页面之前，需要构建DOM树和CSSOM树。 在浏览器还没接收到完整的 HTML 文件时，它就开始渲染页面了，在遇到外部链入的脚本标签或样式标签或图片时，会再次发送 HTTP 请求重复上述的步骤。在收到 CSS 文件后会对已经渲染的页面重新渲染，加入它们应有的样式，图片文件加载完立刻显示在相应位置。在这一过程中可能会触发页面的重绘或重排。这里就涉及了两个重要概念：Reflow和Repaint。 Reflow，也称作Layout，中文叫回流，一般意味着元素的内容、结构、位置或尺寸发生了变化，需要重新计算样式和渲染树，这个过程称为Reflow。Repaint，中文重绘，意味着元素发生的改变只是影响了元素的一些外观之类的时候（例如，背景色，边框颜色，文字颜色等），此时只需要应用新样式绘制这个元素就OK了，这个过程称为Repaint。 所以说Reflow的成本比Repaint的成本高得多的多。DOM树里的每个结点都会有reflow方法，一个结点的reflow很有可能导致子结点，甚至父点以及同级结点的reflow。 7.关闭TCP连接或继续保持连接 通过四次挥手关闭连接(FIN ACK, ACK, FIN ACK, ACK)。 第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式]]></title>
    <url>%2F2016%2F06%2F07%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[单例模式是 确保一个类只有一个实例，自行实例化并向系统提供这个实例一个类只有一个实例对象，避免了重复实例的频繁创建和销毁降低了资源消耗并且共用一个对象有利于数据同步，例如WINDOWS的任务管理器、回收站、网站的计数器、线程池对象、配置文件的读取对象等两种创建方式：1.饿汉单例模式（最常用）： 单例实例在类装载时就构建，急切初始化。（预先加载法）特点：线程安全、在类加载的同时已经创建好一个静态对象，调用时反应速度快，有可能从没用到，有一点点的资源浪费 123456789101112//饿汉单例模式Demo public class SingletonTest &#123; //1.私有化该类的构造方法（不让别人new，只能自己new） private SingletonTest() &#123; &#125; //2.自己内部new一个对象 public static SingletonTest instance = new SingletonTest(); //3.给一个get方法，让外界取它 public SingletonTest getInstance() &#123; return instance; &#125; &#125; ​ 2.懒汉单例模式： 单例实例在第一次被使用时构建，延迟初始化。 12345678910111213141516//懒汉单例模式Demo public class SingletonTest2 &#123; //1.私有化该类的构造方法（不让别人new，只能自己new） private SingletonTest2() &#123; &#125; //2.自己内部维护一个null对象（只要被调用一次就不再是了） public static SingletonTest2 instance = null; //3.给一个get方法，让外界取它，只有有人用才会new一个对象出来 public SingletonTest2 getInstance() &#123; if (instance == null) &#123; //TODO 多线程下可能会出现重复new的情况 instance = new SingletonTest2(); &#125; return instance; &#125; &#125; ​总结： 两种模式各有所长 一种是时间换空间 一种是空间换时间 根据具体场景使用]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring相关知识点总结]]></title>
    <url>%2F2016%2F05%2F01%2FSpring%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Spring是什么 Spring是一个一站式轻量级的开源框架Spring Bean的三种配置方式：xml、注解和Java @Configuration public class BeanConfig { @Bean public BeanFactory beanFactory(){ return new BeanFactoryImpl(); } }Spring的核心： 控制反转（IoC）和面向切面（AOP） AOP代理主要分为静态代理和动态代理 AOP实现的关键在于 代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ； 动态代理则以Spring AOP为代表。 （1）AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类 因此也称为编译时增强，他会在编译阶段将AspectJ(切面)织入到Java字节码中 运行的时候就是增强之后的AOP对象。 （2）Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码 而是每次运行时在内存中临时为方法生成一个AOP对象 这个AOP对象包含了目标对象的全部方法 并且在特定的切点做了增强处理，并回调原对象的方法。Spring中AOP动态代理的两种实现方式： JDK动态代理和CGLIB动态代理 Spring的优点： Spring将对象之间的依赖关系交由框架处理，减低组件的耦合性； Spring对于主流的应用框架提供了非常方便的集成支持； Spring提供的AOP功能，方便进行面向切面的编程，许多不容易用传统OOP实现的功能可以通过AOP轻松应付； Spring框架设计精妙，Spring源码是经典的学习范例Spring的七大组成模块： Spring Core：核心类库，提供IOC服务； Spring Context：提供框架式的Bean访问方式，以及企业级功能（JNDI、定时任务等）； Spring AOP：AOP服务； Spring DAO：对JDBC的抽象，简化了数据访问异常的处理； Spring ORM：对现有的ORM框架的支持； Spring Web：提供了基本的面向Web的综合特性，例如多方文件上传； Spring MVC：提供面向Web应用的Model-View-Controller实现 Spring事务的实现方式和实现原理： Spring事务的本质其实就是数据库对事务的支持 没有数据库的事务支持，spring是无法提供事务功能的。 真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。Spring的IOC有三种注入方式 ： 构造器注入、setter方法注入、注解注入 Spring框架支持以下五种bean的作用域： singleton : bean在每个Spring ioc 容器中只有一个实例。 prototype：一个bean的定义可以有多个实例。 request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。 session：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。缺省的Spring bean 的作用域是Singleton. Spring支持两种类型的事务管理： 编程式事务管理：这意味你通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。 声明式事务管理：这意味着你可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。大多数Spring框架的用户选择声明式事务管理，因为它对应用代码的影响最小，因此更符合一个无侵入的轻量级容器的思想。声明式事务管理要优于编程式事务管理，虽然比编程式事务管理（这种方式允许你通过代码控制事务）少了一点灵活性。 SpringMVC 请求处理流程 1、 用户发送请求被前端控制器DispatcherServlet捕获 2、 DispatcherServlet收到请求调用处理器映射器HandlerMapping。 3、 处理器映射器找到具体的处理器(可以根据xml配置、注解进行查找)，生成映射信息 返回给DispatcherServlet。 4、 DispatcherServlet调用处理器适配器HandlerAdapter。 5、 处理器适配器根据映射信息适配调用具体的处理器(Controller，也叫后端控制器) Controller执行完成返回ModelAndView给DispatcherServlet。 6、 DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体视图view。 7、 DispatcherServlet根据视图View进行渲染视图并且响应给用户。 整个过程都是以DispatcherServlet为中心进行的。 你怎样定义类的作用域? 当定义一个 在Spring里，我们还能给这个bean声明一个作用域。它可以通过bean 定义中的scope属性来定义。如，当Spring要在需要的时候每次生产一个新的bean实例，bean的scope属性被指定为prototype。 另一方面，一个bean每次使用的时候必须返回同一个实例，这个bean的scope 属性 必须设为 singleton。解释Spring支持的几种bean的作用域 Spring框架支持以下五种bean的作用域： singleton : bean在每个Spring ioc 容器中只有一个实例。 prototype ：一个bean的定义可以有多个实例。 request ：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。 session ：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 global-session ：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。缺省的Spring bean 的作用域是Singleton. Spring框架中的单例bean是线程安全的吗? 不，Spring框架中的单例bean不是线程安全的。解释Spring框架中bean的生命周期 PS：可以借鉴Servlet的生命周期，实例化、初始init、接收请求service、销毁destroy。 Spring的核心容器会首先从XML 文件中读取bean的定义，并实例化bean。 然后根据bean的定义填充所有的属性。 然后根据bean内部实现了哪些接口依次调用一堆方法，最后初始化，最后的最后destroy。 如果bean实现了BeanNameAware 接口，Spring 传递bean 的ID 到 setBeanName方法。 如果Bean 实现了 BeanFactoryAware 接口， Spring传递beanfactory 给setBeanFactory 方法。 如果有任何与bean相关联的BeanPostProcessors，Spring会在postProcesserBeforeInitialization()方法内调用它们。 如果bean实现IntializingBean了，调用它的afterPropertySet方法， 如果bean声明了初始化方法，调用此初始化方法。 如果有BeanPostProcessors 和bean 关联，这些bean的postProcessAfterInitialization() 方法将被调用。 如果bean实现了 DisposableBean，它将调用destroy()方法。 哪些是重要的bean生命周期方法？ 你能重载它们吗？ 有两个重要的bean 生命周期方法，第一个是setup ， 它是在容器加载bean的时候被调用。第二个方法是 teardown 它是在容器卸载类的时候被调用。 The bean 标签有两个重要的属性（init-method和destroy-method）。 用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct和@PreDestroy）]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK-JRE-JVM对比]]></title>
    <url>%2F2016%2F04%2F01%2FJDK-JRE-JVM%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[JDK（Java Development Kit）是针对Java开发员的产品，是整个Java的核心，包括了Java运行环境JRE、Java工具和Java基础类库。Java Runtime Environment（JRE）是运行JAVA程序所必须的环境的集合，包含JVM标准实现及Java核心类库。JVM是Java Virtual Machine（Java虚拟机）的缩写，是整个java实现跨平台的最核心的部分，能够运行以Java语言写作的软件程序。 JDK包括了Java运行环境JRE 、一堆Java工具（javac/java/jdb等）和Java基础的类库（即Java API 包括rt.jar）。 rt.jar：Java基础类库，也就是Java doc里面看到的所有的类的class文件。 tools.jar：是系统用来编译一个类的时候用到的，即执行javac的时候用到。 dt.jar：dt.jar是关于运行环境的类库，主要是swing包。JREJava运行环境，并不是一个开发环境，所以没有包含任何开发工具（如编译器和调试器），只是针对于使用Java程序的用户。JRE中包含了 JVM 、runtime class libraries和Java application launcher，这些是运行Java程序的必要组件。 JVM就是我们常说的java虚拟机，它是整个java实现跨平台的最核心的部分，所有的java程序会首先被编译为.class的类文件，这种类文件可以在虚拟机上执行。 也就是说class并不直接与机器的操作系统相对应，而是经过虚拟机间接与操作系统交互，由虚拟机将程序解释给本地系统执行。 只有JVM还不能成class的执行，因为在解释class的时候JVM需要调用解释所需要的类库lib，而jre包含lib类库。 JVM屏蔽了与具体操作系统平台相关的信息，使得Java程序只需生成在Java虚拟机上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JDK</tag>
        <tag>JRE</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《深入理解计算机系统》 day01]]></title>
    <url>%2F2015%2F12%2F20%2F%E3%80%8A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E3%80%8B%20day01%2F</url>
    <content type="text"><![CDATA[预处理阶段预处理器（cpp）识别以#字符开头的指令 修改源程序，比如#iclude&lt;stdio.h&gt;告诉预处理器读取系统头文件stdio.h的内容并把它插入程序文本中就得到另一个C程序，以 .i 为文件扩展名 编译阶段编译器（ccl）将hello.i翻译成文本文件hello.s，它包含一个汇编程序，每条语句都以一种文本格式描述了一条低级的机器语言指令 汇编阶段接下来汇编器（as）将hello.s翻译成机器语言指令，并将结果打包在一个二进制文件 hello.o 中 链接阶段将程序中需要用到的函数，例如printf，从printf.o包中用链接器（ld）合并到hello.o程序中得到一个hello文件，加载到内存中交给系统执行。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>计算机基础</tag>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[选择排序]]></title>
    <url>%2F2015%2F09%2F02%2F%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[思路首先，找到数组中最小的元素，拎出来，将它和数组的第一个元素交换位置，第二步，在剩下的元素中继续寻找最小的元素，拎出来，和数组的第二个元素交换位置，如此循环，直到整个数组排序完成。 至于选大还是选小，这个都无所谓，你也可以每次选择最大的拎出来排，也可以每次选择最小的拎出来的排，只要你的排序的手段是这种方式，都叫选择排序。代码实现 1234567891011121314public static void sort(int arr[])&#123; for( int i = 0;i &lt; arr.length ; i++ )&#123; int min = i;//最小元素的下标 for(int j = i + 1;j &lt; arr.length ; j++ )&#123; if(arr[j] &lt; arr[min])&#123; min = j;//找最小值 &#125; &#125; //交换位置 int temp = arr[i]; arr[i] = arr[min]; arr[min] = temp; &#125;&#125; 双层循环，时间复杂度和冒泡一模一样，都是O(n2)。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>选择排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[希尔排序]]></title>
    <url>%2F2015%2F09%2F02%2F%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[希尔排序这个名字，来源于它的发明者希尔，也称作“缩小增量排序”，是插入排序的一种更高效的改进版本。 我们知道，插入排序对于大规模的乱序数组的时候效率是比较慢的，因为它每次只能将数据移动一位，希尔排序为了加快插入的速度，让数据移动的时候可以实现跳跃移动，节省了一部分的时间开支。 代码实现 123456789101112131415161718192021public static void sort(int[] arr) &#123; int length = arr.length; //区间 int gap = 1; while (gap &lt; length) &#123; gap = gap * 3 + 1; &#125; while (gap &gt; 0) &#123; for (int i = gap; i &lt; length; i++) &#123; int tmp = arr[i]; int j = i - gap; //跨区间排序 while (j &gt;= 0 &amp;&amp; arr[j] &gt; tmp) &#123; arr[j + gap] = arr[j]; j -= gap; &#125; arr[j + gap] = tmp; &#125; gap = gap / 3; &#125;&#125; 可能你会问为什么区间要以 gap = gap*3 + 1 去计算，其实最优的区间计算方法是没有答案的，这是一个长期未解决的问题，不过差不多都会取在二分之一到三分之一附近。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>希尔排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆排序]]></title>
    <url>%2F2015%2F09%2F01%2F%E5%A0%86%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[堆排序顾名思义，是利用堆这种数据结构来进行排序的算法。 如果你不了解堆这种数据结构，可以查看小吴之前的数据结构系列文章—看动画轻松理解堆 如果你了解堆这种数据结构，你应该知道堆是一种优先队列，两种实现，最大堆和最小堆，由于我们这里排序按升序排，所以就直接以最大堆来说吧。 我们完全可以把堆（以下全都默认为最大堆）看成一棵完全二叉树，但是位于堆顶的元素总是整棵树的最大值，每个子节点的值都比父节点小，由于堆要时刻保持这样的规则特性，所以一旦堆里面的数据发生变化，我们必须对堆重新进行一次构建。 既然堆顶元素永远都是整棵树中的最大值，那么我们将数据构建成堆后，只需要从堆顶取元素不就好了吗？ 第一次取的元素，是否取的就是最大值？取完后把堆重新构建一下，然后再取堆顶的元素，是否取的就是第二大的值？ 反复的取，取出来的数据也就是有序的数据。 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public static void sort(int[] arr) &#123; int length = arr.length; //构建堆 buildHeap(arr， length); for ( int i = length - 1; i &gt; 0; i-- ) &#123; //将堆顶元素与末位元素调换 int temp = arr[0]; arr[0] = arr[i]; arr[i] = temp; //数组长度-1 隐藏堆尾元素 length--; //将堆顶元素下沉 目的是将最大的元素浮到堆顶来 sink(arr， 0， length); &#125;&#125;private static void buildHeap(int[] arr， int length) &#123; for (int i = length / 2; i &gt;= 0; i--) &#123; sink(arr， i， length); &#125;&#125;/** * 下沉调整 * @param arr 数组 * @param index 调整位置 * @param length 数组范围 */private static void sink(int[] arr， int index， int length) &#123; int leftChild = 2 * index + 1;//左子节点下标 int rightChild = 2 * index + 2;//右子节点下标 int present = index;//要调整的节点下标 //下沉左边 if (leftChild &lt; length &amp;&amp; arr[leftChild] &gt; arr[present]) &#123; present = leftChild; &#125; //下沉右边 if (rightChild &lt; length &amp;&amp; arr[rightChild] &gt; arr[present]) &#123; present = rightChild; &#125; //如果下标不相等 证明调换过了 if (present != index) &#123; //交换值 int temp = arr[index]; arr[index] = arr[present]; arr[present] = temp; //继续下沉 sink(arr， present， length); &#125;&#125; 堆排序和快速排序的时间复杂度都一样是 O(nlogn)]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>堆排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[冒泡排序]]></title>
    <url>%2F2015%2F09%2F01%2F%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[冒泡排序 冒泡排序无疑是最为出名的排序算法之一，从序列的一端开始往另一端冒泡（你可以从左往右冒泡，也可以从右往左冒泡，看心情），依次比较相邻的两个数的大小（到底是比大还是比小也看你心情）。 冒泡的代码还是相当简单的，两层循环，外层冒泡轮数，里层依次比较，江湖中人人尽皆知。 123456789101112public static void sort(int arr[])&#123; for( int i = 0 ; i &lt; arr.length - 1 ; i++ )&#123; for(int j = 0;j &lt; arr.length - 1 - i ; j++)&#123; int temp = 0; if(arr[j] &lt; arr[j + 1])&#123; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; &#125;&#125; 我们看到嵌套循环，应该立马就可以得出这个算法的时间复杂度为O(n2)。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>冒泡排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序]]></title>
    <url>%2F2015%2F09%2F01%2F%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[快速排序的核心思想也是分治法，分而治之。它的实现方式是每次从序列中选出一个基准值，其他数依次和基准值做比较，比基准值大的放右边，比基准值小的放左边，然后再对左边和右边的两组数分别选出一个基准值，进行同样的比较移动，重复步骤，直到最后都变成单个元素，整个数组就成了有序的序列。 快速排序的关键之处在于切分，切分的同时要进行比较和移动，这里介绍一种叫做单边扫描的做法。 我们随意抽取一个数作为基准值，同时设定一个标记 mark 代表左边序列最右侧的下标位置，当然初始为 0 ，接下来遍历数组，如果元素大于基准值，无操作，继续遍历，如果元素小于基准值，则把 mark + 1 ，再将 mark 所在位置的元素和遍历到的元素交换位置，mark 这个位置存储的是比基准值小的数据，当遍历结束后，将基准值与 mark 所在元素交换位置即可。 代码实现：1234567891011121314151617181920212223242526272829303132public static void sort(int[] arr) &#123; sort(arr， 0， arr.length - 1);&#125;private static void sort(int[] arr， int startIndex， int endIndex) &#123; if (endIndex &lt;= startIndex) &#123; return; &#125; //切分 int pivotIndex = partitionV2(arr， startIndex， endIndex); sort(arr， startIndex， pivotIndex-1); sort(arr， pivotIndex+1， endIndex);&#125;private static int partition(int[] arr， int startIndex， int endIndex) &#123; int pivot = arr[startIndex];//取基准值 int mark = startIndex;//Mark初始化为起始下标 for(int i=startIndex+1; i&lt;=endIndex; i++)&#123; if(arr[i]&lt;pivot)&#123; //小于基准值 则mark+1，并交换位置。 mark ++; int p = arr[mark]; arr[mark] = arr[i]; arr[i] = p; &#125; &#125; //基准值与mark对应元素调换位置 arr[startIndex] = arr[mark]; arr[mark] = pivot; return mark;&#125; 极端情况快速排序的时间复杂度和归并排序一样，O(n log n)，但这是建立在每次切分都能把数组一刀切两半差不多大的前提下，如果出现极端情况，比如排一个有序的序列，如[ 9，8，7，6，5，4，3，2，1 ]，选取基准值 9 ，那么需要切分 n - 1 次才能完成整个快速排序的过程，这种情况下，时间复杂度就退化成了 O(n2)，当然极端情况出现的概率也是比较低的。 所以说，快速排序的时间复杂度是 O(nlogn)，极端情况下会退化成 O(n2)，为了避免极端情况的发生，选取基准值应该做到随机选取，或者是打乱一下数组再选取。 另外，快速排序的空间复杂度为 O(1)。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>快速排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[插入排序]]></title>
    <url>%2F2015%2F09%2F01%2F%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[思想 插入排序的思想和我们打扑克摸牌的时候一样，从牌堆里一张一张摸起来的牌都是乱序的，我们会把摸起来的牌插入到左手中合适的位置，让左手中的牌时刻保持一个有序的状态。 那如果我们不是从牌堆里摸牌，而是左手里面初始化就是一堆乱牌呢？ 一样的道理，我们把牌往手的右边挪一挪，把手的左边空出一点位置来，然后在乱牌中抽一张出来，插入到左边，再抽一张出来，插入到左边，再抽一张，插入到左边，每次插入都插入到左边合适的位置，时刻保持左边的牌是有序的，直到右边的牌抽完，则排序完毕。 代码实现 123456789101112131415public static void sort(int[] arr) &#123; int n = arr.length; for (int i = 1; i &lt; n; ++i) &#123; int value = arr[i]; int j = 0;//插入的位置 for (j = i-1; j &gt;= 0; j--) &#123; if (arr[j] &gt; value) &#123; arr[j+1] = arr[j];//移动数据 &#125; else &#123; break; &#125; &#125; arr[j+1] = value; //插入数据 &#125;&#125; 从代码里我们可以看出，如果找到了合适的位置，就不会再进行比较了，就好比牌堆里抽出的一张牌本身就比我手里的牌都小，那么我只需要直接放在末尾就行了，不用一个一个去移动数据腾出位置插入到中间。 所以说，最好情况的时间复杂度是 O(n)，最坏情况的时间复杂度是 O(n2)，然而时间复杂度这个指标看的是最坏的情况，而不是最好的情况，所以插入排序的时间复杂度是 O(n2)。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>插入排序</tag>
      </tags>
  </entry>
</search>
